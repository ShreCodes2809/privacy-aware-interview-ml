{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":10013905,"sourceType":"datasetVersion","datasetId":6165202},{"sourceId":10014253,"sourceType":"datasetVersion","datasetId":6165433}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install twython","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install vaderSentiment","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Extracting Language Features","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:36.499782Z","iopub.execute_input":"2024-12-03T16:47:36.500395Z","iopub.status.idle":"2024-12-03T16:47:36.901142Z","shell.execute_reply.started":"2024-12-03T16:47:36.500351Z","shell.execute_reply":"2024-12-03T16:47:36.900172Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/ba-codes/Behavioral Annotation Codes.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P007_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P040_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P001_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P016_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P039_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P030_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P010_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P029_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P011_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P023_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P037_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P006_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P008_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P031_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P032_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P013_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P038_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P035_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P015_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P014_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P012_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P034_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P003_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P002_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P018_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P019_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P028_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P026_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P004_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P025_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P009_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P036_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P033_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P005_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P024_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P027_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P017_transcript.csv\n/kaggle/input/veteran-titles/VetTrain_Transcripts/P041_transcript.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport nltk\n\nnltk.download('vader_lexicon')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:36.902764Z","iopub.execute_input":"2024-12-03T16:47:36.903190Z","iopub.status.idle":"2024-12-03T16:47:38.271120Z","shell.execute_reply.started":"2024-12-03T16:47:36.903160Z","shell.execute_reply":"2024-12-03T16:47:38.270157Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package vader_lexicon to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package vader_lexicon is already up-to-date!\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":2},{"cell_type":"markdown","source":"## Data Preparation","metadata":{}},{"cell_type":"code","source":"# Set the local folder path\nfolder_path = r'/kaggle/input/veteran-titles/VetTrain_Transcripts'\n\n# Function to extract the numerical part from the filename\ndef extract_pid(filename):\n    base_name = os.path.splitext(filename)[0]\n    return base_name.split('_')[0]  # Assuming filename is like \"P001_transcript.csv\"\n\n# Get all CSV files in the folder and sort them numerically by filename\nall_files = sorted(\n    [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')],\n    key=lambda x: int(extract_pid(os.path.basename(x))[1:])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.272731Z","iopub.execute_input":"2024-12-03T16:47:38.273201Z","iopub.status.idle":"2024-12-03T16:47:38.279466Z","shell.execute_reply.started":"2024-12-03T16:47:38.273168Z","shell.execute_reply":"2024-12-03T16:47:38.278555Z"},"scrolled":true},"outputs":[],"execution_count":3},{"cell_type":"code","source":"# Initialize the final storage for combined question pairs\ncombined_data = []\n\n# Process each file\nfor file_path in all_files:\n    # Extract PID from filename\n    pid = extract_pid(os.path.basename(file_path))\n    \n    # Read the CSV file\n    df = pd.read_csv(file_path)\n    df = df[df['Type'] != 'IRR']  # Filter irrelevant rows\n\n    # Initialize dialogue extraction\n    current_dialogue = []\n    current_question_id = None\n    qid_counter = 1  # Start QID counter for each file\n\n    # Extract question pairs with PID and QID\n    for _, row in df.iterrows():\n        if row['Type'].startswith('Q'):\n            question_id = row['Type']\n            if current_question_id is None:\n                current_question_id = question_id\n                current_dialogue = [row['Transcript']]\n            elif question_id != current_question_id:\n                combined_data.append({\n                    'PID': pid,\n                    'QID': f\"Q{qid_counter}\",\n                    'Combined_Transcript': \" \".join(current_dialogue)\n                })\n                qid_counter += 1\n                current_dialogue = [row['Transcript']]\n                current_question_id = question_id\n            else:\n                current_dialogue.append(row['Transcript'])\n        else:\n            current_dialogue.append(row['Transcript'])\n\n    # Add the last dialogue for the file\n    if current_dialogue:\n        combined_data.append({\n            'PID': pid,\n            'QID': f\"Q{qid_counter}\",\n            'Combined_Transcript': \" \".join(current_dialogue)\n        })","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.280391Z","iopub.execute_input":"2024-12-03T16:47:38.280723Z","iopub.status.idle":"2024-12-03T16:47:38.715230Z","shell.execute_reply.started":"2024-12-03T16:47:38.280696Z","shell.execute_reply":"2024-12-03T16:47:38.714468Z"},"scrolled":true},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# Convert to a DataFrame\ndf_combined = pd.DataFrame(combined_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.717657Z","iopub.execute_input":"2024-12-03T16:47:38.718557Z","iopub.status.idle":"2024-12-03T16:47:38.723328Z","shell.execute_reply.started":"2024-12-03T16:47:38.718509Z","shell.execute_reply":"2024-12-03T16:47:38.722288Z"},"scrolled":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# Load the behavioral annotation codes \nbehavior_file = r'/kaggle/input/ba-codes/Behavioral Annotation Codes.csv'\ndf_behavior = pd.read_csv(behavior_file)\n\n# Merge behavioral codes\ndf_combined = df_combined.merge(df_behavior, on=['PID', 'QID'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.724398Z","iopub.execute_input":"2024-12-03T16:47:38.724716Z","iopub.status.idle":"2024-12-03T16:47:38.766277Z","shell.execute_reply.started":"2024-12-03T16:47:38.724652Z","shell.execute_reply":"2024-12-03T16:47:38.765343Z"},"scrolled":true},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# Data preprocessing\nimport re\n\ndef clean_text(text):\n    # lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Delete redundant Spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text\n\ndf_combined['Cleaned_Transcript'] = df_combined['Combined_Transcript'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.767415Z","iopub.execute_input":"2024-12-03T16:47:38.767689Z","iopub.status.idle":"2024-12-03T16:47:38.816492Z","shell.execute_reply.started":"2024-12-03T16:47:38.767663Z","shell.execute_reply":"2024-12-03T16:47:38.815692Z"},"scrolled":true},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# displaying the combined dataframe\ndf_combined.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.817398Z","iopub.execute_input":"2024-12-03T16:47:38.817679Z","iopub.status.idle":"2024-12-03T16:47:38.830810Z","shell.execute_reply.started":"2024-12-03T16:47:38.817654Z","shell.execute_reply":"2024-12-03T16:47:38.830047Z"},"scrolled":true},"outputs":[{"execution_count":8,"output_type":"execute_result","data":{"text/plain":"    PID  QID                                Combined_Transcript  \\\n0  P001   Q1  Interviewer: Good. I just uh, I uh, always hav...   \n1  P001   Q2  Interviewer: Okay, good deal. Interviewer: Yep...   \n2  P001   Q3  Interviewer: Okay, so with all of your experie...   \n3  P001   Q4  Interviewer: So, was that an easy transition f...   \n4  P001   Q5  Interviewer: So when we're talking about stren...   \n5  P001   Q6  Interviewer: Yeah, yeah, no, I get it, I get i...   \n6  P001   Q7  Interviewer: So how did you- Interviewer: What...   \n7  P001   Q8  Interviewer: Okay. Good. So, are you and your ...   \n8  P001   Q9  Interviewer: Sure. Yeah. So, when, ah, when ar...   \n9  P001  Q10  Interviewer: Yeah, I think that that is justif...   \n\n  Degree of Explanation                                 Cleaned_Transcript  \n0              Succinct  interviewer good i just uh i uh always have it...  \n1              Succinct  interviewer okay good deal interviewer yep i u...  \n2       Under-explained  interviewer okay so with all of your experienc...  \n3       Under-explained  interviewer so was that an easy transition fro...  \n4              Succinct  interviewer so when were talking about strengt...  \n5              Succinct  interviewer yeah yeah no i get it i get it so ...  \n6         Comprehensive  interviewer so how did you interviewer what wh...  \n7              Succinct  interviewer okay good so are you and your wife...  \n8        Over-explained  interviewer sure yeah so when ah when are you ...  \n9              Succinct  interviewer yeah i think that that is justifie...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>QID</th>\n      <th>Combined_Transcript</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>Q1</td>\n      <td>Interviewer: Good. I just uh, I uh, always hav...</td>\n      <td>Succinct</td>\n      <td>interviewer good i just uh i uh always have it...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>Q2</td>\n      <td>Interviewer: Okay, good deal. Interviewer: Yep...</td>\n      <td>Succinct</td>\n      <td>interviewer okay good deal interviewer yep i u...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P001</td>\n      <td>Q3</td>\n      <td>Interviewer: Okay, so with all of your experie...</td>\n      <td>Under-explained</td>\n      <td>interviewer okay so with all of your experienc...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P001</td>\n      <td>Q4</td>\n      <td>Interviewer: So, was that an easy transition f...</td>\n      <td>Under-explained</td>\n      <td>interviewer so was that an easy transition fro...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P001</td>\n      <td>Q5</td>\n      <td>Interviewer: So when we're talking about stren...</td>\n      <td>Succinct</td>\n      <td>interviewer so when were talking about strengt...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>P001</td>\n      <td>Q6</td>\n      <td>Interviewer: Yeah, yeah, no, I get it, I get i...</td>\n      <td>Succinct</td>\n      <td>interviewer yeah yeah no i get it i get it so ...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>P001</td>\n      <td>Q7</td>\n      <td>Interviewer: So how did you- Interviewer: What...</td>\n      <td>Comprehensive</td>\n      <td>interviewer so how did you interviewer what wh...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>P001</td>\n      <td>Q8</td>\n      <td>Interviewer: Okay. Good. So, are you and your ...</td>\n      <td>Succinct</td>\n      <td>interviewer okay good so are you and your wife...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>P001</td>\n      <td>Q9</td>\n      <td>Interviewer: Sure. Yeah. So, when, ah, when ar...</td>\n      <td>Over-explained</td>\n      <td>interviewer sure yeah so when ah when are you ...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>P001</td>\n      <td>Q10</td>\n      <td>Interviewer: Yeah, I think that that is justif...</td>\n      <td>Succinct</td>\n      <td>interviewer yeah i think that that is justifie...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"df_combined.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.832698Z","iopub.execute_input":"2024-12-03T16:47:38.832976Z","iopub.status.idle":"2024-12-03T16:47:38.838559Z","shell.execute_reply.started":"2024-12-03T16:47:38.832948Z","shell.execute_reply":"2024-12-03T16:47:38.837561Z"},"scrolled":true},"outputs":[{"execution_count":9,"output_type":"execute_result","data":{"text/plain":"(287, 5)"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk import pos_tag, word_tokenize\nimport nltk\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:38.959654Z","iopub.execute_input":"2024-12-03T16:47:38.959991Z","iopub.status.idle":"2024-12-03T16:47:39.057430Z","shell.execute_reply.started":"2024-12-03T16:47:38.959959Z","shell.execute_reply":"2024-12-03T16:47:39.056561Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[nltk_data] Downloading package punkt to /usr/share/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading package averaged_perceptron_tagger to\n[nltk_data]     /usr/share/nltk_data...\n[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n[nltk_data]       date!\n","output_type":"stream"},{"execution_count":10,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"# Count Vectorizer\nvectorizer = CountVectorizer()\ncount_vectorizer = vectorizer.fit_transform(df_combined['Cleaned_Transcript'])\nprint(\"Feature names:\", vectorizer.get_feature_names_out())\nprint(count_vectorizer.toarray())\nnp.shape(count_vectorizer.toarray())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:39.140156Z","iopub.execute_input":"2024-12-03T16:47:39.140867Z","iopub.status.idle":"2024-12-03T16:47:39.242028Z","shell.execute_reply.started":"2024-12-03T16:47:39.140828Z","shell.execute_reply":"2024-12-03T16:47:39.240986Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Feature names: ['01' '0ne' '10' ... 'zone' 'zoom' 'zooming']\n[[0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n [0 0 0 ... 0 0 0]\n ...\n [0 0 0 ... 0 0 0]\n [0 0 1 ... 0 0 0]\n [0 0 0 ... 0 0 0]]\n","output_type":"stream"},{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"(287, 5659)"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"# TF-IDF Vectorizer\ntfidf_vectorizer = TfidfVectorizer()\ntfidf_features = tfidf_vectorizer.fit_transform(df_combined['Cleaned_Transcript'])\nprint(tfidf_features.toarray())\nnp.shape(tfidf_features.toarray())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:47:39.612955Z","iopub.execute_input":"2024-12-03T16:47:39.613359Z","iopub.status.idle":"2024-12-03T16:47:39.693720Z","shell.execute_reply.started":"2024-12-03T16:47:39.613324Z","shell.execute_reply":"2024-12-03T16:47:39.692733Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[[0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]\n ...\n [0.         0.         0.         ... 0.         0.         0.        ]\n [0.         0.         0.05015529 ... 0.         0.         0.        ]\n [0.         0.         0.         ... 0.         0.         0.        ]]\n","output_type":"stream"},{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(287, 5659)"},"metadata":{}}],"execution_count":12},{"cell_type":"markdown","source":"## Adding the extracted features to the main dataframe","metadata":{}},{"cell_type":"code","source":"def add_tfidf_features(df, text_column, max_features=500):\n    \"\"\"\n    Adds TF-IDF features to the dataset.\n\n    Parameters:\n        df (pd.DataFrame): Input dataframe containing the text data.\n        text_column (str): Name of the column containing text data.\n        max_features (int): Maximum number of TF-IDF features to generate (default=500).\n    \n    Returns:\n        pd.DataFrame: Dataframe with TF-IDF features added.\n    \"\"\"\n    # Initialize TF-IDF Vectorizer\n    tfidf = TfidfVectorizer(max_features=max_features)\n    \n    # Fit and transform the text data\n    tfidf_matrix = tfidf.fit_transform(df[text_column])\n    \n    # Convert the TF-IDF matrix to a DataFrame\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=df.index)\n    \n    # Concatenate the TF-IDF features with the original DataFrame\n    df_with_tfidf = pd.concat([df, tfidf_df], axis=1)\n    \n    return df_with_tfidf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:48:30.908499Z","iopub.execute_input":"2024-12-03T16:48:30.908866Z","iopub.status.idle":"2024-12-03T16:48:30.915117Z","shell.execute_reply.started":"2024-12-03T16:48:30.908833Z","shell.execute_reply":"2024-12-03T16:48:30.914174Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"# Example usage\ndf_combined = add_tfidf_features(df_combined, text_column=\"Cleaned_Transcript\", max_features=1000)\ndf_combined.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:49:38.693965Z","iopub.execute_input":"2024-12-03T16:49:38.694356Z","iopub.status.idle":"2024-12-03T16:49:38.798734Z","shell.execute_reply.started":"2024-12-03T16:49:38.694322Z","shell.execute_reply":"2024-12-03T16:49:38.797679Z"}},"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"    PID QID                                Combined_Transcript  \\\n0  P001  Q1  Interviewer: Good. I just uh, I uh, always hav...   \n1  P001  Q2  Interviewer: Okay, good deal. Interviewer: Yep...   \n2  P001  Q3  Interviewer: Okay, so with all of your experie...   \n3  P001  Q4  Interviewer: So, was that an easy transition f...   \n4  P001  Q5  Interviewer: So when we're talking about stren...   \n\n  Degree of Explanation                                 Cleaned_Transcript  \\\n0              Succinct  interviewer good i just uh i uh always have it...   \n1              Succinct  interviewer okay good deal interviewer yep i u...   \n2       Under-explained  interviewer okay so with all of your experienc...   \n3       Under-explained  interviewer so was that an easy transition fro...   \n4              Succinct  interviewer so when were talking about strengt...   \n\n    15   20  ability  able     about  ...  yet       you  youd  youll  young  \\\n0  0.0  0.0      0.0   0.0  0.086414  ...  0.0  0.158448   0.0    0.0    0.0   \n1  0.0  0.0      0.0   0.0  0.134805  ...  0.0  0.154487   0.0    0.0    0.0   \n2  0.0  0.0      0.0   0.0  0.072895  ...  0.0  0.150369   0.0    0.0    0.0   \n3  0.0  0.0      0.0   0.0  0.000000  ...  0.0  0.104521   0.0    0.0    0.0   \n4  0.0  0.0      0.0   0.0  0.035298  ...  0.0  0.364069   0.0    0.0    0.0   \n\n       your     youre  yourself  youve  zoom  \n0  0.000000  0.035163  0.062281    0.0   0.0  \n1  0.292035  0.000000  0.000000    0.0   0.0  \n2  0.067679  0.044494  0.000000    0.0   0.0  \n3  0.000000  0.000000  0.000000    0.0   0.0  \n4  0.032772  0.000000  0.000000    0.0   0.0  \n\n[5 rows x 1005 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>QID</th>\n      <th>Combined_Transcript</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>...</th>\n      <th>yet</th>\n      <th>you</th>\n      <th>youd</th>\n      <th>youll</th>\n      <th>young</th>\n      <th>your</th>\n      <th>youre</th>\n      <th>yourself</th>\n      <th>youve</th>\n      <th>zoom</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>Q1</td>\n      <td>Interviewer: Good. I just uh, I uh, always hav...</td>\n      <td>Succinct</td>\n      <td>interviewer good i just uh i uh always have it...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.086414</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.158448</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.035163</td>\n      <td>0.062281</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>Q2</td>\n      <td>Interviewer: Okay, good deal. Interviewer: Yep...</td>\n      <td>Succinct</td>\n      <td>interviewer okay good deal interviewer yep i u...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.134805</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.154487</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.292035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P001</td>\n      <td>Q3</td>\n      <td>Interviewer: Okay, so with all of your experie...</td>\n      <td>Under-explained</td>\n      <td>interviewer okay so with all of your experienc...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.072895</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.150369</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.067679</td>\n      <td>0.044494</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P001</td>\n      <td>Q4</td>\n      <td>Interviewer: So, was that an easy transition f...</td>\n      <td>Under-explained</td>\n      <td>interviewer so was that an easy transition fro...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.104521</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P001</td>\n      <td>Q5</td>\n      <td>Interviewer: So when we're talking about stren...</td>\n      <td>Succinct</td>\n      <td>interviewer so when were talking about strengt...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.035298</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.364069</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.032772</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1005 columns</p>\n</div>"},"metadata":{}}],"execution_count":16},{"cell_type":"code","source":"def add_pos_tags(df, text_column):\n    \"\"\"\n    Adds POS tagging to the dataframe.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe containing text data.\n        text_column (str): Name of the column containing text data.\n    \n    Returns:\n        pd.DataFrame: Dataframe with added POS tags.\n    \"\"\"\n    def pos_tags(text):\n        tokens = word_tokenize(text)\n        tags = pos_tag(tokens)\n        return {tag: len([word for word, pos in tags if pos == tag]) for tag in set([pos for _, pos in tags])}\n    \n    df['POS_Tags'] = df[text_column].apply(pos_tags)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:49:44.441535Z","iopub.execute_input":"2024-12-03T16:49:44.441870Z","iopub.status.idle":"2024-12-03T16:49:44.447378Z","shell.execute_reply.started":"2024-12-03T16:49:44.441840Z","shell.execute_reply":"2024-12-03T16:49:44.446518Z"},"scrolled":true},"outputs":[],"execution_count":17},{"cell_type":"code","source":"df_combined = add_pos_tags(df_combined, text_column=\"Cleaned_Transcript\")\ndf_combined.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:49:45.589563Z","iopub.execute_input":"2024-12-03T16:49:45.590424Z","iopub.status.idle":"2024-12-03T16:49:49.462277Z","shell.execute_reply.started":"2024-12-03T16:49:45.590387Z","shell.execute_reply":"2024-12-03T16:49:49.461105Z"},"scrolled":true},"outputs":[{"execution_count":18,"output_type":"execute_result","data":{"text/plain":"    PID  QID                                Combined_Transcript  \\\n0  P001   Q1  Interviewer: Good. I just uh, I uh, always hav...   \n1  P001   Q2  Interviewer: Okay, good deal. Interviewer: Yep...   \n2  P001   Q3  Interviewer: Okay, so with all of your experie...   \n3  P001   Q4  Interviewer: So, was that an easy transition f...   \n4  P001   Q5  Interviewer: So when we're talking about stren...   \n5  P001   Q6  Interviewer: Yeah, yeah, no, I get it, I get i...   \n6  P001   Q7  Interviewer: So how did you- Interviewer: What...   \n7  P001   Q8  Interviewer: Okay. Good. So, are you and your ...   \n8  P001   Q9  Interviewer: Sure. Yeah. So, when, ah, when ar...   \n9  P001  Q10  Interviewer: Yeah, I think that that is justif...   \n\n  Degree of Explanation                                 Cleaned_Transcript  \\\n0              Succinct  interviewer good i just uh i uh always have it...   \n1              Succinct  interviewer okay good deal interviewer yep i u...   \n2       Under-explained  interviewer okay so with all of your experienc...   \n3       Under-explained  interviewer so was that an easy transition fro...   \n4              Succinct  interviewer so when were talking about strengt...   \n5              Succinct  interviewer yeah yeah no i get it i get it so ...   \n6         Comprehensive  interviewer so how did you interviewer what wh...   \n7              Succinct  interviewer okay good so are you and your wife...   \n8        Over-explained  interviewer sure yeah so when ah when are you ...   \n9              Succinct  interviewer yeah i think that that is justifie...   \n\n    15   20  ability    able     about  ...       you  youd  youll     young  \\\n0  0.0  0.0      0.0  0.0000  0.086414  ...  0.158448   0.0    0.0  0.000000   \n1  0.0  0.0      0.0  0.0000  0.134805  ...  0.154487   0.0    0.0  0.000000   \n2  0.0  0.0      0.0  0.0000  0.072895  ...  0.150369   0.0    0.0  0.000000   \n3  0.0  0.0      0.0  0.0000  0.000000  ...  0.104521   0.0    0.0  0.000000   \n4  0.0  0.0      0.0  0.0000  0.035298  ...  0.364069   0.0    0.0  0.000000   \n5  0.0  0.0      0.0  0.0000  0.000000  ...  0.092078   0.0    0.0  0.000000   \n6  0.0  0.0      0.0  0.0000  0.057687  ...  0.198330   0.0    0.0  0.000000   \n7  0.0  0.0      0.0  0.0831  0.000000  ...  0.035474   0.0    0.0  0.000000   \n8  0.0  0.0      0.0  0.0000  0.000000  ...  0.232737   0.0    0.0  0.000000   \n9  0.0  0.0      0.0  0.0000  0.038553  ...  0.079527   0.0    0.0  0.104115   \n\n       your     youre  yourself     youve  zoom  \\\n0  0.000000  0.035163  0.062281  0.000000   0.0   \n1  0.292035  0.000000  0.000000  0.000000   0.0   \n2  0.067679  0.044494  0.000000  0.000000   0.0   \n3  0.000000  0.000000  0.000000  0.000000   0.0   \n4  0.032772  0.000000  0.000000  0.000000   0.0   \n5  0.082885  0.054491  0.000000  0.000000   0.0   \n6  0.026779  0.070422  0.000000  0.000000   0.0   \n7  0.047899  0.000000  0.000000  0.000000   0.0   \n8  0.044893  0.029514  0.000000  0.042132   0.0   \n9  0.035794  0.047064  0.000000  0.000000   0.0   \n\n                                            POS_Tags  \n0  {'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...  \n1  {'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...  \n2  {'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...  \n3  {'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...  \n4  {'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...  \n5  {'TO': 3, 'NNS': 11, 'WP': 1, 'EX': 1, 'RP': 1...  \n6  {'MD': 1, 'TO': 14, 'NNS': 8, 'CD': 3, 'VBD': ...  \n7  {'VBZ': 3, 'TO': 4, 'VBP': 4, 'NNS': 4, 'RP': ...  \n8  {'MD': 6, 'TO': 16, 'NNS': 18, 'CD': 6, 'VBD':...  \n9  {'MD': 4, 'TO': 8, 'NNS': 6, 'VBD': 1, 'WP': 3...  \n\n[10 rows x 1006 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>QID</th>\n      <th>Combined_Transcript</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>...</th>\n      <th>you</th>\n      <th>youd</th>\n      <th>youll</th>\n      <th>young</th>\n      <th>your</th>\n      <th>youre</th>\n      <th>yourself</th>\n      <th>youve</th>\n      <th>zoom</th>\n      <th>POS_Tags</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>Q1</td>\n      <td>Interviewer: Good. I just uh, I uh, always hav...</td>\n      <td>Succinct</td>\n      <td>interviewer good i just uh i uh always have it...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.086414</td>\n      <td>...</td>\n      <td>0.158448</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.035163</td>\n      <td>0.062281</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>Q2</td>\n      <td>Interviewer: Okay, good deal. Interviewer: Yep...</td>\n      <td>Succinct</td>\n      <td>interviewer okay good deal interviewer yep i u...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.134805</td>\n      <td>...</td>\n      <td>0.154487</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.292035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P001</td>\n      <td>Q3</td>\n      <td>Interviewer: Okay, so with all of your experie...</td>\n      <td>Under-explained</td>\n      <td>interviewer okay so with all of your experienc...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.072895</td>\n      <td>...</td>\n      <td>0.150369</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.067679</td>\n      <td>0.044494</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P001</td>\n      <td>Q4</td>\n      <td>Interviewer: So, was that an easy transition f...</td>\n      <td>Under-explained</td>\n      <td>interviewer so was that an easy transition fro...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.104521</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P001</td>\n      <td>Q5</td>\n      <td>Interviewer: So when we're talking about stren...</td>\n      <td>Succinct</td>\n      <td>interviewer so when were talking about strengt...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.035298</td>\n      <td>...</td>\n      <td>0.364069</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.032772</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>P001</td>\n      <td>Q6</td>\n      <td>Interviewer: Yeah, yeah, no, I get it, I get i...</td>\n      <td>Succinct</td>\n      <td>interviewer yeah yeah no i get it i get it so ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.092078</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.082885</td>\n      <td>0.054491</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'TO': 3, 'NNS': 11, 'WP': 1, 'EX': 1, 'RP': 1...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>P001</td>\n      <td>Q7</td>\n      <td>Interviewer: So how did you- Interviewer: What...</td>\n      <td>Comprehensive</td>\n      <td>interviewer so how did you interviewer what wh...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.057687</td>\n      <td>...</td>\n      <td>0.198330</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.026779</td>\n      <td>0.070422</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 1, 'TO': 14, 'NNS': 8, 'CD': 3, 'VBD': ...</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>P001</td>\n      <td>Q8</td>\n      <td>Interviewer: Okay. Good. So, are you and your ...</td>\n      <td>Succinct</td>\n      <td>interviewer okay good so are you and your wife...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0831</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.035474</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.047899</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'VBZ': 3, 'TO': 4, 'VBP': 4, 'NNS': 4, 'RP': ...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>P001</td>\n      <td>Q9</td>\n      <td>Interviewer: Sure. Yeah. So, when, ah, when ar...</td>\n      <td>Over-explained</td>\n      <td>interviewer sure yeah so when ah when are you ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.000000</td>\n      <td>...</td>\n      <td>0.232737</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.044893</td>\n      <td>0.029514</td>\n      <td>0.000000</td>\n      <td>0.042132</td>\n      <td>0.0</td>\n      <td>{'MD': 6, 'TO': 16, 'NNS': 18, 'CD': 6, 'VBD':...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>P001</td>\n      <td>Q10</td>\n      <td>Interviewer: Yeah, I think that that is justif...</td>\n      <td>Succinct</td>\n      <td>interviewer yeah i think that that is justifie...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0000</td>\n      <td>0.038553</td>\n      <td>...</td>\n      <td>0.079527</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.104115</td>\n      <td>0.035794</td>\n      <td>0.047064</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 4, 'TO': 8, 'NNS': 6, 'VBD': 1, 'WP': 3...</td>\n    </tr>\n  </tbody>\n</table>\n<p>10 rows × 1006 columns</p>\n</div>"},"metadata":{}}],"execution_count":18},{"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\ndef add_sentiment_scores(df, text_column):\n    \"\"\"\n    Adds sentiment scores as separate columns to the dataframe.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe containing text data.\n        text_column (str): Name of the column containing text data.\n    \n    Returns:\n        pd.DataFrame: Dataframe with added sentiment scores as separate columns.\n    \"\"\"\n    sia = SentimentIntensityAnalyzer()\n    sentiment_scores = df[text_column].apply(lambda text: sia.polarity_scores(text))\n    \n    # Create separate columns for each sentiment score\n    df['Sentiment_Neg'] = sentiment_scores.apply(lambda score: score['neg'])\n    df['Sentiment_Neu'] = sentiment_scores.apply(lambda score: score['neu'])\n    df['Sentiment_Pos'] = sentiment_scores.apply(lambda score: score['pos'])\n    df['Sentiment_Compound'] = sentiment_scores.apply(lambda score: score['compound'])\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:50:01.898716Z","iopub.execute_input":"2024-12-03T16:50:01.899070Z","iopub.status.idle":"2024-12-03T16:50:01.907901Z","shell.execute_reply.started":"2024-12-03T16:50:01.899037Z","shell.execute_reply":"2024-12-03T16:50:01.907139Z"},"scrolled":true},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Adding sentiment scores as separate columns to the dataframe\ndf_combined = add_sentiment_scores(df_combined, text_column=\"Cleaned_Transcript\")\n\n# Example output for the first row\nprint(df_combined[['Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos', 'Sentiment_Compound']].iloc[0])","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-03T16:50:03.235629Z","iopub.execute_input":"2024-12-03T16:50:03.236517Z","iopub.status.idle":"2024-12-03T16:50:04.557579Z","shell.execute_reply.started":"2024-12-03T16:50:03.236478Z","shell.execute_reply":"2024-12-03T16:50:04.556616Z"}},"outputs":[{"name":"stdout","text":"Sentiment_Neg         0.0070\nSentiment_Neu         0.8210\nSentiment_Pos         0.1720\nSentiment_Compound    0.9936\nName: 0, dtype: float64\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"df_combined.drop(['QID', 'Combined_Transcript'], axis=1, inplace=True)\ndf_combined.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:50:06.183778Z","iopub.execute_input":"2024-12-03T16:50:06.184767Z","iopub.status.idle":"2024-12-03T16:50:06.216559Z","shell.execute_reply.started":"2024-12-03T16:50:06.184727Z","shell.execute_reply":"2024-12-03T16:50:06.215522Z"},"scrolled":true},"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"    PID Degree of Explanation  \\\n0  P001              Succinct   \n1  P001              Succinct   \n2  P001       Under-explained   \n3  P001       Under-explained   \n4  P001              Succinct   \n\n                                  Cleaned_Transcript   15   20  ability  able  \\\n0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n\n      about  above  absolutely  ...      your     youre  yourself  youve  \\\n0  0.086414    0.0         0.0  ...  0.000000  0.035163  0.062281    0.0   \n1  0.134805    0.0         0.0  ...  0.292035  0.000000  0.000000    0.0   \n2  0.072895    0.0         0.0  ...  0.067679  0.044494  0.000000    0.0   \n3  0.000000    0.0         0.0  ...  0.000000  0.000000  0.000000    0.0   \n4  0.035298    0.0         0.0  ...  0.032772  0.000000  0.000000    0.0   \n\n   zoom                                           POS_Tags  Sentiment_Neg  \\\n0   0.0  {'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...          0.007   \n1   0.0  {'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...          0.036   \n2   0.0  {'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...          0.011   \n3   0.0  {'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...          0.036   \n4   0.0  {'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...          0.028   \n\n   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \n0          0.821          0.172              0.9936  \n1          0.919          0.045             -0.0889  \n2          0.904          0.085              0.9286  \n3          0.910          0.054              0.5568  \n4          0.828          0.144              0.9653  \n\n[5 rows x 1008 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>above</th>\n      <th>absolutely</th>\n      <th>...</th>\n      <th>your</th>\n      <th>youre</th>\n      <th>yourself</th>\n      <th>youve</th>\n      <th>zoom</th>\n      <th>POS_Tags</th>\n      <th>Sentiment_Neg</th>\n      <th>Sentiment_Neu</th>\n      <th>Sentiment_Pos</th>\n      <th>Sentiment_Compound</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer good i just uh i uh always have it...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.086414</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.035163</td>\n      <td>0.062281</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...</td>\n      <td>0.007</td>\n      <td>0.821</td>\n      <td>0.172</td>\n      <td>0.9936</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer okay good deal interviewer yep i u...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.134805</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.292035</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...</td>\n      <td>0.036</td>\n      <td>0.919</td>\n      <td>0.045</td>\n      <td>-0.0889</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P001</td>\n      <td>Under-explained</td>\n      <td>interviewer okay so with all of your experienc...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.072895</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.067679</td>\n      <td>0.044494</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...</td>\n      <td>0.011</td>\n      <td>0.904</td>\n      <td>0.085</td>\n      <td>0.9286</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P001</td>\n      <td>Under-explained</td>\n      <td>interviewer so was that an easy transition fro...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...</td>\n      <td>0.036</td>\n      <td>0.910</td>\n      <td>0.054</td>\n      <td>0.5568</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer so when were talking about strengt...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.035298</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.032772</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...</td>\n      <td>0.028</td>\n      <td>0.828</td>\n      <td>0.144</td>\n      <td>0.9653</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1008 columns</p>\n</div>"},"metadata":{}}],"execution_count":21},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\ndef add_word_embeddings(df, text_column, model_name='all-MiniLM-L6-v2'):\n    \"\"\"\n    Adds word embeddings to the dataframe using Sentence Transformers.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe containing text data.\n        text_column (str): Name of the column containing text data.\n        model_name (str): Sentence Transformer model name.\n    \n    Returns:\n        pd.DataFrame: Dataframe with added embeddings.\n    \"\"\"\n    model = SentenceTransformer(model_name)\n    embeddings = model.encode(df[text_column].tolist(), show_progress_bar=True)\n    df['Embeddings'] = list(embeddings)\n    return df","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-03T16:50:57.457140Z","iopub.execute_input":"2024-12-03T16:50:57.457495Z","iopub.status.idle":"2024-12-03T16:51:14.760616Z","shell.execute_reply.started":"2024-12-03T16:50:57.457461Z","shell.execute_reply":"2024-12-03T16:51:14.759821Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"df_combined = add_word_embeddings(df_combined, text_column=\"Cleaned_Transcript\")\ndf_combined.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:51:14.762294Z","iopub.execute_input":"2024-12-03T16:51:14.763017Z","iopub.status.idle":"2024-12-03T16:51:18.520044Z","shell.execute_reply.started":"2024-12-03T16:51:14.762975Z","shell.execute_reply":"2024-12-03T16:51:18.519145Z"},"scrolled":true},"outputs":[{"output_type":"display_data","data":{"text/plain":"modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2a41ed4a20964a57ab8621ac97145db5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c57233b553f41d487a6253834adf708"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dcb0652872364503a344306592955409"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d57b6e9d467142759af8971d10ed8e80"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9c2cfeb566704da198c37774e3446343"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7fddc8d7bf084bdb8b1631b41af56f35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"901c8f94c7a14ff78105eceadba9a773"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b8d49d195544224ada8e55a0cfebbe8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2657b4f697ba411aa72b193438c82d51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"032ddc383bae4ac6b47c820b80783533"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7a86a94e44f247599df7fa561b71a6a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Batches:   0%|          | 0/9 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4d7d2564e1543ac9eda4158a4740c25"}},"metadata":{}},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"    PID Degree of Explanation  \\\n0  P001              Succinct   \n1  P001              Succinct   \n2  P001       Under-explained   \n3  P001       Under-explained   \n4  P001              Succinct   \n\n                                  Cleaned_Transcript   15   20  ability  able  \\\n0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n\n      about  above  absolutely  ...     youre  yourself  youve  zoom  \\\n0  0.086414    0.0         0.0  ...  0.035163  0.062281    0.0   0.0   \n1  0.134805    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n2  0.072895    0.0         0.0  ...  0.044494  0.000000    0.0   0.0   \n3  0.000000    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n4  0.035298    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n\n                                            POS_Tags  Sentiment_Neg  \\\n0  {'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...          0.007   \n1  {'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...          0.036   \n2  {'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...          0.011   \n3  {'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...          0.036   \n4  {'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...          0.028   \n\n   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \\\n0          0.821          0.172              0.9936   \n1          0.919          0.045             -0.0889   \n2          0.904          0.085              0.9286   \n3          0.910          0.054              0.5568   \n4          0.828          0.144              0.9653   \n\n                                          Embeddings  \n0  [-0.08247561, 0.009922183, 0.033036105, 0.0038...  \n1  [-0.029192012, 0.056575354, 0.078753695, 0.020...  \n2  [-0.042129416, 0.026093012, 0.020127049, 0.027...  \n3  [-0.067918584, 0.021701401, 0.03339226, -0.006...  \n4  [0.017702455, 0.050047435, 0.011827803, -0.014...  \n\n[5 rows x 1009 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>above</th>\n      <th>absolutely</th>\n      <th>...</th>\n      <th>youre</th>\n      <th>yourself</th>\n      <th>youve</th>\n      <th>zoom</th>\n      <th>POS_Tags</th>\n      <th>Sentiment_Neg</th>\n      <th>Sentiment_Neu</th>\n      <th>Sentiment_Pos</th>\n      <th>Sentiment_Compound</th>\n      <th>Embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer good i just uh i uh always have it...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.086414</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.035163</td>\n      <td>0.062281</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...</td>\n      <td>0.007</td>\n      <td>0.821</td>\n      <td>0.172</td>\n      <td>0.9936</td>\n      <td>[-0.08247561, 0.009922183, 0.033036105, 0.0038...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer okay good deal interviewer yep i u...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.134805</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...</td>\n      <td>0.036</td>\n      <td>0.919</td>\n      <td>0.045</td>\n      <td>-0.0889</td>\n      <td>[-0.029192012, 0.056575354, 0.078753695, 0.020...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P001</td>\n      <td>Under-explained</td>\n      <td>interviewer okay so with all of your experienc...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.072895</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.044494</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...</td>\n      <td>0.011</td>\n      <td>0.904</td>\n      <td>0.085</td>\n      <td>0.9286</td>\n      <td>[-0.042129416, 0.026093012, 0.020127049, 0.027...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P001</td>\n      <td>Under-explained</td>\n      <td>interviewer so was that an easy transition fro...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...</td>\n      <td>0.036</td>\n      <td>0.910</td>\n      <td>0.054</td>\n      <td>0.5568</td>\n      <td>[-0.067918584, 0.021701401, 0.03339226, -0.006...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer so when were talking about strengt...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.035298</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...</td>\n      <td>0.028</td>\n      <td>0.828</td>\n      <td>0.144</td>\n      <td>0.9653</td>\n      <td>[0.017702455, 0.050047435, 0.011827803, -0.014...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1009 columns</p>\n</div>"},"metadata":{}}],"execution_count":23},{"cell_type":"markdown","source":"# Classifying between Over-Explained and Comprehensive","metadata":{}},{"cell_type":"markdown","source":"## Cleaning Dataset","metadata":{}},{"cell_type":"code","source":"# creating a dataset for classification\ndf_new = df_combined.copy()\ndf_new.head()","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-03T16:51:48.558677Z","iopub.execute_input":"2024-12-03T16:51:48.559035Z","iopub.status.idle":"2024-12-03T16:51:48.595375Z","shell.execute_reply.started":"2024-12-03T16:51:48.559004Z","shell.execute_reply":"2024-12-03T16:51:48.594423Z"}},"outputs":[{"execution_count":24,"output_type":"execute_result","data":{"text/plain":"    PID Degree of Explanation  \\\n0  P001              Succinct   \n1  P001              Succinct   \n2  P001       Under-explained   \n3  P001       Under-explained   \n4  P001              Succinct   \n\n                                  Cleaned_Transcript   15   20  ability  able  \\\n0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n\n      about  above  absolutely  ...     youre  yourself  youve  zoom  \\\n0  0.086414    0.0         0.0  ...  0.035163  0.062281    0.0   0.0   \n1  0.134805    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n2  0.072895    0.0         0.0  ...  0.044494  0.000000    0.0   0.0   \n3  0.000000    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n4  0.035298    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n\n                                            POS_Tags  Sentiment_Neg  \\\n0  {'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...          0.007   \n1  {'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...          0.036   \n2  {'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...          0.011   \n3  {'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...          0.036   \n4  {'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...          0.028   \n\n   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \\\n0          0.821          0.172              0.9936   \n1          0.919          0.045             -0.0889   \n2          0.904          0.085              0.9286   \n3          0.910          0.054              0.5568   \n4          0.828          0.144              0.9653   \n\n                                          Embeddings  \n0  [-0.08247561, 0.009922183, 0.033036105, 0.0038...  \n1  [-0.029192012, 0.056575354, 0.078753695, 0.020...  \n2  [-0.042129416, 0.026093012, 0.020127049, 0.027...  \n3  [-0.067918584, 0.021701401, 0.03339226, -0.006...  \n4  [0.017702455, 0.050047435, 0.011827803, -0.014...  \n\n[5 rows x 1009 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>above</th>\n      <th>absolutely</th>\n      <th>...</th>\n      <th>youre</th>\n      <th>yourself</th>\n      <th>youve</th>\n      <th>zoom</th>\n      <th>POS_Tags</th>\n      <th>Sentiment_Neg</th>\n      <th>Sentiment_Neu</th>\n      <th>Sentiment_Pos</th>\n      <th>Sentiment_Compound</th>\n      <th>Embeddings</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer good i just uh i uh always have it...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.086414</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.035163</td>\n      <td>0.062281</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 3, 'TO': 7, 'NNS': 9, 'CD': 3, 'VBD': 4...</td>\n      <td>0.007</td>\n      <td>0.821</td>\n      <td>0.172</td>\n      <td>0.9936</td>\n      <td>[-0.08247561, 0.009922183, 0.033036105, 0.0038...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer okay good deal interviewer yep i u...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.134805</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 2, 'NNS': 6, 'CD': 3, 'VBD': 1...</td>\n      <td>0.036</td>\n      <td>0.919</td>\n      <td>0.045</td>\n      <td>-0.0889</td>\n      <td>[-0.029192012, 0.056575354, 0.078753695, 0.020...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P001</td>\n      <td>Under-explained</td>\n      <td>interviewer okay so with all of your experienc...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.072895</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.044494</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 2, 'TO': 7, 'NNS': 6, 'VBD': 11, 'WP': ...</td>\n      <td>0.011</td>\n      <td>0.904</td>\n      <td>0.085</td>\n      <td>0.9286</td>\n      <td>[-0.042129416, 0.026093012, 0.020127049, 0.027...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P001</td>\n      <td>Under-explained</td>\n      <td>interviewer so was that an easy transition fro...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'TO': 5, 'NNS': 2, 'VBD': 6, 'WP': 1, 'RP': 3...</td>\n      <td>0.036</td>\n      <td>0.910</td>\n      <td>0.054</td>\n      <td>0.5568</td>\n      <td>[-0.067918584, 0.021701401, 0.03339226, -0.006...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P001</td>\n      <td>Succinct</td>\n      <td>interviewer so when were talking about strengt...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.035298</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>{'MD': 4, 'TO': 5, 'NNS': 10, 'VBD': 4, 'WP': ...</td>\n      <td>0.028</td>\n      <td>0.828</td>\n      <td>0.144</td>\n      <td>0.9653</td>\n      <td>[0.017702455, 0.050047435, 0.011827803, -0.014...</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1009 columns</p>\n</div>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"df_new = df_new[df_new['Degree of Explanation'].isin(['Over-explained', 'Comprehensive'])]\ndf_new['DOE_Label'] = df_new['Degree of Explanation'].map({'Over-explained': 0, 'Comprehensive': 1})\ndf_new.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:07.994612Z","iopub.execute_input":"2024-12-03T16:52:07.994975Z","iopub.status.idle":"2024-12-03T16:52:08.033841Z","shell.execute_reply.started":"2024-12-03T16:52:07.994941Z","shell.execute_reply":"2024-12-03T16:52:08.032834Z"},"scrolled":true},"outputs":[{"execution_count":25,"output_type":"execute_result","data":{"text/plain":"     PID Degree of Explanation  \\\n6   P001         Comprehensive   \n8   P001        Over-explained   \n12  P002         Comprehensive   \n14  P002         Comprehensive   \n21  P003         Comprehensive   \n\n                                   Cleaned_Transcript   15   20  ability  \\\n6   interviewer so how did you interviewer what wh...  0.0  0.0      0.0   \n8   interviewer sure yeah so when ah when are you ...  0.0  0.0      0.0   \n12  interviewer yeah having its its sometimes eyeo...  0.0  0.0      0.0   \n14  interviewer okay yeah nope thats fair definite...  0.0  0.0      0.0   \n21  interviewer right its just its its a yeah its ...  0.0  0.0      0.0   \n\n        able     about  above  absolutely  ...  yourself     youve  zoom  \\\n6   0.000000  0.057687    0.0         0.0  ...       0.0  0.000000   0.0   \n8   0.000000  0.000000    0.0         0.0  ...       0.0  0.042132   0.0   \n12  0.096853  0.000000    0.0         0.0  ...       0.0  0.052393   0.0   \n14  0.000000  0.000000    0.0         0.0  ...       0.0  0.000000   0.0   \n21  0.000000  0.037652    0.0         0.0  ...       0.0  0.000000   0.0   \n\n                                             POS_Tags  Sentiment_Neg  \\\n6   {'MD': 1, 'TO': 14, 'NNS': 8, 'CD': 3, 'VBD': ...          0.030   \n8   {'MD': 6, 'TO': 16, 'NNS': 18, 'CD': 6, 'VBD':...          0.021   \n12  {'MD': 3, 'TO': 9, 'NNS': 12, 'CD': 2, 'VBD': ...          0.005   \n14  {'MD': 6, 'TO': 4, 'NNS': 10, 'CD': 5, 'VBD': ...          0.023   \n21  {'MD': 3, 'TO': 6, 'NNS': 9, 'CD': 1, 'VBD': 1...          0.000   \n\n    Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \\\n6           0.939          0.031              0.0267   \n8           0.918          0.061              0.8860   \n12          0.837          0.158              0.9922   \n14          0.871          0.106              0.9865   \n21          0.888          0.112              0.9689   \n\n                                           Embeddings  DOE_Label  \n6   [-0.09835275, 0.03342816, -0.012291069, -0.022...          1  \n8   [-0.059087228, 0.039102662, 0.019083181, -0.00...          0  \n12  [-0.11492155, 0.019738441, 0.029623719, -0.013...          1  \n14  [-0.067289285, 0.03890233, 0.00046749198, 0.02...          1  \n21  [-0.06344438, 0.0012140614, 0.029845642, 0.003...          1  \n\n[5 rows x 1010 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>Degree of Explanation</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>above</th>\n      <th>absolutely</th>\n      <th>...</th>\n      <th>yourself</th>\n      <th>youve</th>\n      <th>zoom</th>\n      <th>POS_Tags</th>\n      <th>Sentiment_Neg</th>\n      <th>Sentiment_Neu</th>\n      <th>Sentiment_Pos</th>\n      <th>Sentiment_Compound</th>\n      <th>Embeddings</th>\n      <th>DOE_Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>6</th>\n      <td>P001</td>\n      <td>Comprehensive</td>\n      <td>interviewer so how did you interviewer what wh...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.057687</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 1, 'TO': 14, 'NNS': 8, 'CD': 3, 'VBD': ...</td>\n      <td>0.030</td>\n      <td>0.939</td>\n      <td>0.031</td>\n      <td>0.0267</td>\n      <td>[-0.09835275, 0.03342816, -0.012291069, -0.022...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>P001</td>\n      <td>Over-explained</td>\n      <td>interviewer sure yeah so when ah when are you ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.042132</td>\n      <td>0.0</td>\n      <td>{'MD': 6, 'TO': 16, 'NNS': 18, 'CD': 6, 'VBD':...</td>\n      <td>0.021</td>\n      <td>0.918</td>\n      <td>0.061</td>\n      <td>0.8860</td>\n      <td>[-0.059087228, 0.039102662, 0.019083181, -0.00...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>P002</td>\n      <td>Comprehensive</td>\n      <td>interviewer yeah having its its sometimes eyeo...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.096853</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.052393</td>\n      <td>0.0</td>\n      <td>{'MD': 3, 'TO': 9, 'NNS': 12, 'CD': 2, 'VBD': ...</td>\n      <td>0.005</td>\n      <td>0.837</td>\n      <td>0.158</td>\n      <td>0.9922</td>\n      <td>[-0.11492155, 0.019738441, 0.029623719, -0.013...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>P002</td>\n      <td>Comprehensive</td>\n      <td>interviewer okay yeah nope thats fair definite...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 6, 'TO': 4, 'NNS': 10, 'CD': 5, 'VBD': ...</td>\n      <td>0.023</td>\n      <td>0.871</td>\n      <td>0.106</td>\n      <td>0.9865</td>\n      <td>[-0.067289285, 0.03890233, 0.00046749198, 0.02...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>21</th>\n      <td>P003</td>\n      <td>Comprehensive</td>\n      <td>interviewer right its just its its a yeah its ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.037652</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>{'MD': 3, 'TO': 6, 'NNS': 9, 'CD': 1, 'VBD': 1...</td>\n      <td>0.000</td>\n      <td>0.888</td>\n      <td>0.112</td>\n      <td>0.9689</td>\n      <td>[-0.06344438, 0.0012140614, 0.029845642, 0.003...</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1010 columns</p>\n</div>"},"metadata":{}}],"execution_count":25},{"cell_type":"code","source":"df_new.drop(['Degree of Explanation'], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:19.636395Z","iopub.execute_input":"2024-12-03T16:52:19.636800Z","iopub.status.idle":"2024-12-03T16:52:19.642599Z","shell.execute_reply.started":"2024-12-03T16:52:19.636765Z","shell.execute_reply":"2024-12-03T16:52:19.641607Z"},"scrolled":true},"outputs":[],"execution_count":26},{"cell_type":"code","source":"df_new.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:21.794243Z","iopub.execute_input":"2024-12-03T16:52:21.794628Z","iopub.status.idle":"2024-12-03T16:52:21.800479Z","shell.execute_reply.started":"2024-12-03T16:52:21.794597Z","shell.execute_reply":"2024-12-03T16:52:21.799575Z"},"scrolled":true},"outputs":[{"execution_count":27,"output_type":"execute_result","data":{"text/plain":"(156, 1009)"},"metadata":{}}],"execution_count":27},{"cell_type":"code","source":"# checking for null values\ndf_new.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:26.213552Z","iopub.execute_input":"2024-12-03T16:52:26.213926Z","iopub.status.idle":"2024-12-03T16:52:26.223692Z","shell.execute_reply.started":"2024-12-03T16:52:26.213880Z","shell.execute_reply":"2024-12-03T16:52:26.222631Z"},"scrolled":true},"outputs":[{"execution_count":28,"output_type":"execute_result","data":{"text/plain":"PID                   0\nCleaned_Transcript    0\n15                    0\n20                    0\nability               0\n                     ..\nSentiment_Neu         0\nSentiment_Pos         0\nSentiment_Compound    0\nEmbeddings            0\nDOE_Label             0\nLength: 1009, dtype: int64"},"metadata":{}}],"execution_count":28},{"cell_type":"code","source":"pos_tags_df = pd.json_normalize(df_new['POS_Tags'])\npos_tags_df.fillna(0, inplace=True)  # Replace NaN with 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:29.622403Z","iopub.execute_input":"2024-12-03T16:52:29.622781Z","iopub.status.idle":"2024-12-03T16:52:29.633240Z","shell.execute_reply.started":"2024-12-03T16:52:29.622732Z","shell.execute_reply":"2024-12-03T16:52:29.632202Z"},"scrolled":true},"outputs":[],"execution_count":29},{"cell_type":"code","source":"pos_tags_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:30.506471Z","iopub.execute_input":"2024-12-03T16:52:30.506823Z","iopub.status.idle":"2024-12-03T16:52:30.512496Z","shell.execute_reply.started":"2024-12-03T16:52:30.506793Z","shell.execute_reply":"2024-12-03T16:52:30.511605Z"},"scrolled":true},"outputs":[{"execution_count":30,"output_type":"execute_result","data":{"text/plain":"(156, 33)"},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"# checking for Null values\npos_tags_df.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:36.971438Z","iopub.execute_input":"2024-12-03T16:52:36.971786Z","iopub.status.idle":"2024-12-03T16:52:36.978765Z","shell.execute_reply.started":"2024-12-03T16:52:36.971756Z","shell.execute_reply":"2024-12-03T16:52:36.977625Z"},"scrolled":true},"outputs":[{"execution_count":31,"output_type":"execute_result","data":{"text/plain":"0"},"metadata":{}}],"execution_count":31},{"cell_type":"code","source":"# Check indices of both DataFrames\nprint(df_new.index)\nprint(pos_tags_df.index)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:40.736464Z","iopub.execute_input":"2024-12-03T16:52:40.737343Z","iopub.status.idle":"2024-12-03T16:52:40.742390Z","shell.execute_reply.started":"2024-12-03T16:52:40.737306Z","shell.execute_reply":"2024-12-03T16:52:40.741425Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Index([  6,   8,  12,  14,  21,  27,  31,  34,  36,  38,\n       ...\n       269, 273, 274, 275, 276, 277, 278, 280, 282, 283],\n      dtype='int64', length=156)\nRangeIndex(start=0, stop=156, step=1)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"# Reset indices before concatenation\ndf_new = df_new.reset_index(drop=True)\npos_tags_df = pos_tags_df.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:47.285817Z","iopub.execute_input":"2024-12-03T16:52:47.286712Z","iopub.status.idle":"2024-12-03T16:52:47.292280Z","shell.execute_reply.started":"2024-12-03T16:52:47.286659Z","shell.execute_reply":"2024-12-03T16:52:47.291301Z"},"scrolled":true},"outputs":[],"execution_count":33},{"cell_type":"code","source":"df_new = pd.concat([df_new, pos_tags_df], axis=1)\ndf_new.drop(columns=['POS_Tags'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:51.130816Z","iopub.execute_input":"2024-12-03T16:52:51.131228Z","iopub.status.idle":"2024-12-03T16:52:51.139191Z","shell.execute_reply.started":"2024-12-03T16:52:51.131193Z","shell.execute_reply":"2024-12-03T16:52:51.138060Z"},"scrolled":true},"outputs":[],"execution_count":34},{"cell_type":"code","source":"df_new.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:52.565268Z","iopub.execute_input":"2024-12-03T16:52:52.565609Z","iopub.status.idle":"2024-12-03T16:52:52.574451Z","shell.execute_reply.started":"2024-12-03T16:52:52.565581Z","shell.execute_reply":"2024-12-03T16:52:52.573543Z"},"scrolled":true},"outputs":[{"execution_count":35,"output_type":"execute_result","data":{"text/plain":"PID                   0\nCleaned_Transcript    0\n15                    0\n20                    0\nability               0\n                     ..\nRP                    0\nNNP                   0\nRBS                   0\n$                     0\n''                    0\nLength: 1041, dtype: int64"},"metadata":{}}],"execution_count":35},{"cell_type":"markdown","source":"In order to filter the extracted features, we also need to handle the `Embeddings` column in such a way that each value corresponds to a single feature column.","metadata":{}},{"cell_type":"code","source":"# Expand Embeddings list into individual columns\nembeddings_df = pd.DataFrame(df_new['Embeddings'].to_list(), index=df_new.index)\nembeddings_df.columns = [f'Embedding_{i}' for i in range(embeddings_df.shape[1])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:52:58.669431Z","iopub.execute_input":"2024-12-03T16:52:58.670351Z","iopub.status.idle":"2024-12-03T16:52:58.702598Z","shell.execute_reply.started":"2024-12-03T16:52:58.670314Z","shell.execute_reply":"2024-12-03T16:52:58.701712Z"},"scrolled":true},"outputs":[],"execution_count":36},{"cell_type":"code","source":"embeddings_df.isna().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:00.438462Z","iopub.execute_input":"2024-12-03T16:53:00.439251Z","iopub.status.idle":"2024-12-03T16:53:00.446747Z","shell.execute_reply.started":"2024-12-03T16:53:00.439218Z","shell.execute_reply":"2024-12-03T16:53:00.445707Z"},"scrolled":true},"outputs":[{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"Embedding_0      0\nEmbedding_1      0\nEmbedding_2      0\nEmbedding_3      0\nEmbedding_4      0\n                ..\nEmbedding_379    0\nEmbedding_380    0\nEmbedding_381    0\nEmbedding_382    0\nEmbedding_383    0\nLength: 384, dtype: int64"},"metadata":{}}],"execution_count":37},{"cell_type":"code","source":"df_new = pd.concat([df_new, embeddings_df], axis=1)\ndf_new.drop(columns=['Embeddings'], inplace=True)\ndf_new.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:01.849157Z","iopub.execute_input":"2024-12-03T16:53:01.849861Z","iopub.status.idle":"2024-12-03T16:53:01.879303Z","shell.execute_reply.started":"2024-12-03T16:53:01.849827Z","shell.execute_reply":"2024-12-03T16:53:01.878328Z"},"scrolled":true},"outputs":[{"execution_count":38,"output_type":"execute_result","data":{"text/plain":"    PID                                 Cleaned_Transcript   15   20  ability  \\\n0  P001  interviewer so how did you interviewer what wh...  0.0  0.0      0.0   \n1  P001  interviewer sure yeah so when ah when are you ...  0.0  0.0      0.0   \n2  P002  interviewer yeah having its its sometimes eyeo...  0.0  0.0      0.0   \n3  P002  interviewer okay yeah nope thats fair definite...  0.0  0.0      0.0   \n4  P003  interviewer right its just its its a yeah its ...  0.0  0.0      0.0   \n\n       able     about  above  absolutely  access  ...  Embedding_374  \\\n0  0.000000  0.057687    0.0         0.0     0.0  ...      -0.002607   \n1  0.000000  0.000000    0.0         0.0     0.0  ...      -0.010548   \n2  0.096853  0.000000    0.0         0.0     0.0  ...      -0.040396   \n3  0.000000  0.000000    0.0         0.0     0.0  ...       0.005491   \n4  0.000000  0.037652    0.0         0.0     0.0  ...       0.025108   \n\n   Embedding_375  Embedding_376  Embedding_377  Embedding_378  Embedding_379  \\\n0       0.044532       0.073801       0.060207      -0.051267      -0.041472   \n1       0.054194       0.042425       0.004835       0.015697      -0.032275   \n2       0.013997       0.022638       0.055026      -0.002914      -0.049714   \n3       0.046501       0.082693       0.031852       0.000795      -0.025027   \n4       0.080443      -0.017584       0.008680       0.016954       0.005951   \n\n   Embedding_380  Embedding_381  Embedding_382  Embedding_383  \n0       0.020514       0.062427       0.026892      -0.018947  \n1       0.077989       0.036923      -0.072840      -0.010929  \n2       0.019154       0.043265      -0.055806       0.011732  \n3       0.065218      -0.016551      -0.126233      -0.021982  \n4       0.052375       0.099363      -0.090463       0.018853  \n\n[5 rows x 1424 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>PID</th>\n      <th>Cleaned_Transcript</th>\n      <th>15</th>\n      <th>20</th>\n      <th>ability</th>\n      <th>able</th>\n      <th>about</th>\n      <th>above</th>\n      <th>absolutely</th>\n      <th>access</th>\n      <th>...</th>\n      <th>Embedding_374</th>\n      <th>Embedding_375</th>\n      <th>Embedding_376</th>\n      <th>Embedding_377</th>\n      <th>Embedding_378</th>\n      <th>Embedding_379</th>\n      <th>Embedding_380</th>\n      <th>Embedding_381</th>\n      <th>Embedding_382</th>\n      <th>Embedding_383</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>P001</td>\n      <td>interviewer so how did you interviewer what wh...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.057687</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.002607</td>\n      <td>0.044532</td>\n      <td>0.073801</td>\n      <td>0.060207</td>\n      <td>-0.051267</td>\n      <td>-0.041472</td>\n      <td>0.020514</td>\n      <td>0.062427</td>\n      <td>0.026892</td>\n      <td>-0.018947</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>P001</td>\n      <td>interviewer sure yeah so when ah when are you ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.010548</td>\n      <td>0.054194</td>\n      <td>0.042425</td>\n      <td>0.004835</td>\n      <td>0.015697</td>\n      <td>-0.032275</td>\n      <td>0.077989</td>\n      <td>0.036923</td>\n      <td>-0.072840</td>\n      <td>-0.010929</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>P002</td>\n      <td>interviewer yeah having its its sometimes eyeo...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.096853</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>-0.040396</td>\n      <td>0.013997</td>\n      <td>0.022638</td>\n      <td>0.055026</td>\n      <td>-0.002914</td>\n      <td>-0.049714</td>\n      <td>0.019154</td>\n      <td>0.043265</td>\n      <td>-0.055806</td>\n      <td>0.011732</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>P002</td>\n      <td>interviewer okay yeah nope thats fair definite...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.005491</td>\n      <td>0.046501</td>\n      <td>0.082693</td>\n      <td>0.031852</td>\n      <td>0.000795</td>\n      <td>-0.025027</td>\n      <td>0.065218</td>\n      <td>-0.016551</td>\n      <td>-0.126233</td>\n      <td>-0.021982</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>P003</td>\n      <td>interviewer right its just its its a yeah its ...</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.000000</td>\n      <td>0.037652</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>...</td>\n      <td>0.025108</td>\n      <td>0.080443</td>\n      <td>-0.017584</td>\n      <td>0.008680</td>\n      <td>0.016954</td>\n      <td>0.005951</td>\n      <td>0.052375</td>\n      <td>0.099363</td>\n      <td>-0.090463</td>\n      <td>0.018853</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows × 1424 columns</p>\n</div>"},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"df_new.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:09.658414Z","iopub.execute_input":"2024-12-03T16:53:09.659135Z","iopub.status.idle":"2024-12-03T16:53:09.664534Z","shell.execute_reply.started":"2024-12-03T16:53:09.659102Z","shell.execute_reply":"2024-12-03T16:53:09.663678Z"},"scrolled":true},"outputs":[{"execution_count":39,"output_type":"execute_result","data":{"text/plain":"(156, 1424)"},"metadata":{}}],"execution_count":39},{"cell_type":"code","source":"# creating a list of feature columns\nfeature_columns = (\n    ['Sentiment_Neg', 'Sentiment_Pos', 'Sentiment_Neu', 'Sentiment_Compound'] + \n    pos_tags_df.columns.tolist() + \n    embeddings_df.columns.tolist()\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:14.622714Z","iopub.execute_input":"2024-12-03T16:53:14.623420Z","iopub.status.idle":"2024-12-03T16:53:14.627564Z","shell.execute_reply.started":"2024-12-03T16:53:14.623386Z","shell.execute_reply":"2024-12-03T16:53:14.626609Z"},"scrolled":true},"outputs":[],"execution_count":40},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\ndef select_features_with_mutual_info(df, feature_columns, target_column, k_values):\n    \"\"\"\n    Selects top features for multiple values of k based on mutual information.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe.\n        feature_columns (list): List of feature column names.\n        target_column (str): Name of the target column.\n        k_values (list): List of different k values to experiment with.\n    \n    Returns:\n        dict: Dictionary where keys are k values and values are lists of top k features.\n    \"\"\"\n    X = df[feature_columns]\n    y = df[target_column]\n    \n    # Calculate mutual information scores once\n    mi_scores = mutual_info_classif(X, y, random_state=42)\n    feature_scores = pd.Series(mi_scores, index=feature_columns)\n    \n    # Sort features by mutual information scores in descending order\n    sorted_features = feature_scores.sort_values(ascending=False).index.tolist()\n    \n    # Generate top k features for all k values\n    results = {k: sorted_features[:k] for k in k_values}\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:16.076277Z","iopub.execute_input":"2024-12-03T16:53:16.076673Z","iopub.status.idle":"2024-12-03T16:53:16.146801Z","shell.execute_reply.started":"2024-12-03T16:53:16.076638Z","shell.execute_reply":"2024-12-03T16:53:16.146136Z"},"scrolled":true},"outputs":[],"execution_count":41},{"cell_type":"code","source":"filtered_features = select_features_with_mutual_info(\n    df_new, \n    feature_columns=feature_columns, \n    target_column='DOE_Label', \n    k_values=[100, 200, 400, 800]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:35.448808Z","iopub.execute_input":"2024-12-03T16:53:35.449216Z","iopub.status.idle":"2024-12-03T16:53:36.327956Z","shell.execute_reply.started":"2024-12-03T16:53:35.449179Z","shell.execute_reply":"2024-12-03T16:53:36.327218Z"},"scrolled":true},"outputs":[],"execution_count":42},{"cell_type":"code","source":"# Print results for each k\nfor k, features in filtered_features.items():\n    print(f\"Top {k} features: {features} \\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:38.422896Z","iopub.execute_input":"2024-12-03T16:53:38.423330Z","iopub.status.idle":"2024-12-03T16:53:38.429337Z","shell.execute_reply.started":"2024-12-03T16:53:38.423295Z","shell.execute_reply":"2024-12-03T16:53:38.428307Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Top 100 features: ['Embedding_233', 'Embedding_81', 'Embedding_355', 'PRP', 'Embedding_3', 'VBG', 'RB', 'Embedding_270', 'DT', 'Embedding_278', 'Embedding_277', 'EX', 'Embedding_171', 'Embedding_176', 'Embedding_177', 'Embedding_224', 'Embedding_45', 'Embedding_54', 'Embedding_361', 'Embedding_127', 'VBD', 'Embedding_146', 'Embedding_27', 'Embedding_337', 'Embedding_76', 'Embedding_78', 'Embedding_154', 'Embedding_279', 'Embedding_253', 'Embedding_290', 'Embedding_221', 'VBN', 'Embedding_346', 'Embedding_256', 'Embedding_118', 'Embedding_82', 'NNS', 'NN', 'Embedding_375', 'Embedding_285', 'Embedding_60', 'Embedding_280', 'Embedding_111', 'CC', 'Embedding_69', 'Embedding_305', 'Sentiment_Compound', 'Embedding_31', 'Embedding_330', 'WDT', 'RBR', 'Embedding_295', 'Embedding_39', 'Embedding_377', 'Embedding_363', 'Embedding_75', 'Embedding_52', 'MD', 'Embedding_62', 'Embedding_170', 'Embedding_245', 'Sentiment_Neg', 'Embedding_180', 'Embedding_333', 'Embedding_237', 'Embedding_71', 'Embedding_381', 'Embedding_114', 'Embedding_108', 'Embedding_268', 'Embedding_125', 'PRP$', 'Embedding_134', 'Embedding_274', 'Embedding_196', 'Embedding_353', 'Embedding_334', 'Embedding_181', 'NNP', 'Embedding_369', 'Embedding_371', 'Embedding_130', 'Embedding_124', 'Embedding_286', 'Embedding_263', 'Embedding_88', 'Embedding_208', 'Embedding_210', 'Embedding_349', 'Embedding_136', 'Embedding_205', 'Embedding_321', 'Embedding_236', 'Embedding_148', 'Embedding_331', 'Embedding_174', 'JJ', 'Embedding_169', 'Embedding_77', 'Embedding_301'] \n\nTop 200 features: ['Embedding_233', 'Embedding_81', 'Embedding_355', 'PRP', 'Embedding_3', 'VBG', 'RB', 'Embedding_270', 'DT', 'Embedding_278', 'Embedding_277', 'EX', 'Embedding_171', 'Embedding_176', 'Embedding_177', 'Embedding_224', 'Embedding_45', 'Embedding_54', 'Embedding_361', 'Embedding_127', 'VBD', 'Embedding_146', 'Embedding_27', 'Embedding_337', 'Embedding_76', 'Embedding_78', 'Embedding_154', 'Embedding_279', 'Embedding_253', 'Embedding_290', 'Embedding_221', 'VBN', 'Embedding_346', 'Embedding_256', 'Embedding_118', 'Embedding_82', 'NNS', 'NN', 'Embedding_375', 'Embedding_285', 'Embedding_60', 'Embedding_280', 'Embedding_111', 'CC', 'Embedding_69', 'Embedding_305', 'Sentiment_Compound', 'Embedding_31', 'Embedding_330', 'WDT', 'RBR', 'Embedding_295', 'Embedding_39', 'Embedding_377', 'Embedding_363', 'Embedding_75', 'Embedding_52', 'MD', 'Embedding_62', 'Embedding_170', 'Embedding_245', 'Sentiment_Neg', 'Embedding_180', 'Embedding_333', 'Embedding_237', 'Embedding_71', 'Embedding_381', 'Embedding_114', 'Embedding_108', 'Embedding_268', 'Embedding_125', 'PRP$', 'Embedding_134', 'Embedding_274', 'Embedding_196', 'Embedding_353', 'Embedding_334', 'Embedding_181', 'NNP', 'Embedding_369', 'Embedding_371', 'Embedding_130', 'Embedding_124', 'Embedding_286', 'Embedding_263', 'Embedding_88', 'Embedding_208', 'Embedding_210', 'Embedding_349', 'Embedding_136', 'Embedding_205', 'Embedding_321', 'Embedding_236', 'Embedding_148', 'Embedding_331', 'Embedding_174', 'JJ', 'Embedding_169', 'Embedding_77', 'Embedding_301', 'Embedding_272', 'Embedding_183', 'Embedding_115', 'Embedding_11', 'Embedding_160', 'Embedding_288', 'Embedding_33', 'Embedding_53', 'CD', 'Embedding_310', 'Embedding_220', 'Embedding_217', 'Embedding_165', 'Embedding_344', 'Embedding_259', 'Embedding_201', 'Embedding_193', 'Embedding_202', 'Embedding_248', 'Embedding_80', 'TO', 'Embedding_126', 'Embedding_152', 'Embedding_104', 'Embedding_318', 'VBP', 'Embedding_273', 'Embedding_30', 'Embedding_354', 'Embedding_143', 'Embedding_292', 'Embedding_121', 'Embedding_131', 'Embedding_21', 'Embedding_157', 'Embedding_266', 'Embedding_91', '$', 'RBS', 'Embedding_38', 'Embedding_235', 'Embedding_23', 'Embedding_56', 'Embedding_246', 'Embedding_67', 'Embedding_107', 'Embedding_303', 'Embedding_133', 'Embedding_214', 'Embedding_362', 'Embedding_383', 'Embedding_85', 'Embedding_63', 'Embedding_179', 'Embedding_100', 'Embedding_368', 'Embedding_351', 'Embedding_15', 'IN', 'Embedding_153', 'Embedding_234', 'Embedding_16', 'Embedding_294', 'Embedding_97', 'Embedding_314', 'Embedding_73', 'WP', 'Embedding_291', 'Embedding_26', 'Embedding_265', 'Embedding_204', 'UH', 'Embedding_315', 'Embedding_311', 'Embedding_376', 'Embedding_55', 'Embedding_58', 'Embedding_166', 'Embedding_79', 'Embedding_24', 'Embedding_238', 'Embedding_373', 'Embedding_189', 'Embedding_276', 'Embedding_372', 'Embedding_66', 'Embedding_142', 'Embedding_135', 'Embedding_322', 'Embedding_382', 'Embedding_94', 'Embedding_198', 'FW', 'Embedding_155', 'Embedding_302', 'Embedding_282', 'Embedding_14', 'Embedding_284', 'Embedding_147', 'Embedding_0'] \n\nTop 400 features: ['Embedding_233', 'Embedding_81', 'Embedding_355', 'PRP', 'Embedding_3', 'VBG', 'RB', 'Embedding_270', 'DT', 'Embedding_278', 'Embedding_277', 'EX', 'Embedding_171', 'Embedding_176', 'Embedding_177', 'Embedding_224', 'Embedding_45', 'Embedding_54', 'Embedding_361', 'Embedding_127', 'VBD', 'Embedding_146', 'Embedding_27', 'Embedding_337', 'Embedding_76', 'Embedding_78', 'Embedding_154', 'Embedding_279', 'Embedding_253', 'Embedding_290', 'Embedding_221', 'VBN', 'Embedding_346', 'Embedding_256', 'Embedding_118', 'Embedding_82', 'NNS', 'NN', 'Embedding_375', 'Embedding_285', 'Embedding_60', 'Embedding_280', 'Embedding_111', 'CC', 'Embedding_69', 'Embedding_305', 'Sentiment_Compound', 'Embedding_31', 'Embedding_330', 'WDT', 'RBR', 'Embedding_295', 'Embedding_39', 'Embedding_377', 'Embedding_363', 'Embedding_75', 'Embedding_52', 'MD', 'Embedding_62', 'Embedding_170', 'Embedding_245', 'Sentiment_Neg', 'Embedding_180', 'Embedding_333', 'Embedding_237', 'Embedding_71', 'Embedding_381', 'Embedding_114', 'Embedding_108', 'Embedding_268', 'Embedding_125', 'PRP$', 'Embedding_134', 'Embedding_274', 'Embedding_196', 'Embedding_353', 'Embedding_334', 'Embedding_181', 'NNP', 'Embedding_369', 'Embedding_371', 'Embedding_130', 'Embedding_124', 'Embedding_286', 'Embedding_263', 'Embedding_88', 'Embedding_208', 'Embedding_210', 'Embedding_349', 'Embedding_136', 'Embedding_205', 'Embedding_321', 'Embedding_236', 'Embedding_148', 'Embedding_331', 'Embedding_174', 'JJ', 'Embedding_169', 'Embedding_77', 'Embedding_301', 'Embedding_272', 'Embedding_183', 'Embedding_115', 'Embedding_11', 'Embedding_160', 'Embedding_288', 'Embedding_33', 'Embedding_53', 'CD', 'Embedding_310', 'Embedding_220', 'Embedding_217', 'Embedding_165', 'Embedding_344', 'Embedding_259', 'Embedding_201', 'Embedding_193', 'Embedding_202', 'Embedding_248', 'Embedding_80', 'TO', 'Embedding_126', 'Embedding_152', 'Embedding_104', 'Embedding_318', 'VBP', 'Embedding_273', 'Embedding_30', 'Embedding_354', 'Embedding_143', 'Embedding_292', 'Embedding_121', 'Embedding_131', 'Embedding_21', 'Embedding_157', 'Embedding_266', 'Embedding_91', '$', 'RBS', 'Embedding_38', 'Embedding_235', 'Embedding_23', 'Embedding_56', 'Embedding_246', 'Embedding_67', 'Embedding_107', 'Embedding_303', 'Embedding_133', 'Embedding_214', 'Embedding_362', 'Embedding_383', 'Embedding_85', 'Embedding_63', 'Embedding_179', 'Embedding_100', 'Embedding_368', 'Embedding_351', 'Embedding_15', 'IN', 'Embedding_153', 'Embedding_234', 'Embedding_16', 'Embedding_294', 'Embedding_97', 'Embedding_314', 'Embedding_73', 'WP', 'Embedding_291', 'Embedding_26', 'Embedding_265', 'Embedding_204', 'UH', 'Embedding_315', 'Embedding_311', 'Embedding_376', 'Embedding_55', 'Embedding_58', 'Embedding_166', 'Embedding_79', 'Embedding_24', 'Embedding_238', 'Embedding_373', 'Embedding_189', 'Embedding_276', 'Embedding_372', 'Embedding_66', 'Embedding_142', 'Embedding_135', 'Embedding_322', 'Embedding_382', 'Embedding_94', 'Embedding_198', 'FW', 'Embedding_155', 'Embedding_302', 'Embedding_282', 'Embedding_14', 'Embedding_284', 'Embedding_147', 'Embedding_0', 'Embedding_42', 'Embedding_87', 'Embedding_343', 'Embedding_209', 'Embedding_222', 'Embedding_164', 'Embedding_40', 'Embedding_364', 'Embedding_241', 'Embedding_257', 'Embedding_120', 'Embedding_255', 'Embedding_254', 'Embedding_258', 'Embedding_260', 'Embedding_119', 'Embedding_117', 'Embedding_122', 'Embedding_116', 'Embedding_262', 'Embedding_264', 'Embedding_267', 'Embedding_269', 'Embedding_271', 'Embedding_275', 'Embedding_113', 'Embedding_281', 'Embedding_110', 'Embedding_261', 'Embedding_137', 'Embedding_223', 'Embedding_225', 'Embedding_226', 'Embedding_227', 'Embedding_228', 'Embedding_229', 'Embedding_132', 'Embedding_230', 'Embedding_129', 'Embedding_231', 'Embedding_250', 'Embedding_239', 'Embedding_128', 'Embedding_240', 'Embedding_123', 'Embedding_242', 'Embedding_243', 'Embedding_244', 'Embedding_247', 'Embedding_249', 'Embedding_251', 'Embedding_252', 'Embedding_232', 'Embedding_95', 'Embedding_306', 'Embedding_307', 'Embedding_308', 'Embedding_309', 'Embedding_312', 'Embedding_313', 'Embedding_316', \"''\", 'Embedding_1', 'Embedding_2', 'Embedding_4', 'Embedding_112', 'VBZ', 'WRB', 'JJR', 'PDT', 'JJS', 'RP', 'Embedding_5', 'Embedding_6', 'Embedding_7', 'Embedding_8', 'Embedding_9', 'Embedding_10', 'Embedding_13', 'VB', 'Embedding_283', 'Embedding_287', 'Embedding_289', 'Embedding_293', 'Embedding_296', 'Embedding_297', 'Embedding_298', 'Embedding_299', 'Embedding_300', 'Embedding_304', 'Embedding_374', 'Embedding_378', 'Embedding_379', 'Embedding_93', 'Embedding_61', 'Embedding_64', 'Embedding_65', 'Embedding_68', 'Embedding_70', 'Embedding_72', 'Embedding_74', 'Embedding_83', 'Embedding_84', 'Embedding_86', 'Embedding_89', 'Embedding_90', 'Embedding_92', 'Embedding_380', 'Embedding_326', 'Embedding_327', 'Embedding_328', 'Embedding_329', 'Embedding_332', 'Embedding_335', 'Embedding_336', 'Embedding_338', 'Embedding_339', 'Embedding_340', 'Embedding_341', 'Embedding_342', 'Embedding_345', 'Embedding_12', 'Embedding_139', 'Embedding_140', 'Embedding_141', 'Embedding_144', 'Embedding_145', 'Embedding_149', 'Embedding_150', 'Embedding_151', 'Embedding_156', 'Embedding_158', 'Embedding_159', 'Embedding_161', 'Embedding_162', 'Embedding_138', 'Embedding_17', 'Embedding_18', 'Embedding_347', 'Embedding_348', 'Sentiment_Pos', 'Sentiment_Neu', 'Embedding_350', 'Embedding_352', 'Embedding_356', 'Embedding_357', 'Embedding_358', 'Embedding_359', 'Embedding_360', 'Embedding_325', 'Embedding_366', 'Embedding_367', 'Embedding_370', 'Embedding_19', 'Embedding_20', 'Embedding_22', 'Embedding_25', 'Embedding_28', 'Embedding_317', 'Embedding_319', 'Embedding_320', 'Embedding_323', 'Embedding_324', 'Embedding_365', 'Embedding_190', 'Embedding_191', 'Embedding_192', 'Embedding_194', 'Embedding_195', 'Embedding_197', 'Embedding_199', 'Embedding_200', 'Embedding_203', 'Embedding_206', 'Embedding_207', 'Embedding_211', 'Embedding_212', 'Embedding_163', 'Embedding_215', 'Embedding_216', 'Embedding_218', 'Embedding_219', 'Embedding_96', 'Embedding_98', 'Embedding_99', 'Embedding_101', 'Embedding_102', 'Embedding_103', 'Embedding_105', 'Embedding_106', 'Embedding_109', 'Embedding_213', 'Embedding_167', 'Embedding_168', 'Embedding_172', 'Embedding_173', 'Embedding_175', 'Embedding_178', 'Embedding_182', 'Embedding_184'] \n\nTop 800 features: ['Embedding_233', 'Embedding_81', 'Embedding_355', 'PRP', 'Embedding_3', 'VBG', 'RB', 'Embedding_270', 'DT', 'Embedding_278', 'Embedding_277', 'EX', 'Embedding_171', 'Embedding_176', 'Embedding_177', 'Embedding_224', 'Embedding_45', 'Embedding_54', 'Embedding_361', 'Embedding_127', 'VBD', 'Embedding_146', 'Embedding_27', 'Embedding_337', 'Embedding_76', 'Embedding_78', 'Embedding_154', 'Embedding_279', 'Embedding_253', 'Embedding_290', 'Embedding_221', 'VBN', 'Embedding_346', 'Embedding_256', 'Embedding_118', 'Embedding_82', 'NNS', 'NN', 'Embedding_375', 'Embedding_285', 'Embedding_60', 'Embedding_280', 'Embedding_111', 'CC', 'Embedding_69', 'Embedding_305', 'Sentiment_Compound', 'Embedding_31', 'Embedding_330', 'WDT', 'RBR', 'Embedding_295', 'Embedding_39', 'Embedding_377', 'Embedding_363', 'Embedding_75', 'Embedding_52', 'MD', 'Embedding_62', 'Embedding_170', 'Embedding_245', 'Sentiment_Neg', 'Embedding_180', 'Embedding_333', 'Embedding_237', 'Embedding_71', 'Embedding_381', 'Embedding_114', 'Embedding_108', 'Embedding_268', 'Embedding_125', 'PRP$', 'Embedding_134', 'Embedding_274', 'Embedding_196', 'Embedding_353', 'Embedding_334', 'Embedding_181', 'NNP', 'Embedding_369', 'Embedding_371', 'Embedding_130', 'Embedding_124', 'Embedding_286', 'Embedding_263', 'Embedding_88', 'Embedding_208', 'Embedding_210', 'Embedding_349', 'Embedding_136', 'Embedding_205', 'Embedding_321', 'Embedding_236', 'Embedding_148', 'Embedding_331', 'Embedding_174', 'JJ', 'Embedding_169', 'Embedding_77', 'Embedding_301', 'Embedding_272', 'Embedding_183', 'Embedding_115', 'Embedding_11', 'Embedding_160', 'Embedding_288', 'Embedding_33', 'Embedding_53', 'CD', 'Embedding_310', 'Embedding_220', 'Embedding_217', 'Embedding_165', 'Embedding_344', 'Embedding_259', 'Embedding_201', 'Embedding_193', 'Embedding_202', 'Embedding_248', 'Embedding_80', 'TO', 'Embedding_126', 'Embedding_152', 'Embedding_104', 'Embedding_318', 'VBP', 'Embedding_273', 'Embedding_30', 'Embedding_354', 'Embedding_143', 'Embedding_292', 'Embedding_121', 'Embedding_131', 'Embedding_21', 'Embedding_157', 'Embedding_266', 'Embedding_91', '$', 'RBS', 'Embedding_38', 'Embedding_235', 'Embedding_23', 'Embedding_56', 'Embedding_246', 'Embedding_67', 'Embedding_107', 'Embedding_303', 'Embedding_133', 'Embedding_214', 'Embedding_362', 'Embedding_383', 'Embedding_85', 'Embedding_63', 'Embedding_179', 'Embedding_100', 'Embedding_368', 'Embedding_351', 'Embedding_15', 'IN', 'Embedding_153', 'Embedding_234', 'Embedding_16', 'Embedding_294', 'Embedding_97', 'Embedding_314', 'Embedding_73', 'WP', 'Embedding_291', 'Embedding_26', 'Embedding_265', 'Embedding_204', 'UH', 'Embedding_315', 'Embedding_311', 'Embedding_376', 'Embedding_55', 'Embedding_58', 'Embedding_166', 'Embedding_79', 'Embedding_24', 'Embedding_238', 'Embedding_373', 'Embedding_189', 'Embedding_276', 'Embedding_372', 'Embedding_66', 'Embedding_142', 'Embedding_135', 'Embedding_322', 'Embedding_382', 'Embedding_94', 'Embedding_198', 'FW', 'Embedding_155', 'Embedding_302', 'Embedding_282', 'Embedding_14', 'Embedding_284', 'Embedding_147', 'Embedding_0', 'Embedding_42', 'Embedding_87', 'Embedding_343', 'Embedding_209', 'Embedding_222', 'Embedding_164', 'Embedding_40', 'Embedding_364', 'Embedding_241', 'Embedding_257', 'Embedding_120', 'Embedding_255', 'Embedding_254', 'Embedding_258', 'Embedding_260', 'Embedding_119', 'Embedding_117', 'Embedding_122', 'Embedding_116', 'Embedding_262', 'Embedding_264', 'Embedding_267', 'Embedding_269', 'Embedding_271', 'Embedding_275', 'Embedding_113', 'Embedding_281', 'Embedding_110', 'Embedding_261', 'Embedding_137', 'Embedding_223', 'Embedding_225', 'Embedding_226', 'Embedding_227', 'Embedding_228', 'Embedding_229', 'Embedding_132', 'Embedding_230', 'Embedding_129', 'Embedding_231', 'Embedding_250', 'Embedding_239', 'Embedding_128', 'Embedding_240', 'Embedding_123', 'Embedding_242', 'Embedding_243', 'Embedding_244', 'Embedding_247', 'Embedding_249', 'Embedding_251', 'Embedding_252', 'Embedding_232', 'Embedding_95', 'Embedding_306', 'Embedding_307', 'Embedding_308', 'Embedding_309', 'Embedding_312', 'Embedding_313', 'Embedding_316', \"''\", 'Embedding_1', 'Embedding_2', 'Embedding_4', 'Embedding_112', 'VBZ', 'WRB', 'JJR', 'PDT', 'JJS', 'RP', 'Embedding_5', 'Embedding_6', 'Embedding_7', 'Embedding_8', 'Embedding_9', 'Embedding_10', 'Embedding_13', 'VB', 'Embedding_283', 'Embedding_287', 'Embedding_289', 'Embedding_293', 'Embedding_296', 'Embedding_297', 'Embedding_298', 'Embedding_299', 'Embedding_300', 'Embedding_304', 'Embedding_374', 'Embedding_378', 'Embedding_379', 'Embedding_93', 'Embedding_61', 'Embedding_64', 'Embedding_65', 'Embedding_68', 'Embedding_70', 'Embedding_72', 'Embedding_74', 'Embedding_83', 'Embedding_84', 'Embedding_86', 'Embedding_89', 'Embedding_90', 'Embedding_92', 'Embedding_380', 'Embedding_326', 'Embedding_327', 'Embedding_328', 'Embedding_329', 'Embedding_332', 'Embedding_335', 'Embedding_336', 'Embedding_338', 'Embedding_339', 'Embedding_340', 'Embedding_341', 'Embedding_342', 'Embedding_345', 'Embedding_12', 'Embedding_139', 'Embedding_140', 'Embedding_141', 'Embedding_144', 'Embedding_145', 'Embedding_149', 'Embedding_150', 'Embedding_151', 'Embedding_156', 'Embedding_158', 'Embedding_159', 'Embedding_161', 'Embedding_162', 'Embedding_138', 'Embedding_17', 'Embedding_18', 'Embedding_347', 'Embedding_348', 'Sentiment_Pos', 'Sentiment_Neu', 'Embedding_350', 'Embedding_352', 'Embedding_356', 'Embedding_357', 'Embedding_358', 'Embedding_359', 'Embedding_360', 'Embedding_325', 'Embedding_366', 'Embedding_367', 'Embedding_370', 'Embedding_19', 'Embedding_20', 'Embedding_22', 'Embedding_25', 'Embedding_28', 'Embedding_317', 'Embedding_319', 'Embedding_320', 'Embedding_323', 'Embedding_324', 'Embedding_365', 'Embedding_190', 'Embedding_191', 'Embedding_192', 'Embedding_194', 'Embedding_195', 'Embedding_197', 'Embedding_199', 'Embedding_200', 'Embedding_203', 'Embedding_206', 'Embedding_207', 'Embedding_211', 'Embedding_212', 'Embedding_163', 'Embedding_215', 'Embedding_216', 'Embedding_218', 'Embedding_219', 'Embedding_96', 'Embedding_98', 'Embedding_99', 'Embedding_101', 'Embedding_102', 'Embedding_103', 'Embedding_105', 'Embedding_106', 'Embedding_109', 'Embedding_213', 'Embedding_167', 'Embedding_168', 'Embedding_172', 'Embedding_173', 'Embedding_175', 'Embedding_178', 'Embedding_182', 'Embedding_184', 'Embedding_185', 'Embedding_186', 'Embedding_187', 'Embedding_188', 'Embedding_29', 'Embedding_59', 'Embedding_34', 'Embedding_35', 'Embedding_36', 'Embedding_37', 'Embedding_41', 'Embedding_43', 'Embedding_44', 'Embedding_46', 'Embedding_47', 'Embedding_48', 'Embedding_49', 'Embedding_50', 'Embedding_51', 'Embedding_57', 'Embedding_32'] \n\n","output_type":"stream"}],"execution_count":43},{"cell_type":"markdown","source":"## Splitting the Dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GroupKFold\n\ndef participant_independent_split(df, feature_columns, target_column, group_column, n_splits=5):\n    \"\"\"\n    Splits the dataset into participant-independent folds using GroupKFold.\n    \n    Parameters:\n        df (pd.DataFrame): The input dataset containing features, target, and group columns.\n        feature_columns (list): List of column names to be used as features.\n        target_column (str): Name of the target column.\n        group_column (str): Name of the column used for grouping (e.g., Participant ID).\n        n_splits (int): Number of folds (default is 5).\n    \n    Returns:\n        list of tuples: Each tuple contains (X_train, X_test, y_train, y_test) for one fold.\n    \"\"\"\n    # Extract features, target, and groups\n    X = df[feature_columns]\n    y = df[target_column]\n    groups = df[group_column]\n    \n    # Initialize GroupKFold\n    gkf = GroupKFold(n_splits=n_splits)\n    \n    # Store splits\n    splits = []\n    for train_idx, test_idx in gkf.split(X, y, groups):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        splits.append((X_train, X_test, y_train, y_test))\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:53:56.374860Z","iopub.execute_input":"2024-12-03T16:53:56.375602Z","iopub.status.idle":"2024-12-03T16:53:56.382168Z","shell.execute_reply.started":"2024-12-03T16:53:56.375565Z","shell.execute_reply":"2024-12-03T16:53:56.381122Z"},"scrolled":true},"outputs":[],"execution_count":44},{"cell_type":"code","source":"target_column = 'DOE_Label'  # Replace with the actual target column name\ngroup_column = 'PID'  # Replace with the participant ID column name\n\n# Call the function\nfolds = participant_independent_split(df_new, feature_columns, target_column, group_column, n_splits=5)\n\n# Display train and test sets for each fold\nfor i, (X_train, X_test, y_train, y_test) in enumerate(folds):\n    print(f\"Fold {i+1}\")\n    print(\"Train PIDs:\", df_new[group_column].iloc[X_train.index].unique())\n    print(\"Test PIDs:\", df_new[group_column].iloc[X_test.index].unique())\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:54:03.957183Z","iopub.execute_input":"2024-12-03T16:54:03.958177Z","iopub.status.idle":"2024-12-03T16:54:03.975856Z","shell.execute_reply.started":"2024-12-03T16:54:03.958137Z","shell.execute_reply":"2024-12-03T16:54:03.975056Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Fold 1\nTrain PIDs: ['P001' 'P002' 'P003' 'P004' 'P005' 'P006' 'P007' 'P008' 'P010' 'P011'\n 'P014' 'P016' 'P018' 'P019' 'P024' 'P025' 'P026' 'P028' 'P030' 'P031'\n 'P032' 'P033' 'P034' 'P035' 'P037' 'P038' 'P039' 'P041']\nTest PIDs: ['P009' 'P012' 'P015' 'P023' 'P027' 'P029' 'P036' 'P040']\n--------------------------------------------------\nFold 2\nTrain PIDs: ['P001' 'P002' 'P003' 'P005' 'P006' 'P008' 'P009' 'P010' 'P011' 'P012'\n 'P015' 'P016' 'P018' 'P019' 'P023' 'P024' 'P027' 'P028' 'P029' 'P032'\n 'P033' 'P034' 'P035' 'P036' 'P037' 'P038' 'P039' 'P040' 'P041']\nTest PIDs: ['P004' 'P007' 'P014' 'P025' 'P026' 'P030' 'P031']\n--------------------------------------------------\nFold 3\nTrain PIDs: ['P001' 'P003' 'P004' 'P006' 'P007' 'P009' 'P011' 'P012' 'P014' 'P015'\n 'P016' 'P018' 'P019' 'P023' 'P024' 'P025' 'P026' 'P027' 'P028' 'P029'\n 'P030' 'P031' 'P033' 'P035' 'P036' 'P037' 'P038' 'P039' 'P040']\nTest PIDs: ['P002' 'P005' 'P008' 'P010' 'P032' 'P034' 'P041']\n--------------------------------------------------\nFold 4\nTrain PIDs: ['P001' 'P002' 'P004' 'P005' 'P006' 'P007' 'P008' 'P009' 'P010' 'P011'\n 'P012' 'P014' 'P015' 'P016' 'P018' 'P023' 'P025' 'P026' 'P027' 'P028'\n 'P029' 'P030' 'P031' 'P032' 'P034' 'P036' 'P037' 'P040' 'P041']\nTest PIDs: ['P003' 'P019' 'P024' 'P033' 'P035' 'P038' 'P039']\n--------------------------------------------------\nFold 5\nTrain PIDs: ['P002' 'P003' 'P004' 'P005' 'P007' 'P008' 'P009' 'P010' 'P012' 'P014'\n 'P015' 'P019' 'P023' 'P024' 'P025' 'P026' 'P027' 'P029' 'P030' 'P031'\n 'P032' 'P033' 'P034' 'P035' 'P036' 'P038' 'P039' 'P040' 'P041']\nTest PIDs: ['P001' 'P006' 'P011' 'P016' 'P018' 'P028' 'P037']\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":45},{"cell_type":"code","source":"# check the number of samples in each fold\nfor i, (X_train, X_test, y_train, y_test) in enumerate(folds):\n    print(f\"Fold {i+1}:\")\n    print(f\"  Train sample shapes: {X_train.shape}\")\n    print(f\"  Test samples shapes: {X_test.shape}\")\n    print(\"-\" * 50)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:54:10.689191Z","iopub.execute_input":"2024-12-03T16:54:10.689528Z","iopub.status.idle":"2024-12-03T16:54:10.695272Z","shell.execute_reply.started":"2024-12-03T16:54:10.689500Z","shell.execute_reply":"2024-12-03T16:54:10.694326Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Fold 1:\n  Train sample shapes: (124, 421)\n  Test samples shapes: (32, 421)\n--------------------------------------------------\nFold 2:\n  Train sample shapes: (125, 421)\n  Test samples shapes: (31, 421)\n--------------------------------------------------\nFold 3:\n  Train sample shapes: (125, 421)\n  Test samples shapes: (31, 421)\n--------------------------------------------------\nFold 4:\n  Train sample shapes: (125, 421)\n  Test samples shapes: (31, 421)\n--------------------------------------------------\nFold 5:\n  Train sample shapes: (125, 421)\n  Test samples shapes: (31, 421)\n--------------------------------------------------\n","output_type":"stream"}],"execution_count":46},{"cell_type":"markdown","source":"## Running Tree-Based ML Models","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\n\ndef evaluate_tree_models(folds, k_features, feature_columns, target_column, top_features_dict):\n    \"\"\"\n    Evaluates three tree-based models (Decision Tree, Random Forest, Gradient Boosting)\n    with hyperparameter tuning for each k value and identifies the best model.\n\n    Parameters:\n        folds (list of tuples): Participant-independent splits (X_train, X_test, y_train, y_test).\n        k_features (list): List of k values representing the number of top features to use.\n        feature_columns (list): List of feature column names.\n        target_column (str): Name of the target column.\n        top_features_dict (dict): Dictionary mapping each k value to the list of top features.\n\n    Returns:\n        dict: Results for each k value containing the best model and its performance metrics.\n    \"\"\"\n    # Define the models and their hyperparameter grids\n    models = {\n        \"Decision Tree\": {\n            \"model\": DecisionTreeClassifier(random_state=42),\n            \"param_grid\": {\"max_depth\": [3, 5, 10, None], \"min_samples_split\": [2, 5, 10]},\n        },\n        \"Random Forest\": {\n            \"model\": RandomForestClassifier(random_state=42),\n            \"param_grid\": {\n                \"n_estimators\": [50, 100, 200],\n                \"max_depth\": [3, 5, 10, None],\n                \"min_samples_split\": [2, 5, 10],\n            },\n        },\n        \"Gradient Boosting\": {\n            \"model\": GradientBoostingClassifier(random_state=42),\n            \"param_grid\": {\n                \"n_estimators\": [50, 100, 200],\n                \"learning_rate\": [0.01, 0.1, 0.2],\n                \"max_depth\": [3, 5, 10],\n            },\n        },\n    }\n\n    results = {}\n\n    # Iterate over each k value\n    for k in k_features:\n        print(f\"Evaluating models for top {k} features...\")\n        top_features = top_features_dict[k]  # Get the top k features\n\n        best_model_name = None\n        best_model = None\n        best_accuracy = 0\n        best_balanced_accuracy = 0\n        best_params = None\n\n        # Iterate over models\n        for model_name, model_info in models.items():\n            print(f\"Training {model_name}...\")\n            total_accuracy = 0\n            total_balanced_accuracy = 0\n\n            # Iterate over folds\n            for X_train, X_test, y_train, y_test in folds:\n                # Filter top k features for the current fold\n                X_train_k = X_train[top_features]\n                X_test_k = X_test[top_features]\n\n                # Perform grid search\n                grid_search = GridSearchCV(\n                    model_info[\"model\"],\n                    model_info[\"param_grid\"],\n                    scoring=\"accuracy\",\n                    cv=3,\n                    n_jobs=-1,\n                )\n                grid_search.fit(X_train_k, y_train)\n\n                # Evaluate on test set\n                best_estimator = grid_search.best_estimator_\n                y_pred = best_estimator.predict(X_test_k)\n                accuracy = accuracy_score(y_test, y_pred)\n                balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n\n                # Accumulate scores\n                total_accuracy += accuracy\n                total_balanced_accuracy += balanced_accuracy\n\n            # Average scores over all folds\n            avg_accuracy = total_accuracy / len(folds)\n            avg_balanced_accuracy = total_balanced_accuracy / len(folds)\n\n            print(f\"{model_name}: Accuracy={avg_accuracy:.4f}, Balanced Accuracy={avg_balanced_accuracy:.4f} \\n\")\n\n            # Update best model if this model performs better\n            if (avg_accuracy > best_accuracy) and (avg_balanced_accuracy > best_balanced_accuracy):\n                best_model_name = model_name\n                best_model = grid_search.best_estimator_\n                best_accuracy = avg_accuracy\n                best_balanced_accuracy = avg_balanced_accuracy\n                best_params = grid_search.best_params_\n\n        # Store results for this k value\n        results[k] = {\n            \"Best Model\": best_model_name,\n            \"Best Accuracy\": best_accuracy,\n            \"Best Balanced Accuracy\": best_balanced_accuracy,\n            \"Best Parameters\": best_params,\n            \"Best Model Object\": best_model,\n        }\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:54:53.124180Z","iopub.execute_input":"2024-12-03T16:54:53.124555Z","iopub.status.idle":"2024-12-03T16:54:53.214837Z","shell.execute_reply.started":"2024-12-03T16:54:53.124525Z","shell.execute_reply":"2024-12-03T16:54:53.213913Z"},"scrolled":true},"outputs":[],"execution_count":47},{"cell_type":"code","source":"k_values_list = list(filtered_features.keys())\nprint(k_values_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:54:56.357212Z","iopub.execute_input":"2024-12-03T16:54:56.357937Z","iopub.status.idle":"2024-12-03T16:54:56.362886Z","shell.execute_reply.started":"2024-12-03T16:54:56.357903Z","shell.execute_reply":"2024-12-03T16:54:56.361946Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"[100, 200, 400, 800]\n","output_type":"stream"}],"execution_count":48},{"cell_type":"code","source":"# Example call\nresults = evaluate_tree_models(\n    folds=folds,\n    k_features=k_values_list,\n    feature_columns=feature_columns,\n    target_column=target_column,\n    top_features_dict=filtered_features\n)\n\n# Display the best model for each k\nfor k, result in results.items():\n    print(f\"Top {k} features:\")\n    print(f\"Best Model: {result['Best Model']}\")\n    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\")\n    print(f\"Best Parameters: {result['Best Parameters']}\")\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T16:55:04.551978Z","iopub.execute_input":"2024-12-03T16:55:04.552391Z","iopub.status.idle":"2024-12-03T17:06:13.688235Z","shell.execute_reply.started":"2024-12-03T16:55:04.552356Z","shell.execute_reply":"2024-12-03T17:06:13.687130Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Evaluating models for top 100 features...\nTraining Decision Tree...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n/opt/conda/lib/python3.10/site-packages/joblib/externals/loky/backend/fork_exec.py:38: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid = os.fork()\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\nhuggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Decision Tree: Accuracy=0.7179, Balanced Accuracy=0.5482 \n\nTraining Random Forest...\nRandom Forest: Accuracy=0.8073, Balanced Accuracy=0.5959 \n\nTraining Gradient Boosting...\nGradient Boosting: Accuracy=0.7683, Balanced Accuracy=0.5121 \n\nEvaluating models for top 200 features...\nTraining Decision Tree...\nDecision Tree: Accuracy=0.7111, Balanced Accuracy=0.5231 \n\nTraining Random Forest...\nRandom Forest: Accuracy=0.7879, Balanced Accuracy=0.5514 \n\nTraining Gradient Boosting...\nGradient Boosting: Accuracy=0.7496, Balanced Accuracy=0.4813 \n\nEvaluating models for top 400 features...\nTraining Decision Tree...\nDecision Tree: Accuracy=0.7242, Balanced Accuracy=0.5179 \n\nTraining Random Forest...\nRandom Forest: Accuracy=0.7877, Balanced Accuracy=0.5424 \n\nTraining Gradient Boosting...\nGradient Boosting: Accuracy=0.7750, Balanced Accuracy=0.4966 \n\nEvaluating models for top 800 features...\nTraining Decision Tree...\nDecision Tree: Accuracy=0.6851, Balanced Accuracy=0.4892 \n\nTraining Random Forest...\nRandom Forest: Accuracy=0.7752, Balanced Accuracy=0.5056 \n\nTraining Gradient Boosting...\nGradient Boosting: Accuracy=0.7623, Balanced Accuracy=0.4969 \n\nTop 100 features:\nBest Model: Random Forest\nBest Accuracy: 0.8073\nBest Balanced Accuracy: 0.5959\nBest Parameters: {'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 200}\n\n\nTop 200 features:\nBest Model: Random Forest\nBest Accuracy: 0.7879\nBest Balanced Accuracy: 0.5514\nBest Parameters: {'max_depth': 3, 'min_samples_split': 2, 'n_estimators': 100}\n\n\nTop 400 features:\nBest Model: Random Forest\nBest Accuracy: 0.7877\nBest Balanced Accuracy: 0.5424\nBest Parameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 50}\n\n\nTop 800 features:\nBest Model: Random Forest\nBest Accuracy: 0.7752\nBest Balanced Accuracy: 0.5056\nBest Parameters: {'max_depth': 3, 'min_samples_split': 10, 'n_estimators': 200}\n\n\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"import joblib\n\n# Iterate over results to save the best model for each k\nfor k, result in results.items():\n    best_model = result[\"Best Model Object\"]  # Get the best model object\n    filename = f\"partd_best_ml_model_top_{k}_features.pkl\"  # Define a filename\n    joblib.dump(best_model, filename)  # Save the model\n    print(f\"Saved best model for top {k} features as '{filename}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:06:39.220412Z","iopub.execute_input":"2024-12-03T17:06:39.220812Z","iopub.status.idle":"2024-12-03T17:06:39.434568Z","shell.execute_reply.started":"2024-12-03T17:06:39.220777Z","shell.execute_reply":"2024-12-03T17:06:39.433665Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Saved best model for top 100 features as 'partd_best_ml_model_top_100_features.pkl'.\nSaved best model for top 200 features as 'partd_best_ml_model_top_200_features.pkl'.\nSaved best model for top 400 features as 'partd_best_ml_model_top_400_features.pkl'.\nSaved best model for top 800 features as 'partd_best_ml_model_top_800_features.pkl'.\n","output_type":"stream"}],"execution_count":50},{"cell_type":"markdown","source":"## Running Deep Learning Models","metadata":{}},{"cell_type":"markdown","source":"In order to run deep learning models such as `Conv1D` and `LSTM`, we first need to scale and reshape the input dataset because these models require their inputs to be in a specific format and shape.","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ndef prepare_data_for_dl(X, top_features):\n    \"\"\"\n    Scales and reshapes data for Conv1D and LSTM models.\n\n    Parameters:\n        X (pd.DataFrame): Input features.\n        top_features (list): List of selected top features.\n\n    Returns:\n        np.ndarray: Scaled and reshaped data.\n    \"\"\"\n    scaler = MinMaxScaler()\n    X_scaled = scaler.fit_transform(X[top_features])\n    \n    # Reshape for Conv1D and LSTM: (samples, timesteps, features)\n    X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n    return X_reshaped","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:06:50.409784Z","iopub.execute_input":"2024-12-03T17:06:50.410188Z","iopub.status.idle":"2024-12-03T17:06:50.415777Z","shell.execute_reply.started":"2024-12-03T17:06:50.410155Z","shell.execute_reply":"2024-12-03T17:06:50.414826Z"},"scrolled":true},"outputs":[],"execution_count":51},{"cell_type":"markdown","source":"The above function will be used before training and evaluating the models on each participant-independent fold.","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\n\ndef train_and_evaluate_dl_models(folds, top_features_dict, model_type=\"Conv1D\"):\n    \"\"\"\n    Trains and evaluates Conv1D or LSTM models on participant-independent folds without hyperparameter tuning.\n    Dataset loading is handled via TensorFlow tensor slices.\n\n    Parameters:\n        folds (list of tuples): Train-test splits from GroupKFold.\n        top_features_dict (dict): Dictionary of top features.\n        model_type (str): \"Conv1D\" or \"LSTM\".\n\n    Returns:\n        dict: Results for each k value containing the model and its performance metrics.\n    \"\"\"\n    results = {}\n\n    for k, top_features in top_features_dict.items():\n        print(f\"Evaluating models for top {k} features...\")\n        \n        best_model = None\n        best_accuracy = 0\n        best_balanced_accuracy = 0\n\n        # Iterate over folds\n        for fold, (X_train, X_test, y_train, y_test) in enumerate(folds):\n            print(f\"Training on Fold {fold + 1}...\")\n\n            # Filter dataset for top features\n            X_train = X_train[top_features]\n            X_test = X_test[top_features]\n\n            # Create TensorFlow datasets\n            train_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).batch(32).prefetch(tf.data.AUTOTUNE)\n            test_ds = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values)).batch(32).prefetch(tf.data.AUTOTUNE)\n\n            # Prepare input shape\n            input_shape = (len(top_features), 1)  # (timesteps, features)\n\n            # Build model\n            if model_type == \"Conv1D\":\n                model = Sequential([\n                    Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n                    MaxPooling1D(pool_size=2),\n                    Dropout(0.2),\n                    Flatten(),\n                    Dense(32, activation='relu'),\n                    Dense(1, activation='sigmoid')\n                ])\n            elif model_type == \"LSTM\":\n                model = Sequential([\n                    LSTM(64, return_sequences=True, input_shape=input_shape),\n                    Dropout(0.2),\n                    LSTM(32),\n                    Dense(1, activation='sigmoid')\n                ])\n            else:\n                raise ValueError(\"Invalid model_type. Choose 'Conv1D' or 'LSTM'.\")\n\n            # Compile model\n            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n            # Train model\n            model.fit(train_ds, epochs=20, verbose=0)\n            print(\"Evaluating on the test dataset....\")\n\n            # Evaluate on test set\n            y_pred = (model.predict(test_ds) > 0.5).astype(int)\n            acc = accuracy_score(y_test, y_pred)\n            bal_acc = balanced_accuracy_score(y_test, y_pred)\n\n            print(f\"Fold {fold + 1}: Accuracy={acc:.4f}, Balanced Accuracy={bal_acc:.4f}\\n\")\n\n            # Update best model if this model performs better\n            if acc > best_accuracy and bal_acc > best_balanced_accuracy:\n                best_model = model\n                best_accuracy = acc\n                best_balanced_accuracy = bal_acc\n\n        # Store results for this k value\n        results[k] = {\n            \"Best Model\": best_model,\n            \"Best Accuracy\": best_accuracy,\n            \"Best Balanced Accuracy\": best_balanced_accuracy,\n        }\n        print(f\"Best Model for top {k} features: Accuracy={best_accuracy:.4f}, Balanced Accuracy={best_balanced_accuracy:.4f}\\n\")\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:07:19.268199Z","iopub.execute_input":"2024-12-03T17:07:19.269008Z","iopub.status.idle":"2024-12-03T17:07:19.346837Z","shell.execute_reply.started":"2024-12-03T17:07:19.268964Z","shell.execute_reply":"2024-12-03T17:07:19.346150Z"},"scrolled":true},"outputs":[],"execution_count":52},{"cell_type":"code","source":"# Call the function for Conv1D\nconv1d_results = train_and_evaluate_dl_models(\n    folds=folds,\n    top_features_dict=filtered_features,\n    model_type=\"Conv1D\"\n)\n\n# Display results\nfor k, result in conv1d_results.items():\n    print(f\"Top {k} features:\")\n    print(f\"Best Model: {result['Best Model']}\")\n    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:07:33.228543Z","iopub.execute_input":"2024-12-03T17:07:33.228894Z","iopub.status.idle":"2024-12-03T17:09:05.554025Z","shell.execute_reply.started":"2024-12-03T17:07:33.228863Z","shell.execute_reply":"2024-12-03T17:09:05.553061Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Evaluating models for top 100 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1733245654.779130     254 service.cc:145] XLA service 0x7bb05c0069a0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1733245654.779192     254 service.cc:153]   StreamExecutor device (0): Tesla P100-PCIE-16GB, Compute Capability 6.0\nI0000 00:00:1733245657.564487     254 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 615ms/step\nFold 1: Accuracy=0.7188, Balanced Accuracy=0.8448\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\nFold 2: Accuracy=0.7419, Balanced Accuracy=0.5556\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\nFold 3: Accuracy=0.8065, Balanced Accuracy=0.6250\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\nFold 4: Accuracy=0.7742, Balanced Accuracy=0.5625\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 132ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.5000\n\nBest Model for top 100 features: Accuracy=0.7188, Balanced Accuracy=0.8448\n\nEvaluating models for top 200 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\nFold 1: Accuracy=0.9062, Balanced Accuracy=0.9483\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 227ms/step\nFold 2: Accuracy=0.7742, Balanced Accuracy=0.6111\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\nFold 3: Accuracy=0.7742, Balanced Accuracy=0.5625\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\nFold 4: Accuracy=0.7742, Balanced Accuracy=0.5625\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.5000\n\nBest Model for top 200 features: Accuracy=0.9062, Balanced Accuracy=0.9483\n\nEvaluating models for top 400 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 179ms/step\nFold 1: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 230ms/step\nFold 2: Accuracy=0.7097, Balanced Accuracy=0.5000\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\nFold 3: Accuracy=0.8065, Balanced Accuracy=0.6250\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\nFold 4: Accuracy=0.8065, Balanced Accuracy=0.6250\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.5000\n\nBest Model for top 400 features: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nEvaluating models for top 800 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 183ms/step\nFold 1: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 245ms/step\nFold 2: Accuracy=0.7742, Balanced Accuracy=0.6111\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\nFold 3: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\nFold 4: Accuracy=0.7742, Balanced Accuracy=0.5625\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 135ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.6267\n\nBest Model for top 800 features: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTop 100 features:\nBest Model: <Sequential name=sequential, built=True>\nBest Accuracy: 0.7188\nBest Balanced Accuracy: 0.8448\n\nTop 200 features:\nBest Model: <Sequential name=sequential_5, built=True>\nBest Accuracy: 0.9062\nBest Balanced Accuracy: 0.9483\n\nTop 400 features:\nBest Model: <Sequential name=sequential_10, built=True>\nBest Accuracy: 0.9062\nBest Balanced Accuracy: 0.5000\n\nTop 800 features:\nBest Model: <Sequential name=sequential_15, built=True>\nBest Accuracy: 0.9062\nBest Balanced Accuracy: 0.5000\n\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"# Call the function for LSTM\nLSTM_results = train_and_evaluate_dl_models(\n    folds=folds,\n    top_features_dict=filtered_features,\n    model_type=\"LSTM\"\n)\n\n# Display results\nfor k, result in LSTM_results.items():\n    print(f\"Top {k} features:\")\n    print(f\"Best LSTM Model: {result['Best Model']}\")\n    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:09:37.102229Z","iopub.execute_input":"2024-12-03T17:09:37.102651Z","iopub.status.idle":"2024-12-03T17:11:53.082757Z","shell.execute_reply.started":"2024-12-03T17:09:37.102619Z","shell.execute_reply":"2024-12-03T17:11:53.081602Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Evaluating models for top 100 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\nFold 1: Accuracy=0.8750, Balanced Accuracy=0.7816\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 148ms/step\nFold 2: Accuracy=0.7097, Balanced Accuracy=0.5000\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\nFold 3: Accuracy=0.8065, Balanced Accuracy=0.6250\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\nFold 4: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\nFold 5: Accuracy=0.7742, Balanced Accuracy=0.5433\n\nBest Model for top 100 features: Accuracy=0.8750, Balanced Accuracy=0.7816\n\nEvaluating models for top 200 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\nFold 1: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\nFold 2: Accuracy=0.7097, Balanced Accuracy=0.5000\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\nFold 3: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 165ms/step\nFold 4: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 168ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.5000\n\nBest Model for top 200 features: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nEvaluating models for top 400 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 166ms/step\nFold 1: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\nFold 2: Accuracy=0.7097, Balanced Accuracy=0.5000\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 161ms/step\nFold 3: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\nFold 4: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.5000\n\nBest Model for top 400 features: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nEvaluating models for top 800 features...\nTraining on Fold 1...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 163ms/step\nFold 1: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTraining on Fold 2...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\nFold 2: Accuracy=0.7097, Balanced Accuracy=0.5000\n\nTraining on Fold 3...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 170ms/step\nFold 3: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 4...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 178ms/step\nFold 4: Accuracy=0.7419, Balanced Accuracy=0.5000\n\nTraining on Fold 5...\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(**kwargs)\n","output_type":"stream"},{"name":"stdout","text":"Evaluating on the test dataset....\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 175ms/step\nFold 5: Accuracy=0.8065, Balanced Accuracy=0.5000\n\nBest Model for top 800 features: Accuracy=0.9062, Balanced Accuracy=0.5000\n\nTop 100 features:\nBest LSTM Model: <Sequential name=sequential_20, built=True>\nBest Accuracy: 0.8750\nBest Balanced Accuracy: 0.7816\n\nTop 200 features:\nBest LSTM Model: <Sequential name=sequential_25, built=True>\nBest Accuracy: 0.9062\nBest Balanced Accuracy: 0.5000\n\nTop 400 features:\nBest LSTM Model: <Sequential name=sequential_30, built=True>\nBest Accuracy: 0.9062\nBest Balanced Accuracy: 0.5000\n\nTop 800 features:\nBest LSTM Model: <Sequential name=sequential_35, built=True>\nBest Accuracy: 0.9062\nBest Balanced Accuracy: 0.5000\n\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# Directory to save models\nsave_dir_conv1d = \"/kaggle/working/saved_models_conv1D\"\nos.makedirs(save_dir_conv1d, exist_ok=True)\n\n# Iterate through the results dictionary\nfor k, result in conv1d_results.items():\n    best_model = result[\"Best Model\"]\n    model_path = os.path.join(save_dir_conv1d, f\"partd_best_dl_conv1d_model_top_{k}_features.pkl\")\n    joblib.dump(best_model, model_path)  # Save the model\n    print(f\"Saved best model for top {k} features at: {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T17:12:47.195438Z","iopub.execute_input":"2024-12-03T17:12:47.195839Z","iopub.status.idle":"2024-12-03T17:12:47.366802Z","shell.execute_reply.started":"2024-12-03T17:12:47.195807Z","shell.execute_reply":"2024-12-03T17:12:47.365559Z"},"scrolled":true},"outputs":[{"name":"stdout","text":"Saved best model for top 100 features at: /kaggle/working/saved_models_conv1D/partd_best_dl_conv1d_model_top_100_features.pkl\nSaved best model for top 200 features at: /kaggle/working/saved_models_conv1D/partd_best_dl_conv1d_model_top_200_features.pkl\nSaved best model for top 400 features at: /kaggle/working/saved_models_conv1D/partd_best_dl_conv1d_model_top_400_features.pkl\nSaved best model for top 800 features at: /kaggle/working/saved_models_conv1D/partd_best_dl_conv1d_model_top_800_features.pkl\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"# Directory to save models\nsave_dir_lstm = \"/kaggle/working/saved_models_LSTM\"\nos.makedirs(save_dir_lstm, exist_ok=True)\n\n# Iterate through the results dictionary\nfor k, result in LSTM_results.items():\n    best_model = result[\"Best Model\"]\n    model_path = os.path.join(save_dir_lstm, f\"partd_best_dl_lstm_model_top_{k}_features.pkl\")\n    joblib.dump(best_model, model_path)  # Save the model\n    print(f\"Saved best model for top {k} features at: {model_path}\")","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-03T17:12:47.656167Z","iopub.execute_input":"2024-12-03T17:12:47.656979Z","iopub.status.idle":"2024-12-03T17:12:47.789806Z","shell.execute_reply.started":"2024-12-03T17:12:47.656939Z","shell.execute_reply":"2024-12-03T17:12:47.788714Z"}},"outputs":[{"name":"stdout","text":"Saved best model for top 100 features at: /kaggle/working/saved_models_LSTM/partd_best_dl_lstm_model_top_100_features.pkl\nSaved best model for top 200 features at: /kaggle/working/saved_models_LSTM/partd_best_dl_lstm_model_top_200_features.pkl\nSaved best model for top 400 features at: /kaggle/working/saved_models_LSTM/partd_best_dl_lstm_model_top_400_features.pkl\nSaved best model for top 800 features at: /kaggle/working/saved_models_LSTM/partd_best_dl_lstm_model_top_800_features.pkl\n","output_type":"stream"}],"execution_count":56}]}