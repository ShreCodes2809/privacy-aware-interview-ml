{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2XcOi_dMlCFs",
        "outputId": "fafd5fb6-6d99-43bb-c20f-a27d824f75b4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: twython in /opt/anaconda3/lib/python3.12/site-packages (3.9.1)\n",
            "Requirement already satisfied: requests>=2.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from twython) (2.32.2)\n",
            "Requirement already satisfied: requests-oauthlib>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from twython) (2.0.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->twython) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->twython) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->twython) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.1.0->twython) (2024.8.30)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from requests-oauthlib>=0.4.0->twython) (3.2.2)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install twython"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M6NVnnITlCFv",
        "outputId": "7bb382b5-fc3c-4e0e-918c-a24214c70fb1"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: sentence-transformers in /opt/anaconda3/lib/python3.12/site-packages (3.3.1)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.47.0)\n",
            "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (4.66.4)\n",
            "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (2.5.1)\n",
            "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: scipy in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (0.26.5)\n",
            "Requirement already satisfied: Pillow in /opt/anaconda3/lib/python3.12/site-packages (from sentence-transformers) (10.3.0)\n",
            "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.3.1)\n",
            "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.11.0)\n",
            "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (69.5.1)\n",
            "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2023.10.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.4.5)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn->sentence-transformers) (2.2.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2024.8.30)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xdfcz6qPlCFw",
        "outputId": "c7fd4be7-c7db-4d3a-c75b-5c650ca9b2ba"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: vaderSentiment in /opt/anaconda3/lib/python3.12/site-packages (3.3.2)\n",
            "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from vaderSentiment) (2.32.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->vaderSentiment) (2024.8.30)\n",
            "\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
            "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/anaconda3/bin/python -m pip install --upgrade pip\u001b[0m\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install vaderSentiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "khVnXp_PlCFw"
      },
      "source": [
        "# Extracting Language Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "scrolled": true,
        "id": "4WG_YCd0lCFx"
      },
      "outputs": [],
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "w4GJNe0plCFy",
        "outputId": "e939f8a6-1efa-4d25-d80b-d800c9a84db4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     /Users/ayushisabnis/nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 141,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "\n",
        "nltk.download('vader_lexicon')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rzf-c2hFlCFy"
      },
      "source": [
        "## Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "lZzQVGuVlCFy"
      },
      "outputs": [],
      "source": [
        "# Set the local folder path\n",
        "folder_path = r'VetTrain_Transcripts'\n",
        "\n",
        "# Function to extract the numerical part from the filename\n",
        "def extract_pid(filename):\n",
        "    base_name = os.path.splitext(filename)[0]\n",
        "    return base_name.split('_')[0]  # Assuming filename is like \"P001_transcript.csv\"\n",
        "\n",
        "# Get all CSV files in the folder and sort them numerically by filename\n",
        "all_files = sorted(\n",
        "    [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')],\n",
        "    key=lambda x: int(extract_pid(os.path.basename(x))[1:])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Q6_KczajlCFz"
      },
      "outputs": [],
      "source": [
        "# Initialize the final storage for combined question pairs\n",
        "combined_data = []\n",
        "\n",
        "# Process each file\n",
        "for file_path in all_files:\n",
        "    # Extract PID from filename\n",
        "    pid = extract_pid(os.path.basename(file_path))\n",
        "\n",
        "    # Read the CSV file\n",
        "    df = pd.read_csv(file_path)\n",
        "    df = df[df['Type'] != 'IRR']  # Filter irrelevant rows\n",
        "\n",
        "    # Initialize dialogue extraction\n",
        "    current_dialogue = []\n",
        "    current_question_id = None\n",
        "    qid_counter = 1  # Start QID counter for each file\n",
        "\n",
        "    # Extract question pairs with PID and QID\n",
        "    for _, row in df.iterrows():\n",
        "        if row['Type'].startswith('Q'):\n",
        "            question_id = row['Type']\n",
        "            if current_question_id is None:\n",
        "                current_question_id = question_id\n",
        "                current_dialogue = [row['Transcript']]\n",
        "            elif question_id != current_question_id:\n",
        "                combined_data.append({\n",
        "                    'PID': pid,\n",
        "                    'QID': f\"Q{qid_counter}\",\n",
        "                    'Combined_Transcript': \" \".join(current_dialogue)\n",
        "                })\n",
        "                qid_counter += 1\n",
        "                current_dialogue = [row['Transcript']]\n",
        "                current_question_id = question_id\n",
        "            else:\n",
        "                current_dialogue.append(row['Transcript'])\n",
        "        else:\n",
        "            current_dialogue.append(row['Transcript'])\n",
        "\n",
        "    # Add the last dialogue for the file\n",
        "    if current_dialogue:\n",
        "        combined_data.append({\n",
        "            'PID': pid,\n",
        "            'QID': f\"Q{qid_counter}\",\n",
        "            'Combined_Transcript': \" \".join(current_dialogue)\n",
        "        })"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "pTMK8ub-lCFz"
      },
      "outputs": [],
      "source": [
        "# Convert to a DataFrame\n",
        "df_combined = pd.DataFrame(combined_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "11z_pRxalCF0"
      },
      "outputs": [],
      "source": [
        "# Load the behavioral annotation codes\n",
        "behavior_file = r'Behavioral Annotation Codes.csv'\n",
        "df_behavior = pd.read_csv(behavior_file)\n",
        "\n",
        "# Merge behavioral codes\n",
        "df_combined = df_combined.merge(df_behavior, on=['PID', 'QID'], how='left')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "Zyfr6UAglCF0"
      },
      "outputs": [],
      "source": [
        "# Data preprocessing\n",
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "    # Delete redundant Spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "    return text\n",
        "\n",
        "df_combined['Cleaned_Transcript'] = df_combined['Combined_Transcript'].apply(clean_text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "a1ayJ5QRlCF0",
        "outputId": "de4afaad-e42b-4ee2-8b53-01672c3b8002"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>QID</th>\n",
              "      <th>Combined_Transcript</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q1</td>\n",
              "      <td>Interviewer: Good. I just uh, I uh, always hav...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q2</td>\n",
              "      <td>Interviewer: Okay, good deal. Interviewer: Yep...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q3</td>\n",
              "      <td>Interviewer: Okay, so with all of your experie...</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q4</td>\n",
              "      <td>Interviewer: So, was that an easy transition f...</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q5</td>\n",
              "      <td>Interviewer: So when we're talking about stren...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q6</td>\n",
              "      <td>Interviewer: Yeah, yeah, no, I get it, I get i...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer yeah yeah no i get it i get it so ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q7</td>\n",
              "      <td>Interviewer: So how did you- Interviewer: What...</td>\n",
              "      <td>Comprehensive</td>\n",
              "      <td>interviewer so how did you interviewer what wh...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q8</td>\n",
              "      <td>Interviewer: Okay. Good. So, are you and your ...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good so are you and your wife...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q9</td>\n",
              "      <td>Interviewer: Sure. Yeah. So, when, ah, when ar...</td>\n",
              "      <td>Over-explained</td>\n",
              "      <td>interviewer sure yeah so when ah when are you ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q10</td>\n",
              "      <td>Interviewer: Yeah, I think that that is justif...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer yeah i think that that is justifie...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID  QID                                Combined_Transcript  \\\n",
              "0  P001   Q1  Interviewer: Good. I just uh, I uh, always hav...   \n",
              "1  P001   Q2  Interviewer: Okay, good deal. Interviewer: Yep...   \n",
              "2  P001   Q3  Interviewer: Okay, so with all of your experie...   \n",
              "3  P001   Q4  Interviewer: So, was that an easy transition f...   \n",
              "4  P001   Q5  Interviewer: So when we're talking about stren...   \n",
              "5  P001   Q6  Interviewer: Yeah, yeah, no, I get it, I get i...   \n",
              "6  P001   Q7  Interviewer: So how did you- Interviewer: What...   \n",
              "7  P001   Q8  Interviewer: Okay. Good. So, are you and your ...   \n",
              "8  P001   Q9  Interviewer: Sure. Yeah. So, when, ah, when ar...   \n",
              "9  P001  Q10  Interviewer: Yeah, I think that that is justif...   \n",
              "\n",
              "  Degree of Explanation                                 Cleaned_Transcript  \n",
              "0              Succinct  interviewer good i just uh i uh always have it...  \n",
              "1              Succinct  interviewer okay good deal interviewer yep i u...  \n",
              "2       Under-explained  interviewer okay so with all of your experienc...  \n",
              "3       Under-explained  interviewer so was that an easy transition fro...  \n",
              "4              Succinct  interviewer so when were talking about strengt...  \n",
              "5              Succinct  interviewer yeah yeah no i get it i get it so ...  \n",
              "6         Comprehensive  interviewer so how did you interviewer what wh...  \n",
              "7              Succinct  interviewer okay good so are you and your wife...  \n",
              "8        Over-explained  interviewer sure yeah so when ah when are you ...  \n",
              "9              Succinct  interviewer yeah i think that that is justifie...  "
            ]
          },
          "execution_count": 151,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# displaying the combined dataframe\n",
        "df_combined.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0zV1yOQ9lCF0",
        "outputId": "7a20148e-df09-4367-ae33-ba91e3e0e8d6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(281, 5)"
            ]
          },
          "execution_count": 152,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_combined.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "0oAM-ScUlCF0",
        "outputId": "b71225bb-0f71-4672-b289-3305940d268e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     /Users/ayushisabnis/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /Users/ayushisabnis/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 155,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "import nltk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "5UHGUQyzlCF0",
        "outputId": "07e3ef0f-0ab9-4f55-f52f-8fa030b0e108"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature names: ['01' '0ne' '10' ... 'zone' 'zoom' 'zooming']\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 1 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(281, 5604)"
            ]
          },
          "execution_count": 162,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Count Vectorizer\n",
        "vectorizer = CountVectorizer()\n",
        "count_vectorizer = vectorizer.fit_transform(df_combined['Cleaned_Transcript'])\n",
        "print(\"Feature names:\", vectorizer.get_feature_names_out())\n",
        "print(count_vectorizer.toarray())\n",
        "np.shape(count_vectorizer.toarray())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3lw6mW60lCF1",
        "outputId": "db04c2e4-529c-4ea5-b7e4-c6b5e0f5703d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " ...\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]\n",
            " [0.        0.        0.0499286 ... 0.        0.        0.       ]\n",
            " [0.        0.        0.        ... 0.        0.        0.       ]]\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "(281, 5604)"
            ]
          },
          "execution_count": 164,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# TF-IDF Vectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer()\n",
        "tfidf_features = tfidf_vectorizer.fit_transform(df_combined['Cleaned_Transcript'])\n",
        "print(tfidf_features.toarray())\n",
        "np.shape(tfidf_features.toarray())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KonRX2fTlCF1"
      },
      "source": [
        "## Adding the extracted features to the main dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tq3UpQFXlCF1"
      },
      "outputs": [],
      "source": [
        "def add_tfidf_features(df, text_column, max_features=500):\n",
        "    \"\"\"\n",
        "    Adds TF-IDF features to the dataset.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataframe containing the text data.\n",
        "        text_column (str): Name of the column containing text data.\n",
        "        max_features (int): Maximum number of TF-IDF features to generate (default=500).\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe with TF-IDF features added.\n",
        "    \"\"\"\n",
        "    # Initialize TF-IDF Vectorizer\n",
        "    tfidf = TfidfVectorizer(max_features=max_features)\n",
        "\n",
        "    # Fit and transform the text data\n",
        "    tfidf_matrix = tfidf.fit_transform(df[text_column])\n",
        "\n",
        "    # Convert the TF-IDF matrix to a DataFrame\n",
        "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=df.index)\n",
        "\n",
        "    # Concatenate the TF-IDF features with the original DataFrame\n",
        "    df_with_tfidf = pd.concat([df, tfidf_df], axis=1)\n",
        "\n",
        "    return df_with_tfidf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i-LjorGflCF1",
        "outputId": "cfd450ae-d4d5-4d2b-f80b-e0f4fdabb747"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>QID</th>\n",
              "      <th>Combined_Transcript</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>...</th>\n",
              "      <th>yet</th>\n",
              "      <th>you</th>\n",
              "      <th>youd</th>\n",
              "      <th>youll</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youve</th>\n",
              "      <th>zoom</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q1</td>\n",
              "      <td>Interviewer: Good. I just uh, I uh, always hav...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.158225</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034997</td>\n",
              "      <td>0.062375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q2</td>\n",
              "      <td>Interviewer: Okay, good deal. Interviewer: Yep...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.155865</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.293292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q3</td>\n",
              "      <td>Interviewer: Okay, so with all of your experie...</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.151316</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.067793</td>\n",
              "      <td>0.044626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q4</td>\n",
              "      <td>Interviewer: So, was that an easy transition f...</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.105391</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q5</td>\n",
              "      <td>Interviewer: So when we're talking about stren...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.360844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.032333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1005 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID QID                                Combined_Transcript  \\\n",
              "0  P001  Q1  Interviewer: Good. I just uh, I uh, always hav...   \n",
              "1  P001  Q2  Interviewer: Okay, good deal. Interviewer: Yep...   \n",
              "2  P001  Q3  Interviewer: Okay, so with all of your experie...   \n",
              "3  P001  Q4  Interviewer: So, was that an easy transition f...   \n",
              "4  P001  Q5  Interviewer: So when we're talking about stren...   \n",
              "\n",
              "  Degree of Explanation                                 Cleaned_Transcript  \\\n",
              "0              Succinct  interviewer good i just uh i uh always have it...   \n",
              "1              Succinct  interviewer okay good deal interviewer yep i u...   \n",
              "2       Under-explained  interviewer okay so with all of your experienc...   \n",
              "3       Under-explained  interviewer so was that an easy transition fro...   \n",
              "4              Succinct  interviewer so when were talking about strengt...   \n",
              "\n",
              "    15   20  ability  able     about  ...  yet       you  youd  youll  young  \\\n",
              "0  0.0  0.0      0.0   0.0  0.086689  ...  0.0  0.158225   0.0    0.0    0.0   \n",
              "1  0.0  0.0      0.0   0.0  0.136634  ...  0.0  0.155865   0.0    0.0    0.0   \n",
              "2  0.0  0.0      0.0   0.0  0.073692  ...  0.0  0.151316   0.0    0.0    0.0   \n",
              "3  0.0  0.0      0.0   0.0  0.000000  ...  0.0  0.105391   0.0    0.0    0.0   \n",
              "4  0.0  0.0      0.0   0.0  0.035147  ...  0.0  0.360844   0.0    0.0    0.0   \n",
              "\n",
              "       your     youre  yourself  youve  zoom  \n",
              "0  0.000000  0.034997  0.062375    0.0   0.0  \n",
              "1  0.293292  0.000000  0.000000    0.0   0.0  \n",
              "2  0.067793  0.044626  0.000000    0.0   0.0  \n",
              "3  0.000000  0.000000  0.000000    0.0   0.0  \n",
              "4  0.032333  0.000000  0.000000    0.0   0.0  \n",
              "\n",
              "[5 rows x 1005 columns]"
            ]
          },
          "execution_count": 170,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Example usage\n",
        "df_combined = add_tfidf_features(df_combined, text_column=\"Cleaned_Transcript\", max_features=1000)\n",
        "df_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "89Lq83E5lCF1"
      },
      "outputs": [],
      "source": [
        "def add_pos_tags(df, text_column):\n",
        "    \"\"\"\n",
        "    Adds POS tagging to the dataframe.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataframe containing text data.\n",
        "        text_column (str): Name of the column containing text data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe with added POS tags.\n",
        "    \"\"\"\n",
        "    def pos_tags(text):\n",
        "        tokens = word_tokenize(text)\n",
        "        tags = pos_tag(tokens)\n",
        "        return {tag: len([word for word, pos in tags if pos == tag]) for tag in set([pos for _, pos in tags])}\n",
        "\n",
        "    df['POS_Tags'] = df[text_column].apply(pos_tags)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viIuCglUlCF1",
        "outputId": "b8b24be9-a610-4748-bb62-4393d6e8535c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>QID</th>\n",
              "      <th>Combined_Transcript</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>...</th>\n",
              "      <th>you</th>\n",
              "      <th>youd</th>\n",
              "      <th>youll</th>\n",
              "      <th>young</th>\n",
              "      <th>your</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youve</th>\n",
              "      <th>zoom</th>\n",
              "      <th>POS_Tags</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q1</td>\n",
              "      <td>Interviewer: Good. I just uh, I uh, always hav...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>...</td>\n",
              "      <td>0.158225</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034997</td>\n",
              "      <td>0.062375</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q2</td>\n",
              "      <td>Interviewer: Okay, good deal. Interviewer: Yep...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>...</td>\n",
              "      <td>0.155865</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.293292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q3</td>\n",
              "      <td>Interviewer: Okay, so with all of your experie...</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>...</td>\n",
              "      <td>0.151316</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.067793</td>\n",
              "      <td>0.044626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q4</td>\n",
              "      <td>Interviewer: So, was that an easy transition f...</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.105391</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q5</td>\n",
              "      <td>Interviewer: So when we're talking about stren...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>...</td>\n",
              "      <td>0.360844</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.032333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q6</td>\n",
              "      <td>Interviewer: Yeah, yeah, no, I get it, I get i...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer yeah yeah no i get it i get it so ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.091637</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.082111</td>\n",
              "      <td>0.054051</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 1, 'PRP$': 5, 'WRB': 1, 'JJ': 10, 'RBS'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q7</td>\n",
              "      <td>Interviewer: So how did you- Interviewer: What...</td>\n",
              "      <td>Comprehensive</td>\n",
              "      <td>interviewer so how did you interviewer what wh...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.057876</td>\n",
              "      <td>...</td>\n",
              "      <td>0.198066</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.026622</td>\n",
              "      <td>0.070096</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'PRP$': 4, 'WRB': 3, 'JJ': 26, 'CC':...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q8</td>\n",
              "      <td>Interviewer: Okay. Good. So, are you and your ...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good so are you and your wife...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.085033</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.035994</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.048379</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'VBP': 4, 'NNS': 4, 'VBG': 2, 'RP': 1, 'PRP$'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q9</td>\n",
              "      <td>Interviewer: Sure. Yeah. So, when, ah, when ar...</td>\n",
              "      <td>Over-explained</td>\n",
              "      <td>interviewer sure yeah so when ah when are you ...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.233472</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.044829</td>\n",
              "      <td>0.029509</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.042184</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 6, 'PRP$': 8, 'WRB': 10, 'JJ': 33, 'CC'...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>P001</td>\n",
              "      <td>Q10</td>\n",
              "      <td>Interviewer: Yeah, I think that that is justif...</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer yeah i think that that is justifie...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.038699</td>\n",
              "      <td>...</td>\n",
              "      <td>0.079463</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.103462</td>\n",
              "      <td>0.035601</td>\n",
              "      <td>0.046870</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'PRP$': 12, 'WRB': 2, 'JJ': 16, 'CC': 9, 'PRP...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 1006 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID  QID                                Combined_Transcript  \\\n",
              "0  P001   Q1  Interviewer: Good. I just uh, I uh, always hav...   \n",
              "1  P001   Q2  Interviewer: Okay, good deal. Interviewer: Yep...   \n",
              "2  P001   Q3  Interviewer: Okay, so with all of your experie...   \n",
              "3  P001   Q4  Interviewer: So, was that an easy transition f...   \n",
              "4  P001   Q5  Interviewer: So when we're talking about stren...   \n",
              "5  P001   Q6  Interviewer: Yeah, yeah, no, I get it, I get i...   \n",
              "6  P001   Q7  Interviewer: So how did you- Interviewer: What...   \n",
              "7  P001   Q8  Interviewer: Okay. Good. So, are you and your ...   \n",
              "8  P001   Q9  Interviewer: Sure. Yeah. So, when, ah, when ar...   \n",
              "9  P001  Q10  Interviewer: Yeah, I think that that is justif...   \n",
              "\n",
              "  Degree of Explanation                                 Cleaned_Transcript  \\\n",
              "0              Succinct  interviewer good i just uh i uh always have it...   \n",
              "1              Succinct  interviewer okay good deal interviewer yep i u...   \n",
              "2       Under-explained  interviewer okay so with all of your experienc...   \n",
              "3       Under-explained  interviewer so was that an easy transition fro...   \n",
              "4              Succinct  interviewer so when were talking about strengt...   \n",
              "5              Succinct  interviewer yeah yeah no i get it i get it so ...   \n",
              "6         Comprehensive  interviewer so how did you interviewer what wh...   \n",
              "7              Succinct  interviewer okay good so are you and your wife...   \n",
              "8        Over-explained  interviewer sure yeah so when ah when are you ...   \n",
              "9              Succinct  interviewer yeah i think that that is justifie...   \n",
              "\n",
              "    15   20  ability      able     about  ...       you  youd  youll  \\\n",
              "0  0.0  0.0      0.0  0.000000  0.086689  ...  0.158225   0.0    0.0   \n",
              "1  0.0  0.0      0.0  0.000000  0.136634  ...  0.155865   0.0    0.0   \n",
              "2  0.0  0.0      0.0  0.000000  0.073692  ...  0.151316   0.0    0.0   \n",
              "3  0.0  0.0      0.0  0.000000  0.000000  ...  0.105391   0.0    0.0   \n",
              "4  0.0  0.0      0.0  0.000000  0.035147  ...  0.360844   0.0    0.0   \n",
              "5  0.0  0.0      0.0  0.000000  0.000000  ...  0.091637   0.0    0.0   \n",
              "6  0.0  0.0      0.0  0.000000  0.057876  ...  0.198066   0.0    0.0   \n",
              "7  0.0  0.0      0.0  0.085033  0.000000  ...  0.035994   0.0    0.0   \n",
              "8  0.0  0.0      0.0  0.000000  0.000000  ...  0.233472   0.0    0.0   \n",
              "9  0.0  0.0      0.0  0.000000  0.038699  ...  0.079463   0.0    0.0   \n",
              "\n",
              "      young      your     youre  yourself     youve  zoom  \\\n",
              "0  0.000000  0.000000  0.034997  0.062375  0.000000   0.0   \n",
              "1  0.000000  0.293292  0.000000  0.000000  0.000000   0.0   \n",
              "2  0.000000  0.067793  0.044626  0.000000  0.000000   0.0   \n",
              "3  0.000000  0.000000  0.000000  0.000000  0.000000   0.0   \n",
              "4  0.000000  0.032333  0.000000  0.000000  0.000000   0.0   \n",
              "5  0.000000  0.082111  0.054051  0.000000  0.000000   0.0   \n",
              "6  0.000000  0.026622  0.070096  0.000000  0.000000   0.0   \n",
              "7  0.000000  0.048379  0.000000  0.000000  0.000000   0.0   \n",
              "8  0.000000  0.044829  0.029509  0.000000  0.042184   0.0   \n",
              "9  0.103462  0.035601  0.046870  0.000000  0.000000   0.0   \n",
              "\n",
              "                                            POS_Tags  \n",
              "0  {'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...  \n",
              "1  {'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...  \n",
              "2  {'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...  \n",
              "3  {'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...  \n",
              "4  {'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...  \n",
              "5  {'RP': 1, 'PRP$': 5, 'WRB': 1, 'JJ': 10, 'RBS'...  \n",
              "6  {'CD': 3, 'PRP$': 4, 'WRB': 3, 'JJ': 26, 'CC':...  \n",
              "7  {'VBP': 4, 'NNS': 4, 'VBG': 2, 'RP': 1, 'PRP$'...  \n",
              "8  {'CD': 6, 'PRP$': 8, 'WRB': 10, 'JJ': 33, 'CC'...  \n",
              "9  {'PRP$': 12, 'WRB': 2, 'JJ': 16, 'CC': 9, 'PRP...  \n",
              "\n",
              "[10 rows x 1006 columns]"
            ]
          },
          "execution_count": 174,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_combined = add_pos_tags(df_combined, text_column=\"Cleaned_Transcript\")\n",
        "df_combined.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Sw6NB91lCF1"
      },
      "outputs": [],
      "source": [
        "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
        "\n",
        "def add_sentiment_scores(df, text_column):\n",
        "    \"\"\"\n",
        "    Adds sentiment scores as separate columns to the dataframe.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataframe containing text data.\n",
        "        text_column (str): Name of the column containing text data.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe with added sentiment scores as separate columns.\n",
        "    \"\"\"\n",
        "    sia = SentimentIntensityAnalyzer()\n",
        "    sentiment_scores = df[text_column].apply(lambda text: sia.polarity_scores(text))\n",
        "\n",
        "    # Create separate columns for each sentiment score\n",
        "    df['Sentiment_Neg'] = sentiment_scores.apply(lambda score: score['neg'])\n",
        "    df['Sentiment_Neu'] = sentiment_scores.apply(lambda score: score['neu'])\n",
        "    df['Sentiment_Pos'] = sentiment_scores.apply(lambda score: score['pos'])\n",
        "    df['Sentiment_Compound'] = sentiment_scores.apply(lambda score: score['compound'])\n",
        "\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "y8vhXmAElCF2",
        "outputId": "689f1620-1566-452f-8ca9-a24e4f3714e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sentiment_Neg         0.0070\n",
            "Sentiment_Neu         0.8210\n",
            "Sentiment_Pos         0.1720\n",
            "Sentiment_Compound    0.9936\n",
            "Name: 0, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Adding sentiment scores as separate columns to the dataframe\n",
        "df_combined = add_sentiment_scores(df_combined, text_column=\"Cleaned_Transcript\")\n",
        "\n",
        "# Example output for the first row\n",
        "print(df_combined[['Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos', 'Sentiment_Compound']].iloc[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xgaj4zgwlCF2",
        "outputId": "8b2af713-71d9-4ebe-9e8a-63fc437b8b6d"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>above</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>...</th>\n",
              "      <th>your</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youve</th>\n",
              "      <th>zoom</th>\n",
              "      <th>POS_Tags</th>\n",
              "      <th>Sentiment_Neg</th>\n",
              "      <th>Sentiment_Neu</th>\n",
              "      <th>Sentiment_Pos</th>\n",
              "      <th>Sentiment_Compound</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.034997</td>\n",
              "      <td>0.062375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.9936</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.293292</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.045</td>\n",
              "      <td>-0.0889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.067793</td>\n",
              "      <td>0.044626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.9286</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.5568</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.032333</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.9653</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1008 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID Degree of Explanation  \\\n",
              "0  P001              Succinct   \n",
              "1  P001              Succinct   \n",
              "2  P001       Under-explained   \n",
              "3  P001       Under-explained   \n",
              "4  P001              Succinct   \n",
              "\n",
              "                                  Cleaned_Transcript   15   20  ability  able  \\\n",
              "0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n",
              "1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n",
              "2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n",
              "3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n",
              "4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n",
              "\n",
              "      about  above  absolutely  ...      your     youre  yourself  youve  \\\n",
              "0  0.086689    0.0         0.0  ...  0.000000  0.034997  0.062375    0.0   \n",
              "1  0.136634    0.0         0.0  ...  0.293292  0.000000  0.000000    0.0   \n",
              "2  0.073692    0.0         0.0  ...  0.067793  0.044626  0.000000    0.0   \n",
              "3  0.000000    0.0         0.0  ...  0.000000  0.000000  0.000000    0.0   \n",
              "4  0.035147    0.0         0.0  ...  0.032333  0.000000  0.000000    0.0   \n",
              "\n",
              "   zoom                                           POS_Tags  Sentiment_Neg  \\\n",
              "0   0.0  {'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...          0.007   \n",
              "1   0.0  {'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...          0.036   \n",
              "2   0.0  {'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...          0.011   \n",
              "3   0.0  {'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...          0.036   \n",
              "4   0.0  {'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...          0.028   \n",
              "\n",
              "   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \n",
              "0          0.821          0.172              0.9936  \n",
              "1          0.919          0.045             -0.0889  \n",
              "2          0.904          0.085              0.9286  \n",
              "3          0.910          0.054              0.5568  \n",
              "4          0.828          0.144              0.9653  \n",
              "\n",
              "[5 rows x 1008 columns]"
            ]
          },
          "execution_count": 177,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_combined.drop(['QID', 'Combined_Transcript'], axis=1, inplace=True)\n",
        "df_combined.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "BaKNHpYolCF2"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "\n",
        "def add_word_embeddings(df, text_column, model_name='all-MiniLM-L6-v2'):\n",
        "    \"\"\"\n",
        "    Adds word embeddings to the dataframe using Sentence Transformers.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataframe containing text data.\n",
        "        text_column (str): Name of the column containing text data.\n",
        "        model_name (str): Sentence Transformer model name.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: Dataframe with added embeddings.\n",
        "    \"\"\"\n",
        "    model = SentenceTransformer(model_name)\n",
        "    embeddings = model.encode(df[text_column].tolist(), show_progress_bar=True)\n",
        "    df['Embeddings'] = list(embeddings)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "rbkKTdnHlCF2",
        "outputId": "49594c4e-1dde-47cb-ad11-1614fd2b9a0a",
        "colab": {
          "referenced_widgets": [
            "298f22c937f34ba5b2fb7448bda3a449"
          ]
        }
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "298f22c937f34ba5b2fb7448bda3a449",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Batches:   0%|          | 0/9 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>above</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>...</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youve</th>\n",
              "      <th>zoom</th>\n",
              "      <th>POS_Tags</th>\n",
              "      <th>Sentiment_Neg</th>\n",
              "      <th>Sentiment_Neu</th>\n",
              "      <th>Sentiment_Pos</th>\n",
              "      <th>Sentiment_Compound</th>\n",
              "      <th>Embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034997</td>\n",
              "      <td>0.062375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.9936</td>\n",
              "      <td>[-0.08247564, 0.009922197, 0.033036184, 0.0038...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.045</td>\n",
              "      <td>-0.0889</td>\n",
              "      <td>[-0.029192025, 0.056575313, 0.07875369, 0.0209...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.9286</td>\n",
              "      <td>[-0.042129382, 0.026093017, 0.02012703, 0.0270...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.5568</td>\n",
              "      <td>[-0.06791862, 0.02170143, 0.033392277, -0.0060...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.9653</td>\n",
              "      <td>[0.01770237, 0.050047472, 0.011827877, -0.0141...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1009 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID Degree of Explanation  \\\n",
              "0  P001              Succinct   \n",
              "1  P001              Succinct   \n",
              "2  P001       Under-explained   \n",
              "3  P001       Under-explained   \n",
              "4  P001              Succinct   \n",
              "\n",
              "                                  Cleaned_Transcript   15   20  ability  able  \\\n",
              "0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n",
              "1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n",
              "2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n",
              "3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n",
              "4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n",
              "\n",
              "      about  above  absolutely  ...     youre  yourself  youve  zoom  \\\n",
              "0  0.086689    0.0         0.0  ...  0.034997  0.062375    0.0   0.0   \n",
              "1  0.136634    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n",
              "2  0.073692    0.0         0.0  ...  0.044626  0.000000    0.0   0.0   \n",
              "3  0.000000    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n",
              "4  0.035147    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n",
              "\n",
              "                                            POS_Tags  Sentiment_Neg  \\\n",
              "0  {'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...          0.007   \n",
              "1  {'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...          0.036   \n",
              "2  {'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...          0.011   \n",
              "3  {'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...          0.036   \n",
              "4  {'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...          0.028   \n",
              "\n",
              "   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \\\n",
              "0          0.821          0.172              0.9936   \n",
              "1          0.919          0.045             -0.0889   \n",
              "2          0.904          0.085              0.9286   \n",
              "3          0.910          0.054              0.5568   \n",
              "4          0.828          0.144              0.9653   \n",
              "\n",
              "                                          Embeddings  \n",
              "0  [-0.08247564, 0.009922197, 0.033036184, 0.0038...  \n",
              "1  [-0.029192025, 0.056575313, 0.07875369, 0.0209...  \n",
              "2  [-0.042129382, 0.026093017, 0.02012703, 0.0270...  \n",
              "3  [-0.06791862, 0.02170143, 0.033392277, -0.0060...  \n",
              "4  [0.01770237, 0.050047472, 0.011827877, -0.0141...  \n",
              "\n",
              "[5 rows x 1009 columns]"
            ]
          },
          "execution_count": 179,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_combined = add_word_embeddings(df_combined, text_column=\"Cleaned_Transcript\")\n",
        "df_combined.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cy9RCEZClCF2"
      },
      "source": [
        "# Classifying between Under-Explained and Succinct"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-VvLcO_lCF2"
      },
      "source": [
        "## Cleaning Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "HplnvpRGlCF2",
        "outputId": "e0c39aac-e385-4824-8119-0a46194ec400"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>above</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>...</th>\n",
              "      <th>youre</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youve</th>\n",
              "      <th>zoom</th>\n",
              "      <th>POS_Tags</th>\n",
              "      <th>Sentiment_Neg</th>\n",
              "      <th>Sentiment_Neu</th>\n",
              "      <th>Sentiment_Pos</th>\n",
              "      <th>Sentiment_Compound</th>\n",
              "      <th>Embeddings</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.034997</td>\n",
              "      <td>0.062375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.9936</td>\n",
              "      <td>[-0.08247564, 0.009922197, 0.033036184, 0.0038...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.045</td>\n",
              "      <td>-0.0889</td>\n",
              "      <td>[-0.029192025, 0.056575313, 0.07875369, 0.0209...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.044626</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.9286</td>\n",
              "      <td>[-0.042129382, 0.026093017, 0.02012703, 0.0270...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.5568</td>\n",
              "      <td>[-0.06791862, 0.02170143, 0.033392277, -0.0060...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.9653</td>\n",
              "      <td>[0.01770237, 0.050047472, 0.011827877, -0.0141...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1009 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID Degree of Explanation  \\\n",
              "0  P001              Succinct   \n",
              "1  P001              Succinct   \n",
              "2  P001       Under-explained   \n",
              "3  P001       Under-explained   \n",
              "4  P001              Succinct   \n",
              "\n",
              "                                  Cleaned_Transcript   15   20  ability  able  \\\n",
              "0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n",
              "1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n",
              "2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n",
              "3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n",
              "4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n",
              "\n",
              "      about  above  absolutely  ...     youre  yourself  youve  zoom  \\\n",
              "0  0.086689    0.0         0.0  ...  0.034997  0.062375    0.0   0.0   \n",
              "1  0.136634    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n",
              "2  0.073692    0.0         0.0  ...  0.044626  0.000000    0.0   0.0   \n",
              "3  0.000000    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n",
              "4  0.035147    0.0         0.0  ...  0.000000  0.000000    0.0   0.0   \n",
              "\n",
              "                                            POS_Tags  Sentiment_Neg  \\\n",
              "0  {'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...          0.007   \n",
              "1  {'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...          0.036   \n",
              "2  {'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...          0.011   \n",
              "3  {'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...          0.036   \n",
              "4  {'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...          0.028   \n",
              "\n",
              "   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \\\n",
              "0          0.821          0.172              0.9936   \n",
              "1          0.919          0.045             -0.0889   \n",
              "2          0.904          0.085              0.9286   \n",
              "3          0.910          0.054              0.5568   \n",
              "4          0.828          0.144              0.9653   \n",
              "\n",
              "                                          Embeddings  \n",
              "0  [-0.08247564, 0.009922197, 0.033036184, 0.0038...  \n",
              "1  [-0.029192025, 0.056575313, 0.07875369, 0.0209...  \n",
              "2  [-0.042129382, 0.026093017, 0.02012703, 0.0270...  \n",
              "3  [-0.06791862, 0.02170143, 0.033392277, -0.0060...  \n",
              "4  [0.01770237, 0.050047472, 0.011827877, -0.0141...  \n",
              "\n",
              "[5 rows x 1009 columns]"
            ]
          },
          "execution_count": 187,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# creating a dataset for classification\n",
        "df_new = df_combined.copy()\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "cuHsE920lCF2",
        "outputId": "315c9108-6748-4f67-f63a-aa5e16297ad7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Degree of Explanation</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>above</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>...</th>\n",
              "      <th>yourself</th>\n",
              "      <th>youve</th>\n",
              "      <th>zoom</th>\n",
              "      <th>POS_Tags</th>\n",
              "      <th>Sentiment_Neg</th>\n",
              "      <th>Sentiment_Neu</th>\n",
              "      <th>Sentiment_Pos</th>\n",
              "      <th>Sentiment_Compound</th>\n",
              "      <th>Embeddings</th>\n",
              "      <th>DOE_Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.062375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...</td>\n",
              "      <td>0.007</td>\n",
              "      <td>0.821</td>\n",
              "      <td>0.172</td>\n",
              "      <td>0.9936</td>\n",
              "      <td>[-0.08247564, 0.009922197, 0.033036184, 0.0038...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.919</td>\n",
              "      <td>0.045</td>\n",
              "      <td>-0.0889</td>\n",
              "      <td>[-0.029192025, 0.056575313, 0.07875369, 0.0209...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...</td>\n",
              "      <td>0.011</td>\n",
              "      <td>0.904</td>\n",
              "      <td>0.085</td>\n",
              "      <td>0.9286</td>\n",
              "      <td>[-0.042129382, 0.026093017, 0.02012703, 0.0270...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>Under-explained</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...</td>\n",
              "      <td>0.036</td>\n",
              "      <td>0.910</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.5568</td>\n",
              "      <td>[-0.06791862, 0.02170143, 0.033392277, -0.0060...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>Succinct</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>{'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...</td>\n",
              "      <td>0.028</td>\n",
              "      <td>0.828</td>\n",
              "      <td>0.144</td>\n",
              "      <td>0.9653</td>\n",
              "      <td>[0.01770237, 0.050047472, 0.011827877, -0.0141...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1010 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID Degree of Explanation  \\\n",
              "0  P001              Succinct   \n",
              "1  P001              Succinct   \n",
              "2  P001       Under-explained   \n",
              "3  P001       Under-explained   \n",
              "4  P001              Succinct   \n",
              "\n",
              "                                  Cleaned_Transcript   15   20  ability  able  \\\n",
              "0  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   0.0   \n",
              "1  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   0.0   \n",
              "2  interviewer okay so with all of your experienc...  0.0  0.0      0.0   0.0   \n",
              "3  interviewer so was that an easy transition fro...  0.0  0.0      0.0   0.0   \n",
              "4  interviewer so when were talking about strengt...  0.0  0.0      0.0   0.0   \n",
              "\n",
              "      about  above  absolutely  ...  yourself  youve  zoom  \\\n",
              "0  0.086689    0.0         0.0  ...  0.062375    0.0   0.0   \n",
              "1  0.136634    0.0         0.0  ...  0.000000    0.0   0.0   \n",
              "2  0.073692    0.0         0.0  ...  0.000000    0.0   0.0   \n",
              "3  0.000000    0.0         0.0  ...  0.000000    0.0   0.0   \n",
              "4  0.035147    0.0         0.0  ...  0.000000    0.0   0.0   \n",
              "\n",
              "                                            POS_Tags  Sentiment_Neg  \\\n",
              "0  {'CD': 3, 'RP': 3, 'PRP$': 5, 'JJ': 36, 'CC': ...          0.007   \n",
              "1  {'CD': 3, 'PRP$': 8, 'WRB': 1, 'JJ': 18, 'CC':...          0.036   \n",
              "2  {'RP': 2, 'PRP$': 7, 'WRB': 5, 'JJ': 17, 'CC':...          0.011   \n",
              "3  {'RP': 3, 'PRP$': 3, 'WRB': 2, 'JJ': 12, 'CC':...          0.036   \n",
              "4  {'PRP$': 4, 'WRB': 3, 'JJ': 18, 'CC': 9, 'PRP'...          0.028   \n",
              "\n",
              "   Sentiment_Neu  Sentiment_Pos  Sentiment_Compound  \\\n",
              "0          0.821          0.172              0.9936   \n",
              "1          0.919          0.045             -0.0889   \n",
              "2          0.904          0.085              0.9286   \n",
              "3          0.910          0.054              0.5568   \n",
              "4          0.828          0.144              0.9653   \n",
              "\n",
              "                                          Embeddings  DOE_Label  \n",
              "0  [-0.08247564, 0.009922197, 0.033036184, 0.0038...          1  \n",
              "1  [-0.029192025, 0.056575313, 0.07875369, 0.0209...          1  \n",
              "2  [-0.042129382, 0.026093017, 0.02012703, 0.0270...          0  \n",
              "3  [-0.06791862, 0.02170143, 0.033392277, -0.0060...          0  \n",
              "4  [0.01770237, 0.050047472, 0.011827877, -0.0141...          1  \n",
              "\n",
              "[5 rows x 1010 columns]"
            ]
          },
          "execution_count": 188,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new = df_new[df_new['Degree of Explanation'].isin(['Under-explained', 'Succinct'])]\n",
        "df_new['DOE_Label'] = df_new['Degree of Explanation'].map({'Under-explained': 0, 'Succinct': 1})\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vKGFSp6MlCF2"
      },
      "outputs": [],
      "source": [
        "df_new.drop(['Degree of Explanation'], axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-Tvc2as-lCF3",
        "outputId": "442527b9-e94a-4e65-9ab8-0c31b456878e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129, 1009)"
            ]
          },
          "execution_count": 190,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o17weYjQlCF3",
        "outputId": "b2b9e0ea-0fe5-4084-af5e-06e9f697049a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PID                   0\n",
              "Cleaned_Transcript    0\n",
              "15                    0\n",
              "20                    0\n",
              "ability               0\n",
              "                     ..\n",
              "Sentiment_Neu         0\n",
              "Sentiment_Pos         0\n",
              "Sentiment_Compound    0\n",
              "Embeddings            0\n",
              "DOE_Label             0\n",
              "Length: 1009, dtype: int64"
            ]
          },
          "execution_count": 191,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking for null values\n",
        "df_new.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPymY2F2lCF3"
      },
      "outputs": [],
      "source": [
        "pos_tags_df = pd.json_normalize(df_new['POS_Tags'])\n",
        "pos_tags_df.fillna(0, inplace=True)  # Replace NaN with 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "twiRCn8ilCF3",
        "outputId": "967f13fd-5aa6-4882-c970-abc52485dbc9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129, 33)"
            ]
          },
          "execution_count": 193,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pos_tags_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b1ECZA_ulCF3",
        "outputId": "49f56be2-1069-45a8-9a18-d0e62f1594fb"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 194,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# checking for Null values\n",
        "pos_tags_df.isna().sum().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyyD47chlCF3",
        "outputId": "3ada9a5a-4067-4108-d01c-bec010503c0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index([  0,   1,   2,   3,   4,   5,   7,   9,  10,  11,\n",
            "       ...\n",
            "       251, 260, 264, 265, 266, 273, 275, 278, 279, 280],\n",
            "      dtype='int64', length=129)\n",
            "RangeIndex(start=0, stop=129, step=1)\n"
          ]
        }
      ],
      "source": [
        "# Check indices of both DataFrames\n",
        "print(df_new.index)\n",
        "print(pos_tags_df.index)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Eh2Wg6TlCF3"
      },
      "outputs": [],
      "source": [
        "# Reset indices before concatenation\n",
        "df_new = df_new.reset_index(drop=True)\n",
        "pos_tags_df = pos_tags_df.reset_index(drop=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O9a8VSsZlCF7"
      },
      "outputs": [],
      "source": [
        "df_new = pd.concat([df_new, pos_tags_df], axis=1)\n",
        "df_new.drop(columns=['POS_Tags'], inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "sHjNlfSslCF7",
        "outputId": "d49de22b-5c31-453b-a8e4-7d5139308233"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "PID                   0\n",
              "Cleaned_Transcript    0\n",
              "15                    0\n",
              "20                    0\n",
              "ability               0\n",
              "                     ..\n",
              "UH                    0\n",
              "NNP                   0\n",
              "''                    0\n",
              "FW                    0\n",
              "POS                   0\n",
              "Length: 1041, dtype: int64"
            ]
          },
          "execution_count": 198,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new.isna().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QrHPAIRlCF8"
      },
      "source": [
        "In order to filter the extracted features, we also need to handle the `Embeddings` column in such a way that each value corresponds to a single feature column."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "mOBXeMkdlCF8"
      },
      "outputs": [],
      "source": [
        "# Expand Embeddings list into individual columns\n",
        "embeddings_df = pd.DataFrame(df_new['Embeddings'].to_list(), index=df_new.index)\n",
        "embeddings_df.columns = [f'Embedding_{i}' for i in range(embeddings_df.shape[1])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YNnKhnqLlCF8",
        "outputId": "3de4a917-f23c-4347-c901-a365c83ac269"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Embedding_0      0\n",
              "Embedding_1      0\n",
              "Embedding_2      0\n",
              "Embedding_3      0\n",
              "Embedding_4      0\n",
              "                ..\n",
              "Embedding_379    0\n",
              "Embedding_380    0\n",
              "Embedding_381    0\n",
              "Embedding_382    0\n",
              "Embedding_383    0\n",
              "Length: 384, dtype: int64"
            ]
          },
          "execution_count": 201,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "embeddings_df.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pajGNFwtlCF8",
        "outputId": "fbb777a0-5382-4bd0-b345-19d00bb94ccb"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>PID</th>\n",
              "      <th>Cleaned_Transcript</th>\n",
              "      <th>15</th>\n",
              "      <th>20</th>\n",
              "      <th>ability</th>\n",
              "      <th>able</th>\n",
              "      <th>about</th>\n",
              "      <th>above</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>access</th>\n",
              "      <th>...</th>\n",
              "      <th>Embedding_374</th>\n",
              "      <th>Embedding_375</th>\n",
              "      <th>Embedding_376</th>\n",
              "      <th>Embedding_377</th>\n",
              "      <th>Embedding_378</th>\n",
              "      <th>Embedding_379</th>\n",
              "      <th>Embedding_380</th>\n",
              "      <th>Embedding_381</th>\n",
              "      <th>Embedding_382</th>\n",
              "      <th>Embedding_383</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>P001</td>\n",
              "      <td>interviewer good i just uh i uh always have it...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.086689</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016676</td>\n",
              "      <td>-0.014764</td>\n",
              "      <td>0.008555</td>\n",
              "      <td>0.033396</td>\n",
              "      <td>-0.043216</td>\n",
              "      <td>0.002666</td>\n",
              "      <td>0.038463</td>\n",
              "      <td>0.003999</td>\n",
              "      <td>-0.043158</td>\n",
              "      <td>0.039700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>P001</td>\n",
              "      <td>interviewer okay good deal interviewer yep i u...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.136634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.115100</td>\n",
              "      <td>-0.059945</td>\n",
              "      <td>-0.027655</td>\n",
              "      <td>0.005605</td>\n",
              "      <td>-0.002344</td>\n",
              "      <td>-0.084586</td>\n",
              "      <td>0.067140</td>\n",
              "      <td>-0.086758</td>\n",
              "      <td>-0.083039</td>\n",
              "      <td>0.033452</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>P001</td>\n",
              "      <td>interviewer okay so with all of your experienc...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.073692</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.044904</td>\n",
              "      <td>0.088308</td>\n",
              "      <td>0.045410</td>\n",
              "      <td>0.036618</td>\n",
              "      <td>-0.042384</td>\n",
              "      <td>-0.000323</td>\n",
              "      <td>0.050070</td>\n",
              "      <td>0.018606</td>\n",
              "      <td>-0.082964</td>\n",
              "      <td>0.005994</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>P001</td>\n",
              "      <td>interviewer so was that an easy transition fro...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.013460</td>\n",
              "      <td>-0.025747</td>\n",
              "      <td>0.005615</td>\n",
              "      <td>0.049796</td>\n",
              "      <td>-0.101872</td>\n",
              "      <td>0.001147</td>\n",
              "      <td>0.017912</td>\n",
              "      <td>-0.049540</td>\n",
              "      <td>-0.002439</td>\n",
              "      <td>0.002425</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>P001</td>\n",
              "      <td>interviewer so when were talking about strengt...</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.035147</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.048642</td>\n",
              "      <td>0.073907</td>\n",
              "      <td>0.025127</td>\n",
              "      <td>0.080058</td>\n",
              "      <td>0.021308</td>\n",
              "      <td>0.004080</td>\n",
              "      <td>0.038495</td>\n",
              "      <td>0.015446</td>\n",
              "      <td>-0.098640</td>\n",
              "      <td>-0.013695</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1424 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    PID                                 Cleaned_Transcript   15   20  ability  \\\n",
              "0  P001  interviewer good i just uh i uh always have it...  0.0  0.0      0.0   \n",
              "1  P001  interviewer okay good deal interviewer yep i u...  0.0  0.0      0.0   \n",
              "2  P001  interviewer okay so with all of your experienc...  0.0  0.0      0.0   \n",
              "3  P001  interviewer so was that an easy transition fro...  0.0  0.0      0.0   \n",
              "4  P001  interviewer so when were talking about strengt...  0.0  0.0      0.0   \n",
              "\n",
              "   able     about  above  absolutely  access  ...  Embedding_374  \\\n",
              "0   0.0  0.086689    0.0         0.0     0.0  ...       0.016676   \n",
              "1   0.0  0.136634    0.0         0.0     0.0  ...      -0.115100   \n",
              "2   0.0  0.073692    0.0         0.0     0.0  ...      -0.044904   \n",
              "3   0.0  0.000000    0.0         0.0     0.0  ...      -0.013460   \n",
              "4   0.0  0.035147    0.0         0.0     0.0  ...       0.048642   \n",
              "\n",
              "   Embedding_375  Embedding_376  Embedding_377  Embedding_378  Embedding_379  \\\n",
              "0      -0.014764       0.008555       0.033396      -0.043216       0.002666   \n",
              "1      -0.059945      -0.027655       0.005605      -0.002344      -0.084586   \n",
              "2       0.088308       0.045410       0.036618      -0.042384      -0.000323   \n",
              "3      -0.025747       0.005615       0.049796      -0.101872       0.001147   \n",
              "4       0.073907       0.025127       0.080058       0.021308       0.004080   \n",
              "\n",
              "   Embedding_380  Embedding_381  Embedding_382  Embedding_383  \n",
              "0       0.038463       0.003999      -0.043158       0.039700  \n",
              "1       0.067140      -0.086758      -0.083039       0.033452  \n",
              "2       0.050070       0.018606      -0.082964       0.005994  \n",
              "3       0.017912      -0.049540      -0.002439       0.002425  \n",
              "4       0.038495       0.015446      -0.098640      -0.013695  \n",
              "\n",
              "[5 rows x 1424 columns]"
            ]
          },
          "execution_count": 202,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new = pd.concat([df_new, embeddings_df], axis=1)\n",
        "df_new.drop(columns=['Embeddings'], inplace=True)\n",
        "df_new.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6mlInsKclCF8",
        "outputId": "0eca9e02-4b91-4cf3-ec0d-77316b097cac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(129, 1424)"
            ]
          },
          "execution_count": 203,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_new.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LGHPvwGUlCF8"
      },
      "outputs": [],
      "source": [
        "# creating a list of feature columns\n",
        "feature_columns = (\n",
        "    ['Sentiment_Neg', 'Sentiment_Pos', 'Sentiment_Neu', 'Sentiment_Compound'] +\n",
        "    pos_tags_df.columns.tolist() +\n",
        "    embeddings_df.columns.tolist()\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-GcyEezlCF8"
      },
      "outputs": [],
      "source": [
        "from sklearn.feature_selection import mutual_info_classif\n",
        "\n",
        "def select_features_with_mutual_info(df, feature_columns, target_column, k_values):\n",
        "    \"\"\"\n",
        "    Selects top features for multiple values of k based on mutual information.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): Input dataframe.\n",
        "        feature_columns (list): List of feature column names.\n",
        "        target_column (str): Name of the target column.\n",
        "        k_values (list): List of different k values to experiment with.\n",
        "\n",
        "    Returns:\n",
        "        dict: Dictionary where keys are k values and values are lists of top k features.\n",
        "    \"\"\"\n",
        "    X = df[feature_columns]\n",
        "    y = df[target_column]\n",
        "\n",
        "    # Calculate mutual information scores once\n",
        "    mi_scores = mutual_info_classif(X, y, random_state=42)\n",
        "    feature_scores = pd.Series(mi_scores, index=feature_columns)\n",
        "\n",
        "    # Sort features by mutual information scores in descending order\n",
        "    sorted_features = feature_scores.sort_values(ascending=False).index.tolist()\n",
        "\n",
        "    # Generate top k features for all k values\n",
        "    results = {k: sorted_features[:k] for k in k_values}\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FfgbsZGblCF8"
      },
      "outputs": [],
      "source": [
        "filtered_features = select_features_with_mutual_info(\n",
        "    df_new,\n",
        "    feature_columns=feature_columns,\n",
        "    target_column='DOE_Label',\n",
        "    k_values=[100, 200, 400, 800]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "-5Sc3lJBlCF9",
        "outputId": "4b1182d9-38af-44a4-b351-220f3dc511d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Top 100 features: ['Embedding_332', 'WRB', 'Embedding_331', 'Embedding_230', 'Embedding_383', 'Embedding_330', 'Embedding_155', 'Embedding_133', 'Embedding_97', 'Embedding_88', 'UH', 'Embedding_114', 'Embedding_1', 'Embedding_247', 'Embedding_62', 'Embedding_316', 'Embedding_12', 'Embedding_30', 'Embedding_336', 'WDT', 'Embedding_52', 'IN', 'Embedding_44', 'Embedding_128', 'Embedding_364', 'Embedding_94', 'Embedding_182', 'Embedding_170', 'Embedding_76', 'Embedding_314', 'Embedding_131', 'Embedding_68', 'JJ', 'Embedding_226', 'Embedding_272', 'Embedding_166', 'Embedding_80', 'DT', 'Embedding_57', 'Embedding_278', 'Embedding_0', 'Embedding_359', 'Embedding_255', 'Embedding_96', 'Embedding_312', 'Embedding_346', 'Embedding_129', 'Embedding_49', 'Embedding_291', 'Embedding_210', 'Embedding_106', 'Embedding_228', 'NN', 'Embedding_305', 'Embedding_85', 'Embedding_117', 'Embedding_242', 'Embedding_2', 'Embedding_199', 'Embedding_327', 'Embedding_92', 'Embedding_326', 'Embedding_202', 'Embedding_28', 'Embedding_308', 'Embedding_32', 'VBD', 'Embedding_379', 'Embedding_307', 'Embedding_187', 'Embedding_150', 'Embedding_6', 'Embedding_176', 'Embedding_367', 'Embedding_235', 'Embedding_192', 'Embedding_351', 'Embedding_317', 'VBP', 'Embedding_263', 'Embedding_200', 'Embedding_251', 'Embedding_127', 'Embedding_122', 'Embedding_207', 'Embedding_253', 'Embedding_244', 'Embedding_159', 'Embedding_14', 'Embedding_138', 'Embedding_294', 'Embedding_342', 'Embedding_78', 'Embedding_318', 'Embedding_132', 'Embedding_321', 'Embedding_124', 'Embedding_347', 'Embedding_298', 'Embedding_160'] \n",
            "\n",
            "Top 200 features: ['Embedding_332', 'WRB', 'Embedding_331', 'Embedding_230', 'Embedding_383', 'Embedding_330', 'Embedding_155', 'Embedding_133', 'Embedding_97', 'Embedding_88', 'UH', 'Embedding_114', 'Embedding_1', 'Embedding_247', 'Embedding_62', 'Embedding_316', 'Embedding_12', 'Embedding_30', 'Embedding_336', 'WDT', 'Embedding_52', 'IN', 'Embedding_44', 'Embedding_128', 'Embedding_364', 'Embedding_94', 'Embedding_182', 'Embedding_170', 'Embedding_76', 'Embedding_314', 'Embedding_131', 'Embedding_68', 'JJ', 'Embedding_226', 'Embedding_272', 'Embedding_166', 'Embedding_80', 'DT', 'Embedding_57', 'Embedding_278', 'Embedding_0', 'Embedding_359', 'Embedding_255', 'Embedding_96', 'Embedding_312', 'Embedding_346', 'Embedding_129', 'Embedding_49', 'Embedding_291', 'Embedding_210', 'Embedding_106', 'Embedding_228', 'NN', 'Embedding_305', 'Embedding_85', 'Embedding_117', 'Embedding_242', 'Embedding_2', 'Embedding_199', 'Embedding_327', 'Embedding_92', 'Embedding_326', 'Embedding_202', 'Embedding_28', 'Embedding_308', 'Embedding_32', 'VBD', 'Embedding_379', 'Embedding_307', 'Embedding_187', 'Embedding_150', 'Embedding_6', 'Embedding_176', 'Embedding_367', 'Embedding_235', 'Embedding_192', 'Embedding_351', 'Embedding_317', 'VBP', 'Embedding_263', 'Embedding_200', 'Embedding_251', 'Embedding_127', 'Embedding_122', 'Embedding_207', 'Embedding_253', 'Embedding_244', 'Embedding_159', 'Embedding_14', 'Embedding_138', 'Embedding_294', 'Embedding_342', 'Embedding_78', 'Embedding_318', 'Embedding_132', 'Embedding_321', 'Embedding_124', 'Embedding_347', 'Embedding_298', 'Embedding_160', 'Embedding_319', 'Embedding_51', 'Embedding_118', 'Embedding_46', 'Embedding_213', 'Embedding_16', 'Embedding_215', 'Embedding_339', 'Embedding_222', 'Embedding_334', 'Embedding_143', 'Embedding_5', 'Embedding_241', 'PRP', 'Embedding_335', 'Embedding_87', 'Embedding_183', 'Embedding_328', 'Embedding_209', 'EX', 'Embedding_178', 'VB', 'Embedding_239', 'Embedding_26', 'Embedding_353', 'Embedding_329', 'Embedding_119', 'Embedding_286', 'Embedding_100', 'Embedding_13', 'Embedding_306', 'Embedding_340', 'Embedding_382', 'Embedding_333', 'Embedding_40', 'Embedding_21', 'Embedding_60', 'Embedding_360', 'Embedding_293', 'Embedding_231', 'Embedding_323', 'Embedding_376', 'Embedding_75', 'Embedding_135', 'JJS', 'Embedding_54', 'NNS', 'Embedding_352', 'Sentiment_Pos', 'Embedding_146', 'Embedding_158', 'Embedding_288', 'Embedding_152', 'Embedding_10', 'Embedding_344', 'RBR', 'Embedding_15', 'Embedding_53', 'Embedding_221', 'Embedding_341', 'Embedding_105', 'Embedding_284', 'Embedding_249', 'Embedding_171', 'Embedding_147', 'PRP$', 'Embedding_292', 'Embedding_212', 'Embedding_39', 'Embedding_358', 'Embedding_123', 'TO', 'Embedding_145', 'Embedding_191', 'Embedding_34', 'Embedding_194', 'Embedding_8', 'Embedding_29', 'Embedding_7', 'Sentiment_Neg', 'Embedding_137', 'Embedding_254', 'FW', 'Embedding_151', 'Embedding_56', 'Embedding_58', 'Embedding_113', 'Embedding_3', 'Embedding_172', 'Embedding_83', 'Embedding_11', 'Embedding_279', 'Embedding_165', 'Embedding_139', 'Embedding_259', 'Embedding_283', 'VBN', 'Embedding_313', 'Embedding_112', 'Embedding_71'] \n",
            "\n",
            "Top 400 features: ['Embedding_332', 'WRB', 'Embedding_331', 'Embedding_230', 'Embedding_383', 'Embedding_330', 'Embedding_155', 'Embedding_133', 'Embedding_97', 'Embedding_88', 'UH', 'Embedding_114', 'Embedding_1', 'Embedding_247', 'Embedding_62', 'Embedding_316', 'Embedding_12', 'Embedding_30', 'Embedding_336', 'WDT', 'Embedding_52', 'IN', 'Embedding_44', 'Embedding_128', 'Embedding_364', 'Embedding_94', 'Embedding_182', 'Embedding_170', 'Embedding_76', 'Embedding_314', 'Embedding_131', 'Embedding_68', 'JJ', 'Embedding_226', 'Embedding_272', 'Embedding_166', 'Embedding_80', 'DT', 'Embedding_57', 'Embedding_278', 'Embedding_0', 'Embedding_359', 'Embedding_255', 'Embedding_96', 'Embedding_312', 'Embedding_346', 'Embedding_129', 'Embedding_49', 'Embedding_291', 'Embedding_210', 'Embedding_106', 'Embedding_228', 'NN', 'Embedding_305', 'Embedding_85', 'Embedding_117', 'Embedding_242', 'Embedding_2', 'Embedding_199', 'Embedding_327', 'Embedding_92', 'Embedding_326', 'Embedding_202', 'Embedding_28', 'Embedding_308', 'Embedding_32', 'VBD', 'Embedding_379', 'Embedding_307', 'Embedding_187', 'Embedding_150', 'Embedding_6', 'Embedding_176', 'Embedding_367', 'Embedding_235', 'Embedding_192', 'Embedding_351', 'Embedding_317', 'VBP', 'Embedding_263', 'Embedding_200', 'Embedding_251', 'Embedding_127', 'Embedding_122', 'Embedding_207', 'Embedding_253', 'Embedding_244', 'Embedding_159', 'Embedding_14', 'Embedding_138', 'Embedding_294', 'Embedding_342', 'Embedding_78', 'Embedding_318', 'Embedding_132', 'Embedding_321', 'Embedding_124', 'Embedding_347', 'Embedding_298', 'Embedding_160', 'Embedding_319', 'Embedding_51', 'Embedding_118', 'Embedding_46', 'Embedding_213', 'Embedding_16', 'Embedding_215', 'Embedding_339', 'Embedding_222', 'Embedding_334', 'Embedding_143', 'Embedding_5', 'Embedding_241', 'PRP', 'Embedding_335', 'Embedding_87', 'Embedding_183', 'Embedding_328', 'Embedding_209', 'EX', 'Embedding_178', 'VB', 'Embedding_239', 'Embedding_26', 'Embedding_353', 'Embedding_329', 'Embedding_119', 'Embedding_286', 'Embedding_100', 'Embedding_13', 'Embedding_306', 'Embedding_340', 'Embedding_382', 'Embedding_333', 'Embedding_40', 'Embedding_21', 'Embedding_60', 'Embedding_360', 'Embedding_293', 'Embedding_231', 'Embedding_323', 'Embedding_376', 'Embedding_75', 'Embedding_135', 'JJS', 'Embedding_54', 'NNS', 'Embedding_352', 'Sentiment_Pos', 'Embedding_146', 'Embedding_158', 'Embedding_288', 'Embedding_152', 'Embedding_10', 'Embedding_344', 'RBR', 'Embedding_15', 'Embedding_53', 'Embedding_221', 'Embedding_341', 'Embedding_105', 'Embedding_284', 'Embedding_249', 'Embedding_171', 'Embedding_147', 'PRP$', 'Embedding_292', 'Embedding_212', 'Embedding_39', 'Embedding_358', 'Embedding_123', 'TO', 'Embedding_145', 'Embedding_191', 'Embedding_34', 'Embedding_194', 'Embedding_8', 'Embedding_29', 'Embedding_7', 'Sentiment_Neg', 'Embedding_137', 'Embedding_254', 'FW', 'Embedding_151', 'Embedding_56', 'Embedding_58', 'Embedding_113', 'Embedding_3', 'Embedding_172', 'Embedding_83', 'Embedding_11', 'Embedding_279', 'Embedding_165', 'Embedding_139', 'Embedding_259', 'Embedding_283', 'VBN', 'Embedding_313', 'Embedding_112', 'Embedding_71', 'Embedding_193', 'Embedding_35', 'Embedding_98', 'Embedding_164', 'Embedding_345', 'Embedding_102', 'Embedding_185', 'Embedding_188', 'Embedding_136', 'Embedding_72', 'Embedding_31', 'Embedding_69', 'Embedding_177', 'Embedding_363', 'Embedding_365', 'Embedding_310', 'Embedding_311', 'Embedding_309', 'Embedding_38', 'Embedding_37', 'Embedding_315', 'Embedding_41', 'Embedding_36', 'Embedding_42', 'Embedding_33', 'Embedding_320', 'Embedding_43', 'Embedding_45', 'Embedding_27', 'Embedding_304', 'Embedding_296', 'Embedding_303', 'Embedding_289', 'Embedding_276', 'Embedding_277', 'Embedding_63', 'Embedding_280', 'Embedding_281', 'Embedding_282', 'Embedding_61', 'Embedding_285', 'Embedding_59', 'Embedding_287', 'Embedding_290', 'Embedding_302', 'Embedding_55', 'Embedding_50', 'Embedding_48', 'Embedding_295', 'Embedding_25', 'Embedding_297', 'Embedding_47', 'Embedding_299', 'Embedding_300', 'Embedding_301', 'Embedding_322', 'Embedding_142', 'Embedding_324', 'Embedding_370', 'Embedding_356', 'Embedding_357', 'VBG', 'MD', 'Embedding_361', 'Embedding_362', 'CC', 'Embedding_366', 'RP', 'Embedding_368', 'Embedding_369', 'Embedding_371', 'Embedding_325', 'Embedding_372', 'Embedding_373', 'Embedding_374', 'Embedding_375', 'CD', 'Embedding_377', 'Embedding_378', 'Sentiment_Compound', 'Embedding_380', 'Embedding_381', 'Sentiment_Neu', 'Embedding_355', 'Embedding_354', 'VBZ', 'RB', 'Embedding_24', 'Embedding_23', 'Embedding_22', 'Embedding_20', 'Embedding_19', 'Embedding_18', 'Embedding_17', 'Embedding_274', 'Embedding_9', 'Embedding_4', 'Embedding_337', 'Embedding_338', 'POS', \"''\", 'NNP', 'Embedding_343', 'RBS', 'JJR', 'PDT', 'Embedding_348', 'Embedding_349', 'Embedding_350', 'WP', 'Embedding_275', 'Embedding_65', 'Embedding_273', 'Embedding_196', 'Embedding_111', 'Embedding_110', 'Embedding_184', 'Embedding_186', 'Embedding_109', 'Embedding_189', 'Embedding_190', 'Embedding_108', 'Embedding_107', 'Embedding_104', 'Embedding_195', 'Embedding_197', 'Embedding_93', 'Embedding_198', 'Embedding_103', 'Embedding_201', 'Embedding_101', 'Embedding_203', 'Embedding_204', 'Embedding_205', 'Embedding_206', 'Embedding_99', 'Embedding_208', 'Embedding_95', 'Embedding_181', 'Embedding_180', 'Embedding_179', 'Embedding_175', 'Embedding_141', 'Embedding_140', 'Embedding_148', 'Embedding_149', 'Embedding_153', 'Embedding_154', 'Embedding_134', 'Embedding_156', 'Embedding_157', 'Embedding_130', 'Embedding_126', 'Embedding_125', 'Embedding_161', 'Embedding_162', 'Embedding_163', 'Embedding_121', 'Embedding_167', 'Embedding_168', 'Embedding_169', 'Embedding_120', 'Embedding_116', 'Embedding_115', 'Embedding_174', 'Embedding_211', 'Embedding_91', 'Embedding_64', 'Embedding_258', 'Embedding_243', 'Embedding_245', 'Embedding_246', 'Embedding_248', 'Embedding_70', 'Embedding_250', 'Embedding_252', 'Embedding_67', 'Embedding_66', 'Embedding_256', 'Embedding_257', 'Embedding_260', 'Embedding_214', 'Embedding_261', 'Embedding_262', 'Embedding_144', 'Embedding_264', 'Embedding_265', 'Embedding_266', 'Embedding_267', 'Embedding_268', 'Embedding_269', 'Embedding_270', 'Embedding_271', 'Embedding_73', 'Embedding_74', 'Embedding_240', 'Embedding_77', 'Embedding_90', 'Embedding_216', 'Embedding_217'] \n",
            "\n",
            "Top 800 features: ['Embedding_332', 'WRB', 'Embedding_331', 'Embedding_230', 'Embedding_383', 'Embedding_330', 'Embedding_155', 'Embedding_133', 'Embedding_97', 'Embedding_88', 'UH', 'Embedding_114', 'Embedding_1', 'Embedding_247', 'Embedding_62', 'Embedding_316', 'Embedding_12', 'Embedding_30', 'Embedding_336', 'WDT', 'Embedding_52', 'IN', 'Embedding_44', 'Embedding_128', 'Embedding_364', 'Embedding_94', 'Embedding_182', 'Embedding_170', 'Embedding_76', 'Embedding_314', 'Embedding_131', 'Embedding_68', 'JJ', 'Embedding_226', 'Embedding_272', 'Embedding_166', 'Embedding_80', 'DT', 'Embedding_57', 'Embedding_278', 'Embedding_0', 'Embedding_359', 'Embedding_255', 'Embedding_96', 'Embedding_312', 'Embedding_346', 'Embedding_129', 'Embedding_49', 'Embedding_291', 'Embedding_210', 'Embedding_106', 'Embedding_228', 'NN', 'Embedding_305', 'Embedding_85', 'Embedding_117', 'Embedding_242', 'Embedding_2', 'Embedding_199', 'Embedding_327', 'Embedding_92', 'Embedding_326', 'Embedding_202', 'Embedding_28', 'Embedding_308', 'Embedding_32', 'VBD', 'Embedding_379', 'Embedding_307', 'Embedding_187', 'Embedding_150', 'Embedding_6', 'Embedding_176', 'Embedding_367', 'Embedding_235', 'Embedding_192', 'Embedding_351', 'Embedding_317', 'VBP', 'Embedding_263', 'Embedding_200', 'Embedding_251', 'Embedding_127', 'Embedding_122', 'Embedding_207', 'Embedding_253', 'Embedding_244', 'Embedding_159', 'Embedding_14', 'Embedding_138', 'Embedding_294', 'Embedding_342', 'Embedding_78', 'Embedding_318', 'Embedding_132', 'Embedding_321', 'Embedding_124', 'Embedding_347', 'Embedding_298', 'Embedding_160', 'Embedding_319', 'Embedding_51', 'Embedding_118', 'Embedding_46', 'Embedding_213', 'Embedding_16', 'Embedding_215', 'Embedding_339', 'Embedding_222', 'Embedding_334', 'Embedding_143', 'Embedding_5', 'Embedding_241', 'PRP', 'Embedding_335', 'Embedding_87', 'Embedding_183', 'Embedding_328', 'Embedding_209', 'EX', 'Embedding_178', 'VB', 'Embedding_239', 'Embedding_26', 'Embedding_353', 'Embedding_329', 'Embedding_119', 'Embedding_286', 'Embedding_100', 'Embedding_13', 'Embedding_306', 'Embedding_340', 'Embedding_382', 'Embedding_333', 'Embedding_40', 'Embedding_21', 'Embedding_60', 'Embedding_360', 'Embedding_293', 'Embedding_231', 'Embedding_323', 'Embedding_376', 'Embedding_75', 'Embedding_135', 'JJS', 'Embedding_54', 'NNS', 'Embedding_352', 'Sentiment_Pos', 'Embedding_146', 'Embedding_158', 'Embedding_288', 'Embedding_152', 'Embedding_10', 'Embedding_344', 'RBR', 'Embedding_15', 'Embedding_53', 'Embedding_221', 'Embedding_341', 'Embedding_105', 'Embedding_284', 'Embedding_249', 'Embedding_171', 'Embedding_147', 'PRP$', 'Embedding_292', 'Embedding_212', 'Embedding_39', 'Embedding_358', 'Embedding_123', 'TO', 'Embedding_145', 'Embedding_191', 'Embedding_34', 'Embedding_194', 'Embedding_8', 'Embedding_29', 'Embedding_7', 'Sentiment_Neg', 'Embedding_137', 'Embedding_254', 'FW', 'Embedding_151', 'Embedding_56', 'Embedding_58', 'Embedding_113', 'Embedding_3', 'Embedding_172', 'Embedding_83', 'Embedding_11', 'Embedding_279', 'Embedding_165', 'Embedding_139', 'Embedding_259', 'Embedding_283', 'VBN', 'Embedding_313', 'Embedding_112', 'Embedding_71', 'Embedding_193', 'Embedding_35', 'Embedding_98', 'Embedding_164', 'Embedding_345', 'Embedding_102', 'Embedding_185', 'Embedding_188', 'Embedding_136', 'Embedding_72', 'Embedding_31', 'Embedding_69', 'Embedding_177', 'Embedding_363', 'Embedding_365', 'Embedding_310', 'Embedding_311', 'Embedding_309', 'Embedding_38', 'Embedding_37', 'Embedding_315', 'Embedding_41', 'Embedding_36', 'Embedding_42', 'Embedding_33', 'Embedding_320', 'Embedding_43', 'Embedding_45', 'Embedding_27', 'Embedding_304', 'Embedding_296', 'Embedding_303', 'Embedding_289', 'Embedding_276', 'Embedding_277', 'Embedding_63', 'Embedding_280', 'Embedding_281', 'Embedding_282', 'Embedding_61', 'Embedding_285', 'Embedding_59', 'Embedding_287', 'Embedding_290', 'Embedding_302', 'Embedding_55', 'Embedding_50', 'Embedding_48', 'Embedding_295', 'Embedding_25', 'Embedding_297', 'Embedding_47', 'Embedding_299', 'Embedding_300', 'Embedding_301', 'Embedding_322', 'Embedding_142', 'Embedding_324', 'Embedding_370', 'Embedding_356', 'Embedding_357', 'VBG', 'MD', 'Embedding_361', 'Embedding_362', 'CC', 'Embedding_366', 'RP', 'Embedding_368', 'Embedding_369', 'Embedding_371', 'Embedding_325', 'Embedding_372', 'Embedding_373', 'Embedding_374', 'Embedding_375', 'CD', 'Embedding_377', 'Embedding_378', 'Sentiment_Compound', 'Embedding_380', 'Embedding_381', 'Sentiment_Neu', 'Embedding_355', 'Embedding_354', 'VBZ', 'RB', 'Embedding_24', 'Embedding_23', 'Embedding_22', 'Embedding_20', 'Embedding_19', 'Embedding_18', 'Embedding_17', 'Embedding_274', 'Embedding_9', 'Embedding_4', 'Embedding_337', 'Embedding_338', 'POS', \"''\", 'NNP', 'Embedding_343', 'RBS', 'JJR', 'PDT', 'Embedding_348', 'Embedding_349', 'Embedding_350', 'WP', 'Embedding_275', 'Embedding_65', 'Embedding_273', 'Embedding_196', 'Embedding_111', 'Embedding_110', 'Embedding_184', 'Embedding_186', 'Embedding_109', 'Embedding_189', 'Embedding_190', 'Embedding_108', 'Embedding_107', 'Embedding_104', 'Embedding_195', 'Embedding_197', 'Embedding_93', 'Embedding_198', 'Embedding_103', 'Embedding_201', 'Embedding_101', 'Embedding_203', 'Embedding_204', 'Embedding_205', 'Embedding_206', 'Embedding_99', 'Embedding_208', 'Embedding_95', 'Embedding_181', 'Embedding_180', 'Embedding_179', 'Embedding_175', 'Embedding_141', 'Embedding_140', 'Embedding_148', 'Embedding_149', 'Embedding_153', 'Embedding_154', 'Embedding_134', 'Embedding_156', 'Embedding_157', 'Embedding_130', 'Embedding_126', 'Embedding_125', 'Embedding_161', 'Embedding_162', 'Embedding_163', 'Embedding_121', 'Embedding_167', 'Embedding_168', 'Embedding_169', 'Embedding_120', 'Embedding_116', 'Embedding_115', 'Embedding_174', 'Embedding_211', 'Embedding_91', 'Embedding_64', 'Embedding_258', 'Embedding_243', 'Embedding_245', 'Embedding_246', 'Embedding_248', 'Embedding_70', 'Embedding_250', 'Embedding_252', 'Embedding_67', 'Embedding_66', 'Embedding_256', 'Embedding_257', 'Embedding_260', 'Embedding_214', 'Embedding_261', 'Embedding_262', 'Embedding_144', 'Embedding_264', 'Embedding_265', 'Embedding_266', 'Embedding_267', 'Embedding_268', 'Embedding_269', 'Embedding_270', 'Embedding_271', 'Embedding_73', 'Embedding_74', 'Embedding_240', 'Embedding_77', 'Embedding_90', 'Embedding_216', 'Embedding_217', 'Embedding_218', 'Embedding_219', 'Embedding_220', 'Embedding_89', 'Embedding_86', 'Embedding_223', 'Embedding_224', 'Embedding_225', 'Embedding_84', 'Embedding_227', 'Embedding_229', 'Embedding_82', 'Embedding_81', 'Embedding_232', 'Embedding_233', 'Embedding_234', 'Embedding_79', 'Embedding_236', 'Embedding_237', 'Embedding_238', 'Embedding_173'] \n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Print results for each k\n",
        "for k, features in filtered_features.items():\n",
        "    print(f\"Top {k} features: {features} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vE-GciiilCF9"
      },
      "source": [
        "## Splitting the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fMQRFcb9lCF9"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import GroupKFold\n",
        "\n",
        "def participant_independent_split(df, feature_columns, target_column, group_column, n_splits=5):\n",
        "    \"\"\"\n",
        "    Splits the dataset into participant-independent folds using GroupKFold.\n",
        "\n",
        "    Parameters:\n",
        "        df (pd.DataFrame): The input dataset containing features, target, and group columns.\n",
        "        feature_columns (list): List of column names to be used as features.\n",
        "        target_column (str): Name of the target column.\n",
        "        group_column (str): Name of the column used for grouping (e.g., Participant ID).\n",
        "        n_splits (int): Number of folds (default is 5).\n",
        "\n",
        "    Returns:\n",
        "        list of tuples: Each tuple contains (X_train, X_test, y_train, y_test) for one fold.\n",
        "    \"\"\"\n",
        "    # Extract features, target, and groups\n",
        "    X = df[feature_columns]\n",
        "    y = df[target_column]\n",
        "    groups = df[group_column]\n",
        "\n",
        "    # Initialize GroupKFold\n",
        "    gkf = GroupKFold(n_splits=n_splits)\n",
        "\n",
        "    # Store splits\n",
        "    splits = []\n",
        "    for train_idx, test_idx in gkf.split(X, y, groups):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "        splits.append((X_train, X_test, y_train, y_test))\n",
        "\n",
        "    return splits"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "hL5pbIK5lCF9",
        "outputId": "479f1fd7-8a7c-4ad6-99d5-d974ea887d8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Train PIDs: ['P001' 'P002' 'P004' 'P006' 'P007' 'P010' 'P011' 'P012' 'P013' 'P014'\n",
            " 'P016' 'P017' 'P018' 'P019' 'P023' 'P024' 'P025' 'P027' 'P028' 'P030'\n",
            " 'P031' 'P032' 'P033' 'P038' 'P039' 'P041']\n",
            "Test PIDs: ['P003' 'P005' 'P008' 'P009' 'P029' 'P036']\n",
            "--------------------------------------------------\n",
            "Fold 2\n",
            "Train PIDs: ['P001' 'P002' 'P003' 'P004' 'P005' 'P006' 'P007' 'P008' 'P009' 'P010'\n",
            " 'P011' 'P012' 'P014' 'P016' 'P019' 'P023' 'P027' 'P029' 'P030' 'P031'\n",
            " 'P032' 'P033' 'P036' 'P038' 'P039' 'P041']\n",
            "Test PIDs: ['P013' 'P017' 'P018' 'P024' 'P025' 'P028']\n",
            "--------------------------------------------------\n",
            "Fold 3\n",
            "Train PIDs: ['P002' 'P003' 'P005' 'P006' 'P007' 'P008' 'P009' 'P010' 'P011' 'P012'\n",
            " 'P013' 'P014' 'P016' 'P017' 'P018' 'P019' 'P023' 'P024' 'P025' 'P028'\n",
            " 'P029' 'P031' 'P033' 'P036' 'P039']\n",
            "Test PIDs: ['P001' 'P004' 'P027' 'P030' 'P032' 'P038' 'P041']\n",
            "--------------------------------------------------\n",
            "Fold 4\n",
            "Train PIDs: ['P001' 'P003' 'P004' 'P005' 'P006' 'P007' 'P008' 'P009' 'P012' 'P013'\n",
            " 'P017' 'P018' 'P019' 'P023' 'P024' 'P025' 'P027' 'P028' 'P029' 'P030'\n",
            " 'P032' 'P033' 'P036' 'P038' 'P041']\n",
            "Test PIDs: ['P002' 'P010' 'P011' 'P014' 'P016' 'P031' 'P039']\n",
            "--------------------------------------------------\n",
            "Fold 5\n",
            "Train PIDs: ['P001' 'P002' 'P003' 'P004' 'P005' 'P008' 'P009' 'P010' 'P011' 'P013'\n",
            " 'P014' 'P016' 'P017' 'P018' 'P024' 'P025' 'P027' 'P028' 'P029' 'P030'\n",
            " 'P031' 'P032' 'P036' 'P038' 'P039' 'P041']\n",
            "Test PIDs: ['P006' 'P007' 'P012' 'P019' 'P023' 'P033']\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "target_column = 'DOE_Label'  # Replace with the actual target column name\n",
        "group_column = 'PID'  # Replace with the participant ID column name\n",
        "\n",
        "# Call the function\n",
        "folds = participant_independent_split(df_new, feature_columns, target_column, group_column, n_splits=5)\n",
        "\n",
        "# Display train and test sets for each fold\n",
        "for i, (X_train, X_test, y_train, y_test) in enumerate(folds):\n",
        "    print(f\"Fold {i+1}\")\n",
        "    print(\"Train PIDs:\", df_new[group_column].iloc[X_train.index].unique())\n",
        "    print(\"Test PIDs:\", df_new[group_column].iloc[X_test.index].unique())\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "wSucZ65ElCF9",
        "outputId": "0a6cf909-ad97-4493-e9f5-7b3745ce73ca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1:\n",
            "  Train sample shapes: (103, 421)\n",
            "  Test samples shapes: (26, 421)\n",
            "--------------------------------------------------\n",
            "Fold 2:\n",
            "  Train sample shapes: (103, 421)\n",
            "  Test samples shapes: (26, 421)\n",
            "--------------------------------------------------\n",
            "Fold 3:\n",
            "  Train sample shapes: (103, 421)\n",
            "  Test samples shapes: (26, 421)\n",
            "--------------------------------------------------\n",
            "Fold 4:\n",
            "  Train sample shapes: (103, 421)\n",
            "  Test samples shapes: (26, 421)\n",
            "--------------------------------------------------\n",
            "Fold 5:\n",
            "  Train sample shapes: (104, 421)\n",
            "  Test samples shapes: (25, 421)\n",
            "--------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# check the number of samples in each fold\n",
        "for i, (X_train, X_test, y_train, y_test) in enumerate(folds):\n",
        "    print(f\"Fold {i+1}:\")\n",
        "    print(f\"  Train sample shapes: {X_train.shape}\")\n",
        "    print(f\"  Test samples shapes: {X_test.shape}\")\n",
        "    print(\"-\" * 50)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4nxWSO0GlCF9"
      },
      "source": [
        "## Running Tree-Based ML Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I7pCtwN8lCF9"
      },
      "outputs": [],
      "source": [
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "\n",
        "def evaluate_tree_models(folds, k_features, feature_columns, target_column, top_features_dict):\n",
        "    \"\"\"\n",
        "    Evaluates three tree-based models (Decision Tree, Random Forest, Gradient Boosting)\n",
        "    with hyperparameter tuning for each k value and identifies the best model.\n",
        "\n",
        "    Parameters:\n",
        "        folds (list of tuples): Participant-independent splits (X_train, X_test, y_train, y_test).\n",
        "        k_features (list): List of k values representing the number of top features to use.\n",
        "        feature_columns (list): List of feature column names.\n",
        "        target_column (str): Name of the target column.\n",
        "        top_features_dict (dict): Dictionary mapping each k value to the list of top features.\n",
        "\n",
        "    Returns:\n",
        "        dict: Results for each k value containing the best model and its performance metrics.\n",
        "    \"\"\"\n",
        "    # Define the models and their hyperparameter grids\n",
        "    models = {\n",
        "        \"Decision Tree\": {\n",
        "            \"model\": DecisionTreeClassifier(random_state=42),\n",
        "            \"param_grid\": {\"max_depth\": [3, 5, 10, None], \"min_samples_split\": [2, 5, 10]},\n",
        "        },\n",
        "        \"Random Forest\": {\n",
        "            \"model\": RandomForestClassifier(random_state=42),\n",
        "            \"param_grid\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                \"max_depth\": [3, 5, 10, None],\n",
        "                \"min_samples_split\": [2, 5, 10],\n",
        "            },\n",
        "        },\n",
        "        \"Gradient Boosting\": {\n",
        "            \"model\": GradientBoostingClassifier(random_state=42),\n",
        "            \"param_grid\": {\n",
        "                \"n_estimators\": [50, 100, 200],\n",
        "                \"learning_rate\": [0.01, 0.1, 0.2],\n",
        "                \"max_depth\": [3, 5, 10],\n",
        "            },\n",
        "        },\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # Iterate over each k value\n",
        "    for k in k_features:\n",
        "        print(f\"Evaluating models for top {k} features...\")\n",
        "        top_features = top_features_dict[k]  # Get the top k features\n",
        "\n",
        "        best_model_name = None\n",
        "        best_model = None\n",
        "        best_accuracy = 0\n",
        "        best_balanced_accuracy = 0\n",
        "        best_params = None\n",
        "\n",
        "        # Iterate over models\n",
        "        for model_name, model_info in models.items():\n",
        "            print(f\"Training {model_name}...\")\n",
        "            total_accuracy = 0\n",
        "            total_balanced_accuracy = 0\n",
        "\n",
        "            # Iterate over folds\n",
        "            for X_train, X_test, y_train, y_test in folds:\n",
        "                # Filter top k features for the current fold\n",
        "                X_train_k = X_train[top_features]\n",
        "                X_test_k = X_test[top_features]\n",
        "\n",
        "                # Perform grid search\n",
        "                grid_search = GridSearchCV(\n",
        "                    model_info[\"model\"],\n",
        "                    model_info[\"param_grid\"],\n",
        "                    scoring=\"accuracy\",\n",
        "                    cv=3,\n",
        "                    n_jobs=-1,\n",
        "                )\n",
        "                grid_search.fit(X_train_k, y_train)\n",
        "\n",
        "                # Evaluate on test set\n",
        "                best_estimator = grid_search.best_estimator_\n",
        "                y_pred = best_estimator.predict(X_test_k)\n",
        "                accuracy = accuracy_score(y_test, y_pred)\n",
        "                balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "                # Accumulate scores\n",
        "                total_accuracy += accuracy\n",
        "                total_balanced_accuracy += balanced_accuracy\n",
        "\n",
        "            # Average scores over all folds\n",
        "            avg_accuracy = total_accuracy / len(folds)\n",
        "            avg_balanced_accuracy = total_balanced_accuracy / len(folds)\n",
        "\n",
        "            print(f\"{model_name}: Accuracy={avg_accuracy:.4f}, Balanced Accuracy={avg_balanced_accuracy:.4f} \\n\")\n",
        "\n",
        "            # Update best model if this model performs better\n",
        "            if (avg_accuracy > best_accuracy) and (avg_balanced_accuracy > best_balanced_accuracy):\n",
        "                best_model_name = model_name\n",
        "                best_model = grid_search.best_estimator_\n",
        "                best_accuracy = avg_accuracy\n",
        "                best_balanced_accuracy = avg_balanced_accuracy\n",
        "                best_params = grid_search.best_params_\n",
        "\n",
        "        # Store results for this k value\n",
        "        results[k] = {\n",
        "            \"Best Model\": best_model_name,\n",
        "            \"Best Accuracy\": best_accuracy,\n",
        "            \"Best Balanced Accuracy\": best_balanced_accuracy,\n",
        "            \"Best Parameters\": best_params,\n",
        "            \"Best Model Object\": best_model,\n",
        "        }\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8vOGqJ19lCF-",
        "outputId": "7cb0ef6a-8048-4ff7-b609-7d8b3f2cb6f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[100, 200, 400, 800]\n"
          ]
        }
      ],
      "source": [
        "k_values_list = list(filtered_features.keys())\n",
        "print(k_values_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "WxO752bmlCF-",
        "outputId": "102cc1d1-d62e-4fed-e232-bf654955ca89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating models for top 100 features...\n",
            "Training Decision Tree...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Decision Tree: Accuracy=0.7132, Balanced Accuracy=0.5062 \n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest: Accuracy=0.8209, Balanced Accuracy=0.5000 \n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting: Accuracy=0.8209, Balanced Accuracy=0.5205 \n",
            "\n",
            "Evaluating models for top 200 features...\n",
            "Training Decision Tree...\n",
            "Decision Tree: Accuracy=0.7283, Balanced Accuracy=0.5132 \n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest: Accuracy=0.8209, Balanced Accuracy=0.5000 \n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting: Accuracy=0.8209, Balanced Accuracy=0.5000 \n",
            "\n",
            "Evaluating models for top 400 features...\n",
            "Training Decision Tree...\n",
            "Decision Tree: Accuracy=0.7514, Balanced Accuracy=0.5193 \n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest: Accuracy=0.8209, Balanced Accuracy=0.5000 \n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting: Accuracy=0.8209, Balanced Accuracy=0.5000 \n",
            "\n",
            "Evaluating models for top 800 features...\n",
            "Training Decision Tree...\n",
            "Decision Tree: Accuracy=0.7209, Balanced Accuracy=0.5315 \n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest: Accuracy=0.8209, Balanced Accuracy=0.5000 \n",
            "\n",
            "Training Gradient Boosting...\n",
            "Gradient Boosting: Accuracy=0.8055, Balanced Accuracy=0.5021 \n",
            "\n",
            "Top 100 features:\n",
            "Best Model: Gradient Boosting\n",
            "Best Accuracy: 0.8209\n",
            "Best Balanced Accuracy: 0.5205\n",
            "Best Parameters: {'learning_rate': 0.01, 'max_depth': 3, 'n_estimators': 50}\n",
            "\n",
            "\n",
            "Top 200 features:\n",
            "Best Model: Decision Tree\n",
            "Best Accuracy: 0.7283\n",
            "Best Balanced Accuracy: 0.5132\n",
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "\n",
            "\n",
            "Top 400 features:\n",
            "Best Model: Decision Tree\n",
            "Best Accuracy: 0.7514\n",
            "Best Balanced Accuracy: 0.5193\n",
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "\n",
            "\n",
            "Top 800 features:\n",
            "Best Model: Decision Tree\n",
            "Best Accuracy: 0.7209\n",
            "Best Balanced Accuracy: 0.5315\n",
            "Best Parameters: {'max_depth': 3, 'min_samples_split': 2}\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Example call\n",
        "results = evaluate_tree_models(\n",
        "    folds=folds,\n",
        "    k_features=k_values_list,\n",
        "    feature_columns=feature_columns,\n",
        "    target_column=target_column,\n",
        "    top_features_dict=filtered_features\n",
        ")\n",
        "\n",
        "# Display the best model for each k\n",
        "for k, result in results.items():\n",
        "    print(f\"Top {k} features:\")\n",
        "    print(f\"Best Model: {result['Best Model']}\")\n",
        "    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n",
        "    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\")\n",
        "    print(f\"Best Parameters: {result['Best Parameters']}\")\n",
        "    print(\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "halm67kClCF-",
        "outputId": "a30a8b8f-2f43-481f-efa1-097431c02a44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model for top 100 features as 'partc_best_model_top_100_features.pkl'.\n",
            "Saved best model for top 200 features as 'partc_best_model_top_200_features.pkl'.\n",
            "Saved best model for top 400 features as 'partc_best_model_top_400_features.pkl'.\n",
            "Saved best model for top 800 features as 'partc_best_model_top_800_features.pkl'.\n"
          ]
        }
      ],
      "source": [
        "import joblib\n",
        "\n",
        "# Iterate over results to save the best model for each k\n",
        "for k, result in results.items():\n",
        "    best_model = result[\"Best Model Object\"]  # Get the best model object\n",
        "    filename = f\"partc_best_model_top_{k}_features.pkl\"  # Define a filename\n",
        "    joblib.dump(best_model, filename)  # Save the model\n",
        "    print(f\"Saved best model for top {k} features as '{filename}'.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3diobJsIlCF-"
      },
      "source": [
        "## Running Deep Learning Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZbA1Ia93lCF-"
      },
      "source": [
        "In order to run deep learning models such as `Conv1D` and `LSTM`, we first need to scale and reshape the input dataset because these models require their inputs to be in a specific format and shape."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_3qEWUi2lCF_"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "def prepare_data_for_dl(X, top_features):\n",
        "    \"\"\"\n",
        "    Scales and reshapes data for Conv1D and LSTM models.\n",
        "\n",
        "    Parameters:\n",
        "        X (pd.DataFrame): Input features.\n",
        "        top_features (list): List of selected top features.\n",
        "\n",
        "    Returns:\n",
        "        np.ndarray: Scaled and reshaped data.\n",
        "    \"\"\"\n",
        "    scaler = MinMaxScaler()\n",
        "    X_scaled = scaler.fit_transform(X[top_features])\n",
        "\n",
        "    # Reshape for Conv1D and LSTM: (samples, timesteps, features)\n",
        "    X_reshaped = X_scaled.reshape(X_scaled.shape[0], X_scaled.shape[1], 1)\n",
        "    return X_reshaped"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9jx5e9T7lCF_"
      },
      "source": [
        "The above function will be used before training and evaluating the models on each participant-independent fold."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBGBUROElCF_"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\n",
        "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "\n",
        "def train_and_evaluate_dl_models(folds, top_features_dict, model_type=\"Conv1D\"):\n",
        "    \"\"\"\n",
        "    Trains and evaluates Conv1D or LSTM models on participant-independent folds without hyperparameter tuning.\n",
        "    Dataset loading is handled via TensorFlow tensor slices.\n",
        "\n",
        "    Parameters:\n",
        "        folds (list of tuples): Train-test splits from GroupKFold.\n",
        "        top_features_dict (dict): Dictionary of top features.\n",
        "        model_type (str): \"Conv1D\" or \"LSTM\".\n",
        "\n",
        "    Returns:\n",
        "        dict: Results for each k value containing the model and its performance metrics.\n",
        "    \"\"\"\n",
        "    results = {}\n",
        "\n",
        "    for k, top_features in top_features_dict.items():\n",
        "        print(f\"Evaluating models for top {k} features...\")\n",
        "\n",
        "        best_model = None\n",
        "        best_accuracy = 0\n",
        "        best_balanced_accuracy = 0\n",
        "\n",
        "        # Iterate over folds\n",
        "        for fold, (X_train, X_test, y_train, y_test) in enumerate(folds):\n",
        "            print(f\"Training on Fold {fold + 1}...\")\n",
        "\n",
        "            # Filter dataset for top features\n",
        "            X_train = X_train[top_features]\n",
        "            X_test = X_test[top_features]\n",
        "\n",
        "            # Create TensorFlow datasets\n",
        "            train_ds = tf.data.Dataset.from_tensor_slices((X_train.values, y_train.values)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "            test_ds = tf.data.Dataset.from_tensor_slices((X_test.values, y_test.values)).batch(32).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "            # Prepare input shape\n",
        "            input_shape = (len(top_features), 1)  # (timesteps, features)\n",
        "\n",
        "            # Build model\n",
        "            if model_type == \"Conv1D\":\n",
        "                model = Sequential([\n",
        "                    Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n",
        "                    MaxPooling1D(pool_size=2),\n",
        "                    Dropout(0.2),\n",
        "                    Flatten(),\n",
        "                    Dense(32, activation='relu'),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "                ])\n",
        "            elif model_type == \"LSTM\":\n",
        "                model = Sequential([\n",
        "                    LSTM(64, return_sequences=True, input_shape=input_shape),\n",
        "                    Dropout(0.2),\n",
        "                    LSTM(32),\n",
        "                    Dense(1, activation='sigmoid')\n",
        "                ])\n",
        "            else:\n",
        "                raise ValueError(\"Invalid model_type. Choose 'Conv1D' or 'LSTM'.\")\n",
        "\n",
        "            # Compile model\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "            # Train model\n",
        "            model.fit(train_ds, epochs=20, verbose=0)\n",
        "            print(\"Evaluating on the test dataset....\")\n",
        "\n",
        "            # Evaluate on test set\n",
        "            y_pred = (model.predict(test_ds) > 0.5).astype(int)\n",
        "            acc = accuracy_score(y_test, y_pred)\n",
        "            bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
        "\n",
        "            print(f\"Fold {fold + 1}: Accuracy={acc:.4f}, Balanced Accuracy={bal_acc:.4f}\\n\")\n",
        "\n",
        "            # Update best model if this model performs better\n",
        "            if acc > best_accuracy and bal_acc > best_balanced_accuracy:\n",
        "                best_model = model\n",
        "                best_accuracy = acc\n",
        "                best_balanced_accuracy = bal_acc\n",
        "\n",
        "        # Store results for this k value\n",
        "        results[k] = {\n",
        "            \"Best Model\": best_model,\n",
        "            \"Best Accuracy\": best_accuracy,\n",
        "            \"Best Balanced Accuracy\": best_balanced_accuracy,\n",
        "        }\n",
        "        print(f\"Best Model for top {k} features: Accuracy={best_accuracy:.4f}, Balanced Accuracy={best_balanced_accuracy:.4f}\\n\")\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "3Iq2t6TslCF_",
        "outputId": "eaf8b381-9dad-4b75-cd8b-419743707d28"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating models for top 100 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "Fold 1: Accuracy=0.8462, Balanced Accuracy=0.7045\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 4: Accuracy=0.8462, Balanced Accuracy=0.4783\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 5: Accuracy=0.6800, Balanced Accuracy=0.5595\n",
            "\n",
            "Best Model for top 100 features: Accuracy=0.8462, Balanced Accuracy=0.7045\n",
            "\n",
            "Evaluating models for top 200 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 1: Accuracy=0.8077, Balanced Accuracy=0.4773\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 2: Accuracy=0.9615, Balanced Accuracy=0.8333\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 4: Accuracy=0.8462, Balanced Accuracy=0.4783\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 5: Accuracy=0.6800, Balanced Accuracy=0.5159\n",
            "\n",
            "Best Model for top 200 features: Accuracy=0.9615, Balanced Accuracy=0.8333\n",
            "\n",
            "Evaluating models for top 400 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 1: Accuracy=0.8077, Balanced Accuracy=0.6818\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 4: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 5: Accuracy=0.7200, Balanced Accuracy=0.5873\n",
            "\n",
            "Best Model for top 400 features: Accuracy=0.8077, Balanced Accuracy=0.6818\n",
            "\n",
            "Evaluating models for top 800 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Fold 1: Accuracy=0.8462, Balanced Accuracy=0.6023\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 15ms/step\n",
            "Fold 4: Accuracy=0.8462, Balanced Accuracy=0.4783\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step\n",
            "Fold 5: Accuracy=0.6400, Balanced Accuracy=0.4444\n",
            "\n",
            "Best Model for top 800 features: Accuracy=0.8462, Balanced Accuracy=0.6023\n",
            "\n",
            "Top 100 features:\n",
            "Best Model: <Sequential name=sequential_40, built=True>\n",
            "Best Accuracy: 0.8462\n",
            "Best Balanced Accuracy: 0.7045\n",
            "\n",
            "Top 200 features:\n",
            "Best Model: <Sequential name=sequential_46, built=True>\n",
            "Best Accuracy: 0.9615\n",
            "Best Balanced Accuracy: 0.8333\n",
            "\n",
            "Top 400 features:\n",
            "Best Model: <Sequential name=sequential_50, built=True>\n",
            "Best Accuracy: 0.8077\n",
            "Best Balanced Accuracy: 0.6818\n",
            "\n",
            "Top 800 features:\n",
            "Best Model: <Sequential name=sequential_55, built=True>\n",
            "Best Accuracy: 0.8462\n",
            "Best Balanced Accuracy: 0.6023\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function for Conv1D\n",
        "conv1d_results = train_and_evaluate_dl_models(\n",
        "    folds=folds,\n",
        "    top_features_dict=filtered_features,\n",
        "    model_type=\"Conv1D\"\n",
        ")\n",
        "\n",
        "# Display results\n",
        "for k, result in conv1d_results.items():\n",
        "    print(f\"Top {k} features:\")\n",
        "    print(f\"Best Model: {result['Best Model']}\")\n",
        "    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n",
        "    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "InVVBr74lCF_",
        "outputId": "793347e8-1462-459c-be8f-b5b76d5e0f9f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating models for top 100 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "Fold 1: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 70ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "Fold 4: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 69ms/step\n",
            "Fold 5: Accuracy=0.7200, Balanced Accuracy=0.5000\n",
            "\n",
            "Best Model for top 100 features: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Evaluating models for top 200 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "Fold 1: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 73ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 75ms/step\n",
            "Fold 4: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 72ms/step\n",
            "Fold 5: Accuracy=0.7200, Balanced Accuracy=0.5000\n",
            "\n",
            "Best Model for top 200 features: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Evaluating models for top 400 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
            "Fold 1: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 84ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 83ms/step\n",
            "Fold 4: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 88ms/step\n",
            "Fold 5: Accuracy=0.7200, Balanced Accuracy=0.5000\n",
            "\n",
            "Best Model for top 400 features: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Evaluating models for top 800 features...\n",
            "Training on Fold 1...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Fold 1: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 2...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 86ms/step\n",
            "Fold 2: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 3...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 82ms/step\n",
            "Fold 3: Accuracy=0.7692, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 4...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 93ms/step\n",
            "Fold 4: Accuracy=0.8846, Balanced Accuracy=0.5000\n",
            "\n",
            "Training on Fold 5...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/keras/src/layers/rnn/rnn.py:200: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating on the test dataset....\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "Fold 5: Accuracy=0.7200, Balanced Accuracy=0.5000\n",
            "\n",
            "Best Model for top 800 features: Accuracy=0.8462, Balanced Accuracy=0.5000\n",
            "\n",
            "Top 100 features:\n",
            "Best LSTM Model: <Sequential name=sequential_60, built=True>\n",
            "Best Accuracy: 0.8462\n",
            "Best Balanced Accuracy: 0.5000\n",
            "\n",
            "Top 200 features:\n",
            "Best LSTM Model: <Sequential name=sequential_65, built=True>\n",
            "Best Accuracy: 0.8462\n",
            "Best Balanced Accuracy: 0.5000\n",
            "\n",
            "Top 400 features:\n",
            "Best LSTM Model: <Sequential name=sequential_70, built=True>\n",
            "Best Accuracy: 0.8462\n",
            "Best Balanced Accuracy: 0.5000\n",
            "\n",
            "Top 800 features:\n",
            "Best LSTM Model: <Sequential name=sequential_75, built=True>\n",
            "Best Accuracy: 0.8462\n",
            "Best Balanced Accuracy: 0.5000\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Call the function for LSTM\n",
        "LSTM_results = train_and_evaluate_dl_models(\n",
        "    folds=folds,\n",
        "    top_features_dict=filtered_features,\n",
        "    model_type=\"LSTM\"\n",
        ")\n",
        "\n",
        "# Display results\n",
        "for k, result in LSTM_results.items():\n",
        "    print(f\"Top {k} features:\")\n",
        "    print(f\"Best LSTM Model: {result['Best Model']}\")\n",
        "    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n",
        "    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhmkYI7SlCF_",
        "outputId": "155e1ddb-e80c-4421-c199-fc8ac0a25061"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model for top 100 features at: saved_models_conv1D/partc_best_dl_conv1d_model_top_100_features.pkl\n",
            "Saved best model for top 200 features at: saved_models_conv1D/partc_best_dl_conv1d_model_top_200_features.pkl\n",
            "Saved best model for top 400 features at: saved_models_conv1D/partc_best_dl_conv1d_model_top_400_features.pkl\n",
            "Saved best model for top 800 features at: saved_models_conv1D/partc_best_dl_conv1d_model_top_800_features.pkl\n"
          ]
        }
      ],
      "source": [
        "# Directory to save models\n",
        "save_dir_conv1d = \"saved_models_conv1D\"\n",
        "os.makedirs(save_dir_conv1d, exist_ok=True)\n",
        "\n",
        "# Iterate through the results dictionary\n",
        "for k, result in conv1d_results.items():\n",
        "    best_model = result[\"Best Model\"]\n",
        "    model_path = os.path.join(save_dir_conv1d, f\"partc_best_dl_conv1d_model_top_{k}_features.pkl\")\n",
        "    joblib.dump(best_model, model_path)  # Save the model\n",
        "    print(f\"Saved best model for top {k} features at: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2VTQhpFUlCF_",
        "outputId": "673f4d27-7d2f-45f7-c750-9000ea27a400"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved best model for top 100 features at: saved_models_LSTM/partc_best_dl_lstm_model_top_100_features.pkl\n",
            "Saved best model for top 200 features at: saved_models_LSTM/partc_best_dl_lstm_model_top_200_features.pkl\n",
            "Saved best model for top 400 features at: saved_models_LSTM/partc_best_dl_lstm_model_top_400_features.pkl\n",
            "Saved best model for top 800 features at: saved_models_LSTM/partc_best_dl_lstm_model_top_800_features.pkl\n"
          ]
        }
      ],
      "source": [
        "# Directory to save models\n",
        "save_dir_lstm = \"saved_models_LSTM\"\n",
        "os.makedirs(save_dir_lstm, exist_ok=True)\n",
        "\n",
        "# Iterate through the results dictionary\n",
        "for k, result in LSTM_results.items():\n",
        "    best_model = result[\"Best Model\"]\n",
        "    model_path = os.path.join(save_dir_lstm, f\"partc_best_dl_lstm_model_top_{k}_features.pkl\")\n",
        "    joblib.dump(best_model, model_path)  # Save the model\n",
        "    print(f\"Saved best model for top {k} features at: {model_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4xFfYx_lCGA",
        "outputId": "6f3cc37e-f752-41d4-f10a-7d985f5f0fb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Processing Fold 1\n",
            "\n",
            "Identifying speaker-dependent features...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Features removed due to high speaker dependency:\n",
            "Embedding_302: 0.3796\n",
            "Embedding_364: 0.3384\n",
            "Embedding_250: 0.3127\n",
            "Embedding_253: 0.3118\n",
            "Embedding_238: 0.3046\n",
            "Embedding_222: 0.2995\n",
            "Embedding_225: 0.2992\n",
            "Embedding_217: 0.2959\n",
            "Embedding_220: 0.2920\n",
            "MD: 0.2916\n",
            "\n",
            "Training models with 411 features...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "accuracy: 0.8462\n",
            "balanced_accuracy: 0.5000\n",
            "accuracy_pid_P003: 0.7778\n",
            "accuracy_pid_P005: 1.0000\n",
            "accuracy_pid_P008: 0.7500\n",
            "accuracy_pid_P009: 0.8000\n",
            "accuracy_pid_P029: 1.0000\n",
            "accuracy_pid_P036: 1.0000\n",
            "demographic_parity_diff: 0.0000\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "accuracy: 0.7308\n",
            "balanced_accuracy: 0.5341\n",
            "accuracy_pid_P003: 0.6667\n",
            "accuracy_pid_P005: 0.7500\n",
            "accuracy_pid_P008: 1.0000\n",
            "accuracy_pid_P009: 0.8000\n",
            "accuracy_pid_P029: 0.5000\n",
            "accuracy_pid_P036: 0.5000\n",
            "demographic_parity_diff: 0.5000\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "accuracy: 0.8462\n",
            "balanced_accuracy: 0.6023\n",
            "accuracy_pid_P003: 0.6667\n",
            "accuracy_pid_P005: 1.0000\n",
            "accuracy_pid_P008: 1.0000\n",
            "accuracy_pid_P009: 0.8000\n",
            "accuracy_pid_P029: 1.0000\n",
            "accuracy_pid_P036: 1.0000\n",
            "demographic_parity_diff: 0.2500\n",
            "\n",
            "Processing Fold 2\n",
            "\n",
            "Identifying speaker-dependent features...\n",
            "Features removed due to high speaker dependency:\n",
            "MD: 0.3225\n",
            "Embedding_255: 0.3191\n",
            "Embedding_198: 0.3188\n",
            "Embedding_364: 0.3120\n",
            "Embedding_222: 0.3106\n",
            "Embedding_190: 0.3070\n",
            "Embedding_212: 0.2767\n",
            "Embedding_378: 0.2700\n",
            "Embedding_126: 0.2641\n",
            "Embedding_294: 0.2537\n",
            "\n",
            "Training models with 411 features...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "accuracy: 0.8846\n",
            "balanced_accuracy: 0.5000\n",
            "accuracy_pid_P013: 1.0000\n",
            "accuracy_pid_P017: 0.8750\n",
            "accuracy_pid_P018: 0.7500\n",
            "accuracy_pid_P024: 1.0000\n",
            "accuracy_pid_P025: 1.0000\n",
            "accuracy_pid_P028: 0.8333\n",
            "demographic_parity_diff: 0.0000\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "accuracy: 0.7308\n",
            "balanced_accuracy: 0.5580\n",
            "accuracy_pid_P013: 0.6667\n",
            "accuracy_pid_P017: 0.7500\n",
            "accuracy_pid_P018: 0.5000\n",
            "accuracy_pid_P024: 1.0000\n",
            "accuracy_pid_P025: 0.5000\n",
            "accuracy_pid_P028: 0.8333\n",
            "demographic_parity_diff: 0.7500\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "accuracy: 0.8846\n",
            "balanced_accuracy: 0.6449\n",
            "accuracy_pid_P013: 1.0000\n",
            "accuracy_pid_P017: 0.7500\n",
            "accuracy_pid_P018: 1.0000\n",
            "accuracy_pid_P024: 1.0000\n",
            "accuracy_pid_P025: 1.0000\n",
            "accuracy_pid_P028: 0.8333\n",
            "demographic_parity_diff: 0.2500\n",
            "\n",
            "Processing Fold 3\n",
            "\n",
            "Identifying speaker-dependent features...\n",
            "Features removed due to high speaker dependency:\n",
            "MD: 0.6455\n",
            "Embedding_222: 0.4206\n",
            "Embedding_220: 0.3379\n",
            "Embedding_250: 0.3351\n",
            "Embedding_30: 0.3118\n",
            "Embedding_328: 0.3039\n",
            "PRP$: 0.3009\n",
            "Embedding_302: 0.2809\n",
            "Embedding_80: 0.2802\n",
            "Embedding_281: 0.2794\n",
            "\n",
            "Training models with 411 features...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "accuracy: 0.7692\n",
            "balanced_accuracy: 0.5000\n",
            "accuracy_pid_P001: 0.7500\n",
            "accuracy_pid_P004: 0.6000\n",
            "accuracy_pid_P027: 1.0000\n",
            "accuracy_pid_P030: 1.0000\n",
            "accuracy_pid_P032: 0.0000\n",
            "accuracy_pid_P038: 1.0000\n",
            "accuracy_pid_P041: 0.8000\n",
            "demographic_parity_diff: 0.0000\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "accuracy: 0.6538\n",
            "balanced_accuracy: 0.4250\n",
            "accuracy_pid_P001: 0.7500\n",
            "accuracy_pid_P004: 0.2000\n",
            "accuracy_pid_P027: 0.6667\n",
            "accuracy_pid_P030: 1.0000\n",
            "accuracy_pid_P032: 0.0000\n",
            "accuracy_pid_P038: 1.0000\n",
            "accuracy_pid_P041: 0.8000\n",
            "demographic_parity_diff: 0.4000\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "accuracy: 0.8077\n",
            "balanced_accuracy: 0.7000\n",
            "accuracy_pid_P001: 1.0000\n",
            "accuracy_pid_P004: 0.6000\n",
            "accuracy_pid_P027: 1.0000\n",
            "accuracy_pid_P030: 1.0000\n",
            "accuracy_pid_P032: 0.0000\n",
            "accuracy_pid_P038: 1.0000\n",
            "accuracy_pid_P041: 0.6000\n",
            "demographic_parity_diff: 0.4000\n",
            "\n",
            "Processing Fold 4\n",
            "\n",
            "Identifying speaker-dependent features...\n",
            "Features removed due to high speaker dependency:\n",
            "Embedding_253: 0.3781\n",
            "PRP$: 0.3430\n",
            "Embedding_302: 0.3349\n",
            "Embedding_240: 0.3152\n",
            "Embedding_340: 0.3007\n",
            "Embedding_84: 0.2846\n",
            "Embedding_365: 0.2712\n",
            "MD: 0.2707\n",
            "Embedding_190: 0.2521\n",
            "Embedding_212: 0.2468\n",
            "\n",
            "Training models with 411 features...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "accuracy: 0.8846\n",
            "balanced_accuracy: 0.5000\n",
            "accuracy_pid_P002: 0.8333\n",
            "accuracy_pid_P010: 1.0000\n",
            "accuracy_pid_P011: 1.0000\n",
            "accuracy_pid_P014: 1.0000\n",
            "accuracy_pid_P016: 1.0000\n",
            "accuracy_pid_P031: 1.0000\n",
            "accuracy_pid_P039: 0.5000\n",
            "demographic_parity_diff: 0.0000\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "accuracy: 0.6538\n",
            "balanced_accuracy: 0.3696\n",
            "accuracy_pid_P002: 0.6667\n",
            "accuracy_pid_P010: 1.0000\n",
            "accuracy_pid_P011: 1.0000\n",
            "accuracy_pid_P014: 0.8000\n",
            "accuracy_pid_P016: 0.6667\n",
            "accuracy_pid_P031: 0.3333\n",
            "accuracy_pid_P039: 0.5000\n",
            "demographic_parity_diff: 0.6667\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "accuracy: 0.8077\n",
            "balanced_accuracy: 0.4565\n",
            "accuracy_pid_P002: 0.8333\n",
            "accuracy_pid_P010: 1.0000\n",
            "accuracy_pid_P011: 1.0000\n",
            "accuracy_pid_P014: 0.8000\n",
            "accuracy_pid_P016: 1.0000\n",
            "accuracy_pid_P031: 0.6667\n",
            "accuracy_pid_P039: 0.5000\n",
            "demographic_parity_diff: 0.3333\n",
            "\n",
            "Processing Fold 5\n",
            "\n",
            "Identifying speaker-dependent features...\n",
            "Features removed due to high speaker dependency:\n",
            "PRP$: 0.3785\n",
            "Embedding_302: 0.3410\n",
            "Embedding_198: 0.3345\n",
            "Embedding_364: 0.3319\n",
            "Embedding_222: 0.3310\n",
            "JJS: 0.3278\n",
            "Embedding_84: 0.3239\n",
            "Embedding_238: 0.3124\n",
            "Embedding_75: 0.2939\n",
            "Embedding_253: 0.2895\n",
            "\n",
            "Training models with 411 features...\n",
            "\n",
            "Training Random Forest...\n",
            "Random Forest Results:\n",
            "accuracy: 0.7200\n",
            "balanced_accuracy: 0.5000\n",
            "accuracy_pid_P006: 0.3333\n",
            "accuracy_pid_P007: 0.5000\n",
            "accuracy_pid_P012: 1.0000\n",
            "accuracy_pid_P019: 1.0000\n",
            "accuracy_pid_P023: 0.8333\n",
            "accuracy_pid_P033: 0.7500\n",
            "demographic_parity_diff: 0.0000\n",
            "\n",
            "Training Decision Tree...\n",
            "Decision Tree Results:\n",
            "accuracy: 0.5600\n",
            "balanced_accuracy: 0.4325\n",
            "accuracy_pid_P006: 0.6667\n",
            "accuracy_pid_P007: 0.5000\n",
            "accuracy_pid_P012: 1.0000\n",
            "accuracy_pid_P019: 0.6000\n",
            "accuracy_pid_P023: 0.5000\n",
            "accuracy_pid_P033: 0.5000\n",
            "demographic_parity_diff: 0.4000\n",
            "\n",
            "Training Logistic Regression...\n",
            "Logistic Regression Results:\n",
            "accuracy: 0.6800\n",
            "balanced_accuracy: 0.4722\n",
            "accuracy_pid_P006: 0.3333\n",
            "accuracy_pid_P007: 0.5000\n",
            "accuracy_pid_P012: 1.0000\n",
            "accuracy_pid_P019: 0.8000\n",
            "accuracy_pid_P023: 0.8333\n",
            "accuracy_pid_P033: 0.7500\n",
            "demographic_parity_diff: 0.2000\n",
            "\n",
            "Average Results by Model:\n",
            "                     accuracy  balanced_accuracy  accuracy_pid_P003  \\\n",
            "model                                                                 \n",
            "Decision Tree        0.665846           0.463833           0.666667   \n",
            "Logistic Regression  0.805231           0.575189           0.666667   \n",
            "Random Forest        0.820923           0.500000           0.777778   \n",
            "\n",
            "                     accuracy_pid_P005  accuracy_pid_P008  accuracy_pid_P009  \\\n",
            "model                                                                          \n",
            "Decision Tree                     0.75               1.00                0.8   \n",
            "Logistic Regression               1.00               1.00                0.8   \n",
            "Random Forest                     1.00               0.75                0.8   \n",
            "\n",
            "                     accuracy_pid_P029  accuracy_pid_P036  \\\n",
            "model                                                       \n",
            "Decision Tree                      0.5                0.5   \n",
            "Logistic Regression                1.0                1.0   \n",
            "Random Forest                      1.0                1.0   \n",
            "\n",
            "                     demographic_parity_diff  accuracy_pid_P013  ...  \\\n",
            "model                                                            ...   \n",
            "Decision Tree                       0.543333           0.666667  ...   \n",
            "Logistic Regression                 0.286667           1.000000  ...   \n",
            "Random Forest                       0.000000           1.000000  ...   \n",
            "\n",
            "                     accuracy_pid_P014  accuracy_pid_P016  accuracy_pid_P031  \\\n",
            "model                                                                          \n",
            "Decision Tree                      0.8           0.666667           0.333333   \n",
            "Logistic Regression                0.8           1.000000           0.666667   \n",
            "Random Forest                      1.0           1.000000           1.000000   \n",
            "\n",
            "                     accuracy_pid_P039  accuracy_pid_P006  accuracy_pid_P007  \\\n",
            "model                                                                          \n",
            "Decision Tree                      0.5           0.666667                0.5   \n",
            "Logistic Regression                0.5           0.333333                0.5   \n",
            "Random Forest                      0.5           0.333333                0.5   \n",
            "\n",
            "                     accuracy_pid_P012  accuracy_pid_P019  accuracy_pid_P023  \\\n",
            "model                                                                          \n",
            "Decision Tree                      1.0                0.6           0.500000   \n",
            "Logistic Regression                1.0                0.8           0.833333   \n",
            "Random Forest                      1.0                1.0           0.833333   \n",
            "\n",
            "                     accuracy_pid_P033  \n",
            "model                                   \n",
            "Decision Tree                     0.50  \n",
            "Logistic Regression               0.75  \n",
            "Random Forest                     0.75  \n",
            "\n",
            "[3 rows x 35 columns]\n",
            "\n",
            "Standard Deviation of Results by Model:\n",
            "                     accuracy  balanced_accuracy  accuracy_pid_P003  \\\n",
            "model                                                                 \n",
            "Decision Tree        0.070572           0.079328                NaN   \n",
            "Logistic Regression  0.076928           0.107073                NaN   \n",
            "Random Forest        0.073498           0.000000                NaN   \n",
            "\n",
            "                     accuracy_pid_P005  accuracy_pid_P008  accuracy_pid_P009  \\\n",
            "model                                                                          \n",
            "Decision Tree                      NaN                NaN                NaN   \n",
            "Logistic Regression                NaN                NaN                NaN   \n",
            "Random Forest                      NaN                NaN                NaN   \n",
            "\n",
            "                     accuracy_pid_P029  accuracy_pid_P036  \\\n",
            "model                                                       \n",
            "Decision Tree                      NaN                NaN   \n",
            "Logistic Regression                NaN                NaN   \n",
            "Random Forest                      NaN                NaN   \n",
            "\n",
            "                     demographic_parity_diff  accuracy_pid_P013  ...  \\\n",
            "model                                                            ...   \n",
            "Decision Tree                       0.158815                NaN  ...   \n",
            "Logistic Regression                 0.079408                NaN  ...   \n",
            "Random Forest                       0.000000                NaN  ...   \n",
            "\n",
            "                     accuracy_pid_P014  accuracy_pid_P016  accuracy_pid_P031  \\\n",
            "model                                                                          \n",
            "Decision Tree                      NaN                NaN                NaN   \n",
            "Logistic Regression                NaN                NaN                NaN   \n",
            "Random Forest                      NaN                NaN                NaN   \n",
            "\n",
            "                     accuracy_pid_P039  accuracy_pid_P006  accuracy_pid_P007  \\\n",
            "model                                                                          \n",
            "Decision Tree                      NaN                NaN                NaN   \n",
            "Logistic Regression                NaN                NaN                NaN   \n",
            "Random Forest                      NaN                NaN                NaN   \n",
            "\n",
            "                     accuracy_pid_P012  accuracy_pid_P019  accuracy_pid_P023  \\\n",
            "model                                                                          \n",
            "Decision Tree                      NaN                NaN                NaN   \n",
            "Logistic Regression                NaN                NaN                NaN   \n",
            "Random Forest                      NaN                NaN                NaN   \n",
            "\n",
            "                     accuracy_pid_P033  \n",
            "model                                   \n",
            "Decision Tree                      NaN  \n",
            "Logistic Regression                NaN  \n",
            "Random Forest                      NaN  \n",
            "\n",
            "[3 rows x 35 columns]\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.feature_selection import mutual_info_classif\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "\n",
        "class SpeakerDebiasingPipeline:\n",
        "    \"\"\"\n",
        "    Pipeline for reducing speaker identity dependencies in features using\n",
        "    feature selection and instance reweighting.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, n_features_to_remove=10):\n",
        "        self.n_features_to_remove = n_features_to_remove\n",
        "        self.removed_features = None\n",
        "        self.scaler = StandardScaler()\n",
        "\n",
        "    def identify_speaker_dependent_features(self, X, pids, features):\n",
        "        \"\"\"\n",
        "        Identify features most dependent on speaker identity using mutual information.\n",
        "\n",
        "        Args:\n",
        "            X: Feature matrix\n",
        "            pids: Array of speaker IDs\n",
        "            features: List of feature names\n",
        "        \"\"\"\n",
        "        # Calculate mutual information between features and speaker ID\n",
        "        mi_scores = mutual_info_classif(\n",
        "            X[features],\n",
        "            pids,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # Create feature importance DataFrame\n",
        "        feature_importance = pd.DataFrame({\n",
        "            'feature': features,\n",
        "            'mi_score': mi_scores\n",
        "        })\n",
        "\n",
        "        # Sort features by mutual information score\n",
        "        feature_importance = feature_importance.sort_values(\n",
        "            'mi_score',\n",
        "            ascending=False\n",
        "        )\n",
        "\n",
        "        # Select top N most speaker-dependent features\n",
        "        self.removed_features = feature_importance['feature'].head(\n",
        "            self.n_features_to_remove\n",
        "        ).tolist()\n",
        "\n",
        "        print(\"Features removed due to high speaker dependency:\")\n",
        "        for idx, row in feature_importance.head(self.n_features_to_remove).iterrows():\n",
        "            print(f\"{row['feature']}: {row['mi_score']:.4f}\")\n",
        "\n",
        "        return self.removed_features\n",
        "\n",
        "    def remove_speaker_dependent_features(self, X, features):\n",
        "        \"\"\"Remove identified speaker-dependent features from dataset.\"\"\"\n",
        "        if self.removed_features is None:\n",
        "            raise ValueError(\"Must run identify_speaker_dependent_features first\")\n",
        "\n",
        "        remaining_features = [f for f in features if f not in self.removed_features]\n",
        "        return remaining_features, X[remaining_features]\n",
        "\n",
        "    def compute_balanced_weights(self, y, pids):\n",
        "        \"\"\"Compute sample weights to balance both class and protected attribute.\"\"\"\n",
        "        combined_labels = [f\"{y_i}_{p_i}\" for y_i, p_i in zip(y, pids)]\n",
        "        weights = compute_sample_weight('balanced', combined_labels)\n",
        "        return weights\n",
        "\n",
        "    def evaluate_model(self, y_true, y_pred, pids):\n",
        "        \"\"\"Evaluate model performance with fairness metrics.\"\"\"\n",
        "        from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
        "\n",
        "        metrics = {\n",
        "            'accuracy': accuracy_score(y_true, y_pred),\n",
        "            'balanced_accuracy': balanced_accuracy_score(y_true, y_pred)\n",
        "        }\n",
        "\n",
        "        # Calculate per-speaker accuracy\n",
        "        for pid in np.unique(pids):\n",
        "            mask = pids == pid\n",
        "            metrics[f'accuracy_pid_{pid}'] = accuracy_score(y_true[mask], y_pred[mask])\n",
        "\n",
        "        # Calculate demographic parity\n",
        "        pred_rates = {}\n",
        "        for pid in np.unique(pids):\n",
        "            mask = pids == pid\n",
        "            pred_rates[pid] = np.mean(y_pred[mask])\n",
        "\n",
        "        metrics['demographic_parity_diff'] = max(pred_rates.values()) - min(pred_rates.values())\n",
        "\n",
        "        return metrics\n",
        "\n",
        "def run_debiased_pipeline(df_new, feature_columns, target_column, folds, models_dict):\n",
        "    \"\"\"\n",
        "    Run complete debiasing pipeline with cross-validation.\n",
        "    \"\"\"\n",
        "    pipeline = SpeakerDebiasingPipeline(n_features_to_remove=10)\n",
        "    results = []\n",
        "\n",
        "    for fold_idx, (X_train, X_test, y_train, y_test) in enumerate(folds):\n",
        "        print(f\"\\nProcessing Fold {fold_idx + 1}\")\n",
        "\n",
        "        # Get PIDs for train and test sets\n",
        "        # Assuming PIDs are in df_new with the same index as X_train/X_test\n",
        "        pid_train = df_new.loc[X_train.index]['PID'].values\n",
        "        pid_test = df_new.loc[X_test.index]['PID'].values\n",
        "\n",
        "        # Remove speaker-dependent features\n",
        "        print(\"\\nIdentifying speaker-dependent features...\")\n",
        "        speaker_features = pipeline.identify_speaker_dependent_features(\n",
        "            X_train, pid_train, feature_columns\n",
        "        )\n",
        "        remaining_features, X_train_filtered = pipeline.remove_speaker_dependent_features(\n",
        "            X_train, feature_columns\n",
        "        )\n",
        "        _, X_test_filtered = pipeline.remove_speaker_dependent_features(\n",
        "            X_test, feature_columns\n",
        "        )\n",
        "\n",
        "        # Scale features\n",
        "        X_train_scaled = pipeline.scaler.fit_transform(X_train_filtered)\n",
        "        X_test_scaled = pipeline.scaler.transform(X_test_filtered)\n",
        "\n",
        "        # Compute balanced weights\n",
        "        sample_weights = pipeline.compute_balanced_weights(y_train, pid_train)\n",
        "\n",
        "        print(f\"\\nTraining models with {len(remaining_features)} features...\")\n",
        "\n",
        "        # Train and evaluate each model\n",
        "        for model_name, model in models_dict.items():\n",
        "            print(f\"\\nTraining {model_name}...\")\n",
        "\n",
        "            # Fit model with sample weights\n",
        "            model.fit(X_train_scaled, y_train, sample_weight=sample_weights)\n",
        "\n",
        "            # Get predictions\n",
        "            y_pred = model.predict(X_test_scaled)\n",
        "\n",
        "            # Evaluate results\n",
        "            fold_metrics = pipeline.evaluate_model(y_test, y_pred, pid_test)\n",
        "            fold_metrics.update({\n",
        "                'fold': fold_idx,\n",
        "                'model': model_name\n",
        "            })\n",
        "            results.append(fold_metrics)\n",
        "\n",
        "            print(f\"{model_name} Results:\")\n",
        "            for metric, value in fold_metrics.items():\n",
        "                if metric not in ['fold', 'model']:\n",
        "                    print(f\"{metric}: {value:.4f}\")\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "# Example usage:\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Define models to evaluate\n",
        "models = {\n",
        "    'Random Forest': RandomForestClassifier(random_state=42),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
        "    'Logistic Regression': LogisticRegression(random_state=42)\n",
        "}\n",
        "\n",
        "# Run pipeline\n",
        "results = run_debiased_pipeline(\n",
        "    df_new=df_new,\n",
        "    feature_columns=feature_columns,\n",
        "    target_column='DOE_Label',\n",
        "    folds=folds,\n",
        "    models_dict=models\n",
        ")\n",
        "\n",
        "# Print summary results\n",
        "print(\"\\nAverage Results by Model:\")\n",
        "summary = results.groupby('model').mean().drop('fold', axis=1)\n",
        "print(summary)\n",
        "\n",
        "print(\"\\nStandard Deviation of Results by Model:\")\n",
        "std_dev = results.groupby('model').std().drop('fold', axis=1)\n",
        "print(std_dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vVm1Z6jVlCGA",
        "outputId": "7e34c9c2-2aa3-4c2d-a602-74d7fd3dcaf0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at sentence-transformers/all-MiniLM-L6-v2 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Device set to use mps:0\n",
            "Failed to determine 'entailment' label id from the label2id mapping in the model config. Setting to -1. Define a descriptive label2id mapping in the model config to ensure correct outputs.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label: Succinct\n",
            "Reasoning: The transcript was classified as 'Succinct' because it aligns with the characteristics of Succinct examples, such as succinctness and clarity.\n",
            "Evaluation Metrics: {'accuracy': 0.5, 'precision': 0.25, 'recall': 0.5, 'f1_score': 0.3333333333333333}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
        "\n",
        "# Load a pre-trained transformer model and tokenizer\n",
        "\n",
        "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "\n",
        "# Define a pipeline for zero-shot classification or text classification\n",
        "classifier = pipeline(\"zero-shot-classification\", model=model_name)\n",
        "\n",
        "# Example labeled transcripts for few-shot learning\n",
        "few_shot_examples = [\n",
        "    {\"text\": \"The explanation skipped important details about the process.\", \"label\": \"Under-explained\"},\n",
        "    {\"text\": \"The answer was clear and concise.\", \"label\": \"Succinct\"},\n",
        "    {\"text\": \"There were some good points, but the explanation could have been clearer.\", \"label\": \"Under-explained\"},\n",
        "    {\"text\": \"The response was to the point and adequately covered the question.\", \"label\": \"Succinct\"}\n",
        "]\n",
        "\n",
        "# Create a prompt with few-shot examples\n",
        "def create_few_shot_prompt(transcript):\n",
        "    examples_text = \"\\n\".join([\n",
        "        f\"Example: \\\"{example['text']}\\\" -> Label: {example['label']}\"\n",
        "        for example in few_shot_examples\n",
        "    ])\n",
        "    prompt = (\n",
        "        f\"The task is to classify the transcript as 'Under-explained' or 'Succinct' based on its explanation quality.\\n\"\n",
        "        f\"Here are some examples:\\n{examples_text}\\n\\n\"\n",
        "        f\"Transcript: \\\"{transcript}\\\"\\n\"\n",
        "        f\"Label:\"\n",
        "    )\n",
        "    return prompt\n",
        "\n",
        "# Function to classify transcript and generate reasoning\n",
        "def classify_and_reason(transcript):\n",
        "    # Use the classifier to predict the label\n",
        "    prompt = create_few_shot_prompt(transcript)\n",
        "    result = classifier(\n",
        "        sequences=transcript,\n",
        "        candidate_labels=[\"Under-explained\", \"Succinct\"],\n",
        "        hypothesis_template=\"This text is {}.\"\n",
        "    )\n",
        "\n",
        "    # Extract the most likely label\n",
        "    label = result[\"labels\"][0]\n",
        "\n",
        "    # Generate reasoning\n",
        "    reasoning = f\"The transcript was classified as '{label}' because it aligns with the characteristics of {label} examples, such as {'succinctness and clarity' if label == 'Succinct' else 'lack of detail and clarity'}.\"\n",
        "\n",
        "    return label, reasoning\n",
        "\n",
        "# Example usage\n",
        "transcript = \"The steps for solving the problem were not clearly outlined, making it hard to follow.\"\n",
        "label, reasoning = classify_and_reason(transcript)\n",
        "\n",
        "print(\"Label:\", label)\n",
        "print(\"Reasoning:\", reasoning)\n",
        "\n",
        "# Add evaluation metrics (accuracy, precision, recall)\n",
        "def evaluate_model(transcripts, true_labels):\n",
        "    from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "    predictions = [classify_and_reason(t)[0] for t in transcripts]\n",
        "    accuracy = accuracy_score(true_labels, predictions)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(true_labels, predictions, average='weighted')\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1\n",
        "    }\n",
        "\n",
        "# Example evaluation\n",
        "# Replace with your test dataset\n",
        "transcripts = [\n",
        "    \"The answer was to the point and addressed the query effectively.\",\n",
        "    \"Critical details were missing, making it hard to grasp the full explanation.\"\n",
        "]\n",
        "true_labels = [\"Succinct\", \"Under-explained\"]\n",
        "\n",
        "metrics = evaluate_model(transcripts, true_labels)\n",
        "print(\"Evaluation Metrics:\", metrics)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y0UrI02NlCGA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 6165202,
          "sourceId": 10013905,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 6165433,
          "sourceId": 10014253,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30786,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}