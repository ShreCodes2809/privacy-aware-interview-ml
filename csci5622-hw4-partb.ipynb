{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10091698,"sourceType":"datasetVersion","datasetId":6222948},{"sourceId":10091704,"sourceType":"datasetVersion","datasetId":6222953}],"dockerImageVersionId":30805,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install twython","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install sentence-transformers","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pip install vaderSentiment","metadata":{"trusted":true,"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Feature Extraction and Selection","metadata":{}},{"cell_type":"markdown","source":"## Data Preprocessing","metadata":{}},{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:30:21.488605Z","iopub.execute_input":"2024-12-03T20:30:21.488931Z","iopub.status.idle":"2024-12-03T20:30:21.843742Z","shell.execute_reply.started":"2024-12-03T20:30:21.488899Z","shell.execute_reply":"2024-12-03T20:30:21.842875Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nimport nltk\n\nnltk.download('vader_lexicon')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:30:21.845332Z","iopub.execute_input":"2024-12-03T20:30:21.845673Z","iopub.status.idle":"2024-12-03T20:30:23.234253Z","shell.execute_reply.started":"2024-12-03T20:30:21.845647Z","shell.execute_reply":"2024-12-03T20:30:23.233155Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set the local folder path\nfolder_path = r'/kaggle/input/datasets-transcripts/VetTrain_Transcripts'\n\n# Function to extract the numerical part from the filename\ndef extract_pid(filename):\n    base_name = os.path.splitext(filename)[0]\n    return base_name.split('_')[0]  # Assuming filename is like \"P001_transcript.csv\"\n\n# Get all CSV files in the folder and sort them numerically by filename\nall_files = sorted(\n    [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith('.csv')],\n    key=lambda x: int(extract_pid(os.path.basename(x))[1:])\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:30:23.236043Z","iopub.execute_input":"2024-12-03T20:30:23.236565Z","iopub.status.idle":"2024-12-03T20:30:23.242589Z","shell.execute_reply.started":"2024-12-03T20:30:23.236523Z","shell.execute_reply":"2024-12-03T20:30:23.241729Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef clean_text(text):\n    # lowercase\n    text = text.lower()\n    # Remove punctuation\n    text = re.sub(r'[^\\w\\s]', '', text)\n    # Delete redundant Spaces\n    text = re.sub(r'\\s+', ' ', text).strip()\n    return text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:30:23.243849Z","iopub.execute_input":"2024-12-03T20:30:23.244420Z","iopub.status.idle":"2024-12-03T20:30:23.257870Z","shell.execute_reply.started":"2024-12-03T20:30:23.244380Z","shell.execute_reply":"2024-12-03T20:30:23.256998Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialize the final storage for combined question pairs\ncombined_data = []\n\n# Process each file\nfor file_path in all_files:\n    # Extract PID from filename\n    pid = extract_pid(os.path.basename(file_path))\n\n    # Read the CSV file\n    df = pd.read_csv(file_path)\n    df = df[df['Type'] != 'IRR']  # Filter irrelevant rows\n\n    # Initialize dialogue extraction\n    current_dialogue = []\n    veteran_dialogue = []\n    current_question_id = None\n    qid_counter = 1  # Start QID counter for each file\n\n    # Extract question pairs with PID and QID\n    for _, row in df.iterrows():\n        if row['Type'].startswith('Q'):\n            question_id = row['Type']\n            if current_question_id is None:\n                current_question_id = question_id\n                current_dialogue = [row['Transcript']]\n                veteran_dialogue = []\n            elif question_id != current_question_id:\n                combined_data.append({\n                    'PID': pid,\n                    'QID': f\"Q{qid_counter}\",\n                    'Combined_Transcript': \" \".join(current_dialogue),\n                    'veteran_transcript':\" \".join(veteran_dialogue)\n                })\n                qid_counter += 1\n                current_dialogue = [row['Transcript']]\n                veteran_dialogue = []\n                current_question_id = question_id\n            else:\n                current_dialogue.append(row['Transcript'])\n        else:\n            current_dialogue.append(row['Transcript'])\n            if row['Type']=='BCV' or row['Type'].startswith('A'):\n              veteran_dialogue.append(row['Transcript'])\n\n\n    # Add the last dialogue for the file\n    if current_dialogue:\n        combined_data.append({\n            'PID': pid,\n            'QID': f\"Q{qid_counter}\",\n            'Combined_Transcript': \" \".join(current_dialogue),\n            'veteran_transcript':\" \".join(veteran_dialogue)\n        })","metadata":{"trusted":true,"scrolled":true,"execution":{"iopub.status.busy":"2024-12-03T20:30:23.259745Z","iopub.execute_input":"2024-12-03T20:30:23.260099Z","iopub.status.idle":"2024-12-03T20:30:23.767586Z","shell.execute_reply.started":"2024-12-03T20:30:23.260062Z","shell.execute_reply":"2024-12-03T20:30:23.766556Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Convert to a DataFrame\ntranscripts_df = pd.DataFrame(combined_data)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:31:03.464235Z","iopub.execute_input":"2024-12-03T20:31:03.464883Z","iopub.status.idle":"2024-12-03T20:31:03.469399Z","shell.execute_reply.started":"2024-12-03T20:31:03.464848Z","shell.execute_reply":"2024-12-03T20:31:03.468547Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load the behavioral annotation codes\nbehavior_file = r'/kaggle/input/ba-codes-dataset/Behavioral Annotation Codes.csv'\ndf_behavior = pd.read_csv(behavior_file)\n\n# Merge behavioral codes\ntranscripts_df = transcripts_df.merge(df_behavior, on=['PID', 'QID'], how='left')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:32:09.287855Z","iopub.execute_input":"2024-12-03T20:32:09.288689Z","iopub.status.idle":"2024-12-03T20:32:09.314018Z","shell.execute_reply.started":"2024-12-03T20:32:09.288650Z","shell.execute_reply":"2024-12-03T20:32:09.313382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df['Cleaned_Transcript'] = transcripts_df['Combined_Transcript'].apply(clean_text)\ntranscripts_df['Cleaned_veteran_transcript'] = transcripts_df['veteran_transcript'].apply(clean_text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:32:40.772351Z","iopub.execute_input":"2024-12-03T20:32:40.773091Z","iopub.status.idle":"2024-12-03T20:32:40.846241Z","shell.execute_reply.started":"2024-12-03T20:32:40.773052Z","shell.execute_reply":"2024-12-03T20:32:40.845625Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df = transcripts_df[['PID', 'Cleaned_veteran_transcript']]\ntranscripts_df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:37:11.874790Z","iopub.execute_input":"2024-12-03T20:37:11.875150Z","iopub.status.idle":"2024-12-03T20:37:11.885226Z","shell.execute_reply.started":"2024-12-03T20:37:11.875118Z","shell.execute_reply":"2024-12-03T20:37:11.884442Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Extracting features and adding it to the main dataset","metadata":{}},{"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom nltk import pos_tag, word_tokenize\nimport nltk\n\n# Download required NLTK data\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:37:31.158754Z","iopub.execute_input":"2024-12-03T20:37:31.159111Z","iopub.status.idle":"2024-12-03T20:37:31.246324Z","shell.execute_reply.started":"2024-12-03T20:37:31.159080Z","shell.execute_reply":"2024-12-03T20:37:31.245466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_tfidf_features(df, text_column, max_features=500):\n    \"\"\"\n    Adds TF-IDF features to the dataset.\n\n    Parameters:\n        df (pd.DataFrame): Input dataframe containing the text data.\n        text_column (str): Name of the column containing text data.\n        max_features (int): Maximum number of TF-IDF features to generate (default=500).\n    \n    Returns:\n        pd.DataFrame: Dataframe with TF-IDF features added.\n    \"\"\"\n    # Initialize TF-IDF Vectorizer\n    tfidf = TfidfVectorizer(max_features=max_features)\n    \n    # Fit and transform the text data\n    tfidf_matrix = tfidf.fit_transform(df[text_column])\n    \n    # Convert the TF-IDF matrix to a DataFrame\n    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf.get_feature_names_out(), index=df.index)\n    \n    # Concatenate the TF-IDF features with the original DataFrame\n    df_with_tfidf = pd.concat([df, tfidf_df], axis=1)\n    \n    return df_with_tfidf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:37:38.558721Z","iopub.execute_input":"2024-12-03T20:37:38.559390Z","iopub.status.idle":"2024-12-03T20:37:38.564731Z","shell.execute_reply.started":"2024-12-03T20:37:38.559352Z","shell.execute_reply":"2024-12-03T20:37:38.563772Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Example usage\ntranscripts_df = add_tfidf_features(transcripts_df, text_column=\"Cleaned_veteran_transcript\", max_features=2000)\ntranscripts_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:02.750959Z","iopub.execute_input":"2024-12-03T20:38:02.751323Z","iopub.status.idle":"2024-12-03T20:38:02.841398Z","shell.execute_reply.started":"2024-12-03T20:38:02.751296Z","shell.execute_reply":"2024-12-03T20:38:02.840460Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def add_pos_tags(df, text_column):\n    \"\"\"\n    Adds POS tagging to the dataframe.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe containing text data.\n        text_column (str): Name of the column containing text data.\n    \n    Returns:\n        pd.DataFrame: Dataframe with added POS tags.\n    \"\"\"\n    def pos_tags(text):\n        tokens = word_tokenize(text)\n        tags = pos_tag(tokens)\n        return {tag: len([word for word, pos in tags if pos == tag]) for tag in set([pos for _, pos in tags])}\n    \n    df['POS_Tags'] = df[text_column].apply(pos_tags)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:10.817435Z","iopub.execute_input":"2024-12-03T20:38:10.817794Z","iopub.status.idle":"2024-12-03T20:38:10.823591Z","shell.execute_reply.started":"2024-12-03T20:38:10.817762Z","shell.execute_reply":"2024-12-03T20:38:10.822603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df = add_pos_tags(transcripts_df, text_column=\"Cleaned_veteran_transcript\")\ntranscripts_df.head(10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:23.597723Z","iopub.execute_input":"2024-12-03T20:38:23.598104Z","iopub.status.idle":"2024-12-03T20:38:26.258909Z","shell.execute_reply.started":"2024-12-03T20:38:23.598070Z","shell.execute_reply":"2024-12-03T20:38:26.258038Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n\ndef add_sentiment_scores(df, text_column):\n    \"\"\"\n    Adds sentiment scores as separate columns to the dataframe.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe containing text data.\n        text_column (str): Name of the column containing text data.\n    \n    Returns:\n        pd.DataFrame: Dataframe with added sentiment scores as separate columns.\n    \"\"\"\n    sia = SentimentIntensityAnalyzer()\n    sentiment_scores = df[text_column].apply(lambda text: sia.polarity_scores(text))\n    \n    # Create separate columns for each sentiment score\n    df['Sentiment_Neg'] = sentiment_scores.apply(lambda score: score['neg'])\n    df['Sentiment_Neu'] = sentiment_scores.apply(lambda score: score['neu'])\n    df['Sentiment_Pos'] = sentiment_scores.apply(lambda score: score['pos'])\n    df['Sentiment_Compound'] = sentiment_scores.apply(lambda score: score['compound'])\n    \n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:30.689258Z","iopub.execute_input":"2024-12-03T20:38:30.689699Z","iopub.status.idle":"2024-12-03T20:38:30.701588Z","shell.execute_reply.started":"2024-12-03T20:38:30.689656Z","shell.execute_reply":"2024-12-03T20:38:30.700670Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Adding sentiment scores as separate columns to the dataframe\ntranscripts_df = add_sentiment_scores(transcripts_df, text_column=\"Cleaned_veteran_transcript\")\n\n# Example output for the first row\nprint(transcripts_df[['Sentiment_Neg', 'Sentiment_Neu', 'Sentiment_Pos', 'Sentiment_Compound']].iloc[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:37.177969Z","iopub.execute_input":"2024-12-03T20:38:37.178440Z","iopub.status.idle":"2024-12-03T20:38:37.890025Z","shell.execute_reply.started":"2024-12-03T20:38:37.178406Z","shell.execute_reply":"2024-12-03T20:38:37.889147Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sentence_transformers import SentenceTransformer\nimport numpy as np\n\ndef add_word_embeddings(df, text_column, model_name='all-MiniLM-L6-v2'):\n    \"\"\"\n    Adds word embeddings to the dataframe using Sentence Transformers.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe containing text data.\n        text_column (str): Name of the column containing text data.\n        model_name (str): Sentence Transformer model name.\n    \n    Returns:\n        pd.DataFrame: Dataframe with added embeddings.\n    \"\"\"\n    model = SentenceTransformer(model_name)\n    embeddings = model.encode(df[text_column].tolist(), show_progress_bar=True)\n    df['Embeddings'] = list(embeddings)\n    return df","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:41.236455Z","iopub.execute_input":"2024-12-03T20:38:41.236792Z","iopub.status.idle":"2024-12-03T20:38:58.459139Z","shell.execute_reply.started":"2024-12-03T20:38:41.236763Z","shell.execute_reply":"2024-12-03T20:38:58.458435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df = add_word_embeddings(transcripts_df, text_column=\"Cleaned_veteran_transcript\")\ntranscripts_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:38:58.460458Z","iopub.execute_input":"2024-12-03T20:38:58.461015Z","iopub.status.idle":"2024-12-03T20:39:03.346034Z","shell.execute_reply.started":"2024-12-03T20:38:58.460970Z","shell.execute_reply":"2024-12-03T20:39:03.345157Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = LabelEncoder()\ntarget_encoded = label_encoder.fit_transform(transcripts_df['PID'])\ntranscripts_df['PID'] = target_encoded","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:02:49.545212Z","iopub.execute_input":"2024-12-03T21:02:49.545828Z","iopub.status.idle":"2024-12-03T21:02:49.551189Z","shell.execute_reply.started":"2024-12-03T21:02:49.545791Z","shell.execute_reply":"2024-12-03T21:02:49.550262Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:02:53.303429Z","iopub.execute_input":"2024-12-03T21:02:53.303752Z","iopub.status.idle":"2024-12-03T21:02:53.332919Z","shell.execute_reply.started":"2024-12-03T21:02:53.303724Z","shell.execute_reply":"2024-12-03T21:02:53.332079Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# create a dataframe that contains the values for every POS tag in every sample\npos_tags_df = pd.json_normalize(transcripts_df['POS_Tags'])\npos_tags_df.fillna(0, inplace=True)  # Replace NaN with 0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:44:36.286926Z","iopub.execute_input":"2024-12-03T20:44:36.287625Z","iopub.status.idle":"2024-12-03T20:44:36.297911Z","shell.execute_reply.started":"2024-12-03T20:44:36.287591Z","shell.execute_reply":"2024-12-03T20:44:36.296969Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"pos_tags_df.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:44:36.775563Z","iopub.execute_input":"2024-12-03T20:44:36.776489Z","iopub.status.idle":"2024-12-03T20:44:36.782168Z","shell.execute_reply.started":"2024-12-03T20:44:36.776454Z","shell.execute_reply":"2024-12-03T20:44:36.781124Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Reset indices before concatenation\ntranscripts_df = transcripts_df.reset_index(drop=True)\npos_tags_df = pos_tags_df.reset_index(drop=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:44:51.580043Z","iopub.execute_input":"2024-12-03T20:44:51.580371Z","iopub.status.idle":"2024-12-03T20:44:51.585496Z","shell.execute_reply.started":"2024-12-03T20:44:51.580345Z","shell.execute_reply":"2024-12-03T20:44:51.584513Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# concatenate the two dataframes\ntranscripts_df = pd.concat([transcripts_df, pos_tags_df], axis=1)\ntranscripts_df.drop(columns=['POS_Tags'], inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:03:12.838866Z","iopub.execute_input":"2024-12-03T21:03:12.839669Z","iopub.status.idle":"2024-12-03T21:03:12.851719Z","shell.execute_reply.started":"2024-12-03T21:03:12.839635Z","shell.execute_reply":"2024-12-03T21:03:12.850977Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In order to filter the extracted features, we also need to handle the `Embeddings` column in such a way that each value corresponds to a single feature column.","metadata":{}},{"cell_type":"code","source":"# Expand Embeddings list into individual columns\nembeddings_df = pd.DataFrame(transcripts_df['Embeddings'].to_list(), index=transcripts_df.index)\nembeddings_df.columns = [f'Embedding_{i}' for i in range(embeddings_df.shape[1])]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T20:45:21.585488Z","iopub.execute_input":"2024-12-03T20:45:21.585811Z","iopub.status.idle":"2024-12-03T20:45:21.634797Z","shell.execute_reply.started":"2024-12-03T20:45:21.585784Z","shell.execute_reply":"2024-12-03T20:45:21.633977Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df = pd.concat([transcripts_df, embeddings_df], axis=1)\ntranscripts_df.drop(columns=['Embeddings'], inplace=True)\ntranscripts_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:03:39.742310Z","iopub.execute_input":"2024-12-03T21:03:39.743138Z","iopub.status.idle":"2024-12-03T21:03:39.770978Z","shell.execute_reply.started":"2024-12-03T21:03:39.743099Z","shell.execute_reply":"2024-12-03T21:03:39.770200Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df.isna().sum().sum()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:03:48.141352Z","iopub.execute_input":"2024-12-03T21:03:48.141703Z","iopub.status.idle":"2024-12-03T21:03:48.150540Z","shell.execute_reply.started":"2024-12-03T21:03:48.141672Z","shell.execute_reply":"2024-12-03T21:03:48.149670Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## Feature Selection","metadata":{}},{"cell_type":"markdown","source":"### Removing correlated features","metadata":{}},{"cell_type":"code","source":"corr_matrix = transcripts_df.drop(['PID','Cleaned_veteran_transcript'],axis=1).corr().abs()\nupper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:04:33.749575Z","iopub.execute_input":"2024-12-03T21:04:33.750260Z","iopub.status.idle":"2024-12-03T21:04:37.663611Z","shell.execute_reply.started":"2024-12-03T21:04:33.750224Z","shell.execute_reply":"2024-12-03T21:04:37.662635Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"threshold = 0.8\nto_drop = [column for column in upper.columns if any(upper[column] > threshold)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:04:37.665116Z","iopub.execute_input":"2024-12-03T21:04:37.665464Z","iopub.status.idle":"2024-12-03T21:04:38.558465Z","shell.execute_reply.started":"2024-12-03T21:04:37.665429Z","shell.execute_reply":"2024-12-03T21:04:38.557651Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"to_drop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:04:39.187630Z","iopub.execute_input":"2024-12-03T21:04:39.188400Z","iopub.status.idle":"2024-12-03T21:04:39.195910Z","shell.execute_reply.started":"2024-12-03T21:04:39.188363Z","shell.execute_reply":"2024-12-03T21:04:39.195049Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df.drop(to_drop,axis=1,inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:04:48.503861Z","iopub.execute_input":"2024-12-03T21:04:48.504544Z","iopub.status.idle":"2024-12-03T21:04:48.510234Z","shell.execute_reply.started":"2024-12-03T21:04:48.504508Z","shell.execute_reply":"2024-12-03T21:04:48.509318Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcripts_df.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:04:57.991435Z","iopub.execute_input":"2024-12-03T21:04:57.992286Z","iopub.status.idle":"2024-12-03T21:04:58.022781Z","shell.execute_reply.started":"2024-12-03T21:04:57.992235Z","shell.execute_reply":"2024-12-03T21:04:58.021782Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Feature selection based on mutual information scores","metadata":{}},{"cell_type":"code","source":"exclude_columns = ['PID', 'Cleaned_veteran_transcript']\n\n# Get all feature column names except the specified ones\nfeature_columns = [col for col in transcripts_df.columns if col not in exclude_columns]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:10:58.653356Z","iopub.execute_input":"2024-12-03T21:10:58.653698Z","iopub.status.idle":"2024-12-03T21:10:58.658918Z","shell.execute_reply.started":"2024-12-03T21:10:58.653666Z","shell.execute_reply":"2024-12-03T21:10:58.657897Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(feature_columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:11:10.229542Z","iopub.execute_input":"2024-12-03T21:11:10.230395Z","iopub.status.idle":"2024-12-03T21:11:10.236820Z","shell.execute_reply.started":"2024-12-03T21:11:10.230346Z","shell.execute_reply":"2024-12-03T21:11:10.235743Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.feature_selection import mutual_info_classif\n\ndef select_features_with_mutual_info(df, feature_columns, target_column, k_values):\n    \"\"\"\n    Selects top features for multiple values of k based on mutual information.\n    \n    Parameters:\n        df (pd.DataFrame): Input dataframe.\n        feature_columns (list): List of feature column names.\n        target_column (str): Name of the target column.\n        k_values (list): List of different k values to experiment with.\n    \n    Returns:\n        dict: Dictionary where keys are k values and values are lists of top k features.\n    \"\"\"\n    X = df[feature_columns]\n    y = df[target_column]\n    \n    # Calculate mutual information scores once\n    mi_scores = mutual_info_classif(X, y, random_state=42)\n    feature_scores = pd.Series(mi_scores, index=feature_columns)\n    \n    # Sort features by mutual information scores in descending order\n    sorted_features = feature_scores.sort_values(ascending=False).index.tolist()\n    \n    # Generate top k features for all k values\n    results = {k: sorted_features[:k] for k in k_values}\n    \n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:11:22.900730Z","iopub.execute_input":"2024-12-03T21:11:22.901604Z","iopub.status.idle":"2024-12-03T21:11:22.967731Z","shell.execute_reply.started":"2024-12-03T21:11:22.901561Z","shell.execute_reply":"2024-12-03T21:11:22.967019Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"filtered_features = select_features_with_mutual_info(\n    transcripts_df, \n    feature_columns=feature_columns, \n    target_column='PID', \n    k_values=[100, 200, 400, 800]\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:13:02.856937Z","iopub.execute_input":"2024-12-03T21:13:02.857325Z","iopub.status.idle":"2024-12-03T21:14:05.068739Z","shell.execute_reply.started":"2024-12-03T21:13:02.857278Z","shell.execute_reply":"2024-12-03T21:14:05.067958Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print results for each k\nfor k, features in filtered_features.items():\n    print(f\"Top {k} features: {features} \\n\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Splitting the dataset in 5 folds","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\n\ndef random_stratified_split(df, feature_columns, target_column, n_splits=5):\n    \"\"\"\n    Splits the dataset into stratified random folds using StratifiedKFold.\n    \n    Parameters:\n        df (pd.DataFrame): The input dataset containing features and target columns.\n        feature_columns (list): List of column names to be used as features.\n        target_column (str): Name of the target column.\n        n_splits (int): Number of folds (default is 5).\n    \n    Returns:\n        list of tuples: Each tuple contains (X_train, X_test, y_train, y_test) for one fold.\n    \"\"\"\n    # Extract features and target\n    X = df[feature_columns]\n    y = df[target_column]\n    \n    # Initialize StratifiedKFold\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n    \n    # Store splits\n    splits = []\n    for train_idx, test_idx in skf.split(X, y):\n        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n        splits.append((X_train, X_test, y_train, y_test))\n    \n    return splits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:19:10.325401Z","iopub.execute_input":"2024-12-03T21:19:10.326162Z","iopub.status.idle":"2024-12-03T21:19:10.332289Z","shell.execute_reply.started":"2024-12-03T21:19:10.326126Z","shell.execute_reply":"2024-12-03T21:19:10.331171Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"target_column = 'PID'\n\n# Perform stratified random splitting\nsplits = random_stratified_split(transcripts_df, feature_columns, target_column, n_splits=5)\n\n# Print results\nfor i, (X_train, X_test, y_train, y_test) in enumerate(splits):\n    print(f\"Fold {i+1}\")\n    print(\"X_train: \", X_train.shape)\n    print(\"X_test: \", X_test.shape)\n    print(\"y_train: \", y_train.shape)\n    print(\"y_test: \", y_test.shape)\n    print()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:22:21.393284Z","iopub.execute_input":"2024-12-03T21:22:21.394075Z","iopub.status.idle":"2024-12-03T21:22:21.415127Z","shell.execute_reply.started":"2024-12-03T21:22:21.394038Z","shell.execute_reply":"2024-12-03T21:22:21.414274Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Running Tree-Based ML Models","metadata":{}},{"cell_type":"code","source":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.model_selection import StratifiedKFold, GridSearchCV\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\n\ndef evaluate_tree_models_on_splits(df, feature_columns, target_column, k_features, top_features_dict, n_splits=5):\n    \"\"\"\n    Evaluates three tree-based models (Decision Tree, Random Forest, Gradient Boosting)\n    with hyperparameter tuning for each k value using StratifiedKFold splits.\n\n    Parameters:\n        df (pd.DataFrame): The input dataset containing features and the target column.\n        feature_columns (list): List of feature column names.\n        target_column (str): Name of the target column.\n        k_features (list): List of k values representing the number of top features to use.\n        top_features_dict (dict): Dictionary mapping each k value to the list of top features.\n        n_splits (int): Number of StratifiedKFold splits (default is 5).\n\n    Returns:\n        dict: Results for each k value containing the best model and its performance metrics.\n    \"\"\"\n    # Define the models and their hyperparameter grids\n    models = {\n        \"Decision Tree\": {\n            \"model\": DecisionTreeClassifier(random_state=42),\n            \"param_grid\": {\"max_depth\": [3, 5, 10, None], \"min_samples_split\": [2, 5, 10]},\n        },\n        \"Random Forest\": {\n            \"model\": RandomForestClassifier(random_state=42),\n            \"param_grid\": {\n                \"n_estimators\": [50, 100, 200],\n                \"max_depth\": [3, 5, 10, None],\n                \"min_samples_split\": [2, 5, 10],\n            },\n        }\n    }\n\n    results = {}\n\n    # Initialize StratifiedKFold\n    skf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n\n    # Iterate over each k value\n    for k in k_features:\n        print(f\"Evaluating models for top {k} features...\")\n        top_features = top_features_dict[k]  # Get the top k features\n\n        best_model_name = None\n        best_model = None\n        best_accuracy = 0\n        best_balanced_accuracy = 0\n        best_params = None\n\n        # Iterate over models\n        for model_name, model_info in models.items():\n            print(f\"Training {model_name}...\")\n            total_accuracy = 0\n            total_balanced_accuracy = 0\n\n            # Iterate over StratifiedKFold splits\n            for train_idx, test_idx in skf.split(df[feature_columns], df[target_column]):\n                X_train, X_test = df.iloc[train_idx][top_features], df.iloc[test_idx][top_features]\n                y_train, y_test = df.iloc[train_idx][target_column], df.iloc[test_idx][target_column]\n\n                # Perform grid search\n                grid_search = GridSearchCV(\n                    model_info[\"model\"],\n                    model_info[\"param_grid\"],\n                    scoring=\"accuracy\",\n                    cv=3,\n                    n_jobs=-1,\n                )\n                grid_search.fit(X_train, y_train)\n\n                # Evaluate on test set\n                best_estimator = grid_search.best_estimator_\n                y_pred = best_estimator.predict(X_test)\n                accuracy = accuracy_score(y_test, y_pred)\n                balanced_accuracy = balanced_accuracy_score(y_test, y_pred)\n\n                # Accumulate scores\n                total_accuracy += accuracy\n                total_balanced_accuracy += balanced_accuracy\n\n            # Average scores over all splits\n            avg_accuracy = total_accuracy / n_splits\n            avg_balanced_accuracy = total_balanced_accuracy / n_splits\n\n            print(f\"{model_name}: Accuracy={avg_accuracy:.4f}, Balanced Accuracy={avg_balanced_accuracy:.4f} \\n\")\n\n            # Update best model if this model performs better\n            if (avg_accuracy > best_accuracy) and (avg_balanced_accuracy > best_balanced_accuracy):\n                best_model_name = model_name\n                best_model = grid_search.best_estimator_\n                best_accuracy = avg_accuracy\n                best_balanced_accuracy = avg_balanced_accuracy\n                best_params = grid_search.best_params_\n\n        # Store results for this k value\n        results[k] = {\n            \"Best Model\": best_model_name,\n            \"Best Accuracy\": best_accuracy,\n            \"Best Balanced Accuracy\": best_balanced_accuracy,\n            \"Best Parameters\": best_params,\n            \"Best Model Object\": best_model,\n        }\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:34:15.532809Z","iopub.execute_input":"2024-12-03T21:34:15.533515Z","iopub.status.idle":"2024-12-03T21:34:15.544892Z","shell.execute_reply.started":"2024-12-03T21:34:15.533477Z","shell.execute_reply":"2024-12-03T21:34:15.543971Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"k_values_list = list(filtered_features.keys())\nprint(k_values_list)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:34:17.616251Z","iopub.execute_input":"2024-12-03T21:34:17.616599Z","iopub.status.idle":"2024-12-03T21:34:17.621271Z","shell.execute_reply.started":"2024-12-03T21:34:17.616569Z","shell.execute_reply":"2024-12-03T21:34:17.620365Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the function\nresults = evaluate_tree_models_on_splits(transcripts_df, feature_columns, target_column, k_values_list, filtered_features)\n\n# Display the best model for each k\nfor k, result in results.items():\n    print(f\"Top {k} features:\")\n    print(f\"Best Model: {result['Best Model']}\")\n    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\")\n    print(f\"Best Parameters: {result['Best Parameters']}\")\n    print(\"\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:34:18.068584Z","iopub.execute_input":"2024-12-03T21:34:18.068886Z","iopub.status.idle":"2024-12-03T21:38:54.323088Z","shell.execute_reply.started":"2024-12-03T21:34:18.068860Z","shell.execute_reply":"2024-12-03T21:38:54.321831Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import joblib\n\n# Iterate over results to save the best model for each k\nfor k, result in results.items():\n    best_model = result[\"Best Model Object\"]  # Get the best model object\n    filename = f\"partb_best_ml_model_top_{k}_features.pkl\"\n    joblib.dump(best_model, filename)\n    print(f\"Saved best model for top {k} features as '{filename}'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:40:11.881876Z","iopub.execute_input":"2024-12-03T21:40:11.882282Z","iopub.status.idle":"2024-12-03T21:40:12.220978Z","shell.execute_reply.started":"2024-12-03T21:40:11.882247Z","shell.execute_reply":"2024-12-03T21:40:12.220112Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Running DL Models","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Conv1D, MaxPooling1D, Flatten, Dense, Dropout, LSTM\nfrom sklearn.metrics import accuracy_score, balanced_accuracy_score\n\ndef train_and_evaluate_dl_models_on_stratified_splits(stratified_splits, top_features_dict, model_type=\"Conv1D\"):\n    \"\"\"\n    Trains and evaluates Conv1D or LSTM models on stratified splits without hyperparameter tuning.\n    Dataset loading is handled via TensorFlow tensor slices.\n\n    Parameters:\n        stratified_splits (list of tuples): Train-test splits from StratifiedKFold.\n        top_features_dict (dict): Dictionary of top features for each k value.\n        model_type (str): \"Conv1D\" or \"LSTM\".\n\n    Returns:\n        dict: Results for each k value containing the model and its performance metrics.\n    \"\"\"\n    results = {}\n\n    for k, top_features in top_features_dict.items():\n        print(f\"Evaluating models for top {k} features...\")\n        \n        best_model = None\n        best_accuracy = 0\n        best_balanced_accuracy = 0\n\n        # Iterate over splits\n        for split_idx, (X_train, X_test, y_train, y_test) in enumerate(stratified_splits):\n            print(f\"Training on Split {split_idx + 1}...\")\n\n            # Filter dataset for top features\n            X_train_k = X_train[top_features]\n            X_test_k = X_test[top_features]\n\n            # Prepare TensorFlow datasets\n            train_ds = tf.data.Dataset.from_tensor_slices((X_train_k.values, y_train.values)).batch(32).prefetch(tf.data.AUTOTUNE)\n            test_ds = tf.data.Dataset.from_tensor_slices((X_test_k.values, y_test.values)).batch(32).prefetch(tf.data.AUTOTUNE)\n\n            # Prepare input shape\n            input_shape = (len(top_features), 1)  # (timesteps, features)\n\n            # Build model\n            if model_type == \"Conv1D\":\n                model = Sequential([\n                    Conv1D(64, kernel_size=3, activation='relu', input_shape=input_shape),\n                    MaxPooling1D(pool_size=2),\n                    Dropout(0.2),\n                    Flatten(),\n                    Dense(32, activation='relu'),\n                    Dense(1, activation='sigmoid')\n                ])\n            elif model_type == \"LSTM\":\n                model = Sequential([\n                    LSTM(64, return_sequences=True, input_shape=input_shape),\n                    Dropout(0.2),\n                    LSTM(32),\n                    Dense(1, activation='sigmoid')\n                ])\n            else:\n                raise ValueError(\"Invalid model_type. Choose 'Conv1D' or 'LSTM'.\")\n\n            # Compile model\n            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n            # Train model\n            model.fit(train_ds, epochs=20, verbose=0)\n            print(\"Evaluating on the test dataset....\")\n\n            # Evaluate on test set\n            y_pred = (model.predict(test_ds) > 0.5).astype(int)\n            acc = accuracy_score(y_test, y_pred)\n            bal_acc = balanced_accuracy_score(y_test, y_pred)\n\n            print(f\"Split {split_idx + 1}: Accuracy={acc:.4f}, Balanced Accuracy={bal_acc:.4f}\\n\")\n\n            # Update best model if this model performs better\n            if acc > best_accuracy and bal_acc > best_balanced_accuracy:\n                best_model = model\n                best_accuracy = acc\n                best_balanced_accuracy = bal_acc\n\n        # Store results for this k value\n        results[k] = {\n            \"Best Model\": best_model,\n            \"Best Accuracy\": best_accuracy,\n            \"Best Balanced Accuracy\": best_balanced_accuracy,\n        }\n        print(f\"Best Model for top {k} features: Accuracy={best_accuracy:.4f}, Balanced Accuracy={best_balanced_accuracy:.4f}\\n\")\n\n    return results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:59:08.311958Z","iopub.execute_input":"2024-12-03T21:59:08.312366Z","iopub.status.idle":"2024-12-03T21:59:08.325581Z","shell.execute_reply.started":"2024-12-03T21:59:08.312334Z","shell.execute_reply":"2024-12-03T21:59:08.324728Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the function for Conv1D\nconv1d_results = train_and_evaluate_dl_models_on_stratified_splits(\n    stratified_splits=splits,\n    top_features_dict=filtered_features,\n    model_type=\"Conv1D\"\n)\n\n# Display results\nfor k, result in conv1d_results.items():\n    print(f\"Top {k} features:\")\n    print(f\"Best Model: {result['Best Model']}\")\n    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T21:59:35.255941Z","iopub.execute_input":"2024-12-03T21:59:35.256384Z","iopub.status.idle":"2024-12-03T22:01:05.905691Z","shell.execute_reply.started":"2024-12-03T21:59:35.256323Z","shell.execute_reply":"2024-12-03T22:01:05.904670Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Call the function for Conv1D\nlstm_results = train_and_evaluate_dl_models_on_stratified_splits(\n    stratified_splits=splits,\n    top_features_dict=filtered_features,\n    model_type=\"LSTM\"\n)\n\n# Display results\nfor k, result in lstm_results.items():\n    print(f\"Top {k} features:\")\n    print(f\"Best Model: {result['Best Model']}\")\n    print(f\"Best Accuracy: {result['Best Accuracy']:.4f}\")\n    print(f\"Best Balanced Accuracy: {result['Best Balanced Accuracy']:.4f}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-03T22:01:05.907147Z","iopub.execute_input":"2024-12-03T22:01:05.907434Z","iopub.status.idle":"2024-12-03T22:03:25.040855Z","shell.execute_reply.started":"2024-12-03T22:01:05.907405Z","shell.execute_reply":"2024-12-03T22:03:25.040072Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}